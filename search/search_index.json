{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction The goal of Denim Compute is to provide a reference implementation for a Digital Business Automation (DBA) solution on Kubernetes application platforms. Denim Compute uses a poster child of business automation, the automobile insurance claim processing, and focuses on the capabilities available in the IBM Cloud Pak for Automation to implement it. As of version 19.0.1 of the Cloud Pak, these capabilities are: Workflow, with the Business Automation Workflow (BAW) on CAM component. Content Management, with the FileNet Content Manager component. Decisions, with the Operational Decision Manager (ODM) component Operational Intelligence, with the Business Automation Insights (BAI) component Capture, with the Business Automation Content Analyzer (BACA) component With this reference implementation, we intend to illustrate how the different capabilities of the platform come together through the sample use case to create a holistic automation solution. This includes demonstrating: How to install the different components of the Cloud Pak for Automation on a private cloud environment. When to use a mix of ad-hoc (Case Management) and sequential activities (BPMN) How to implement the hand-shake between Case and BPMN, and the exchange of data elements. How a machine learning model can be used in conjunction with ODM to render decisions. How to use BAI to monitor business events from decisions and process execution. Since the Cloud Pak for Automation is on a continuous release schedule, the architecture and implementation of Denim Compute will be aligned with the new features and capabilities of the pak as they become available. So check back often for improvements and extensions in the What's new section below. What's new 07/22/19 Initial release!","title":"Introduction"},{"location":"#introduction","text":"The goal of Denim Compute is to provide a reference implementation for a Digital Business Automation (DBA) solution on Kubernetes application platforms. Denim Compute uses a poster child of business automation, the automobile insurance claim processing, and focuses on the capabilities available in the IBM Cloud Pak for Automation to implement it. As of version 19.0.1 of the Cloud Pak, these capabilities are: Workflow, with the Business Automation Workflow (BAW) on CAM component. Content Management, with the FileNet Content Manager component. Decisions, with the Operational Decision Manager (ODM) component Operational Intelligence, with the Business Automation Insights (BAI) component Capture, with the Business Automation Content Analyzer (BACA) component With this reference implementation, we intend to illustrate how the different capabilities of the platform come together through the sample use case to create a holistic automation solution. This includes demonstrating: How to install the different components of the Cloud Pak for Automation on a private cloud environment. When to use a mix of ad-hoc (Case Management) and sequential activities (BPMN) How to implement the hand-shake between Case and BPMN, and the exchange of data elements. How a machine learning model can be used in conjunction with ODM to render decisions. How to use BAI to monitor business events from decisions and process execution. Since the Cloud Pak for Automation is on a continuous release schedule, the architecture and implementation of Denim Compute will be aligned with the new features and capabilities of the pak as they become available. So check back often for improvements and extensions in the What's new section below.","title":"Introduction"},{"location":"#whats-new","text":"07/22/19 Initial release!","title":"What's new "},{"location":"references/","text":"References DBA Architecture Center","title":"References"},{"location":"references/#references","text":"DBA Architecture Center","title":"References"},{"location":"design/decisions/","text":"Business decisions The claim processing scenario currently involves three rule-based decisions: Perform claim segmentation which evaluates the information available from the initial claim intake and uses it to compute a complexity score for the claim. This score will be used to route the claim to an adjuster with the right experience or for simple cases, bypass the need to involve the adjuster. Assess fraud potential is also using the the information from the initial claim intake as well as information available on the driver as well as the policy holder if they are different persons. The decision returns a fraud propensity score that is then used to decide whether a detailed fraud investigation is warranted. Review for escalation uses information from the claim case to determine whether the claim should be reviewed by the claim manager before a claim settlement is created and proposed to the claimant. It returns a yes/no decision, with a set of justifications when the decision is to escalate. Object model The different decision are built using a common object model based on the concepts of Loss, Claim and Policy. A high-level diagram of the input model is shown below: Output of the three decisions is captured by the classes below: Rule projects The rule projects are organized as a decision service in a standard fashion, placing the BOM in a separate project that is referenced by the individual rule projects. Each Injecting machine learning The claim fraud assessment decision service is a good candidate to inject a fraud detection service based on a machine learning (ML) model. The typical division of responsibility is that the ML model excels in detecting patterns of fraud, while rules can flag outliers, marginal or heuristic cases that have been detected but do not yet represent a pattern. NB : We are planning to integrate the ML-based scoring model for fraud in a future Denim Compute iteration. For now, we just have a clean set of labeled data (see {denim-compute-repo}/source/ml/denim-insurance_fraud-data.csv ). This sample fraud dataset is a good fit for the AutoAI feature of Waston Studio : AutoAI will automatically determine a set of ML pipelines that perform well given the dataset and the dependent feature (in our case, the Fraudulent column). After the experiment execution completes, Watson AutoAI presents the possible pipelines to choose from, each with their associated KPIs. Once the desired pipeline is selected, it can be simply operationalized by creating a deployment, which exposes a scoring end-point: From there, the scoring service can be manually tested: Sample input: { input_data : [ { fields : [ Claim Amount , Coverage , Education , Employment Status , Income , Marital Status , Monthly Premium Auto , Months Since Last Claim , Months Since Policy Inception , Claim Reason , Sales Channel ], values : [ [ 276, Basic , Bachelor , Employed , 56274, Married , 69, 32, 5, Collision , Agent ], [ 265, Basic , High School , Unemployed , 0, Married , 70, 7, Scratch or Dent , CallCenter ] ] } ] } Sample output: { predictions : [ { fields : [ prediction , probability ], values : [ [ 0, [1, 9.530106164762553e-22]], [ 1, [0.0001811385154724121, 0.9998188614845276]] ] } ] }","title":"Decisions"},{"location":"design/decisions/#business-decisions","text":"The claim processing scenario currently involves three rule-based decisions: Perform claim segmentation which evaluates the information available from the initial claim intake and uses it to compute a complexity score for the claim. This score will be used to route the claim to an adjuster with the right experience or for simple cases, bypass the need to involve the adjuster. Assess fraud potential is also using the the information from the initial claim intake as well as information available on the driver as well as the policy holder if they are different persons. The decision returns a fraud propensity score that is then used to decide whether a detailed fraud investigation is warranted. Review for escalation uses information from the claim case to determine whether the claim should be reviewed by the claim manager before a claim settlement is created and proposed to the claimant. It returns a yes/no decision, with a set of justifications when the decision is to escalate.","title":"Business decisions"},{"location":"design/decisions/#rule-projects","text":"The rule projects are organized as a decision service in a standard fashion, placing the BOM in a separate project that is referenced by the individual rule projects. Each","title":"Rule projects"},{"location":"design/decisions/#injecting-machine-learning","text":"The claim fraud assessment decision service is a good candidate to inject a fraud detection service based on a machine learning (ML) model. The typical division of responsibility is that the ML model excels in detecting patterns of fraud, while rules can flag outliers, marginal or heuristic cases that have been detected but do not yet represent a pattern. NB : We are planning to integrate the ML-based scoring model for fraud in a future Denim Compute iteration. For now, we just have a clean set of labeled data (see {denim-compute-repo}/source/ml/denim-insurance_fraud-data.csv ). This sample fraud dataset is a good fit for the AutoAI feature of Waston Studio : AutoAI will automatically determine a set of ML pipelines that perform well given the dataset and the dependent feature (in our case, the Fraudulent column). After the experiment execution completes, Watson AutoAI presents the possible pipelines to choose from, each with their associated KPIs. Once the desired pipeline is selected, it can be simply operationalized by creating a deployment, which exposes a scoring end-point: From there, the scoring service can be manually tested: Sample input: { input_data : [ { fields : [ Claim Amount , Coverage , Education , Employment Status , Income , Marital Status , Monthly Premium Auto , Months Since Last Claim , Months Since Policy Inception , Claim Reason , Sales Channel ], values : [ [ 276, Basic , Bachelor , Employed , 56274, Married , 69, 32, 5, Collision , Agent ], [ 265, Basic , High School , Unemployed , 0, Married , 70, 7, Scratch or Dent , CallCenter ] ] } ] } Sample output: { predictions : [ { fields : [ prediction , probability ], values : [ [ 0, [1, 9.530106164762553e-22]], [ 1, [0.0001811385154724121, 0.9998188614845276]] ] } ] }","title":"Injecting machine learning"},{"location":"design/insights/","text":"Operational intelligence The Business Automation Insights (BAI) component provides capabilities to visualize the business events that are generated by an automation solution, as well as feed these events to a data lake so that deeper insights can be derived about the solution, in particular by applying machine learning algorithms to the events. In the following sections, we describe how BAI is put to work for the different components of the Denim Compute solution. ODM dashboard The Denim Compute implementation uses ODM events collected by BAI to present a Kibana dashboard that shows the distribution of the different business decision outcomes over a period of time. For the claim segmentation decision, the visualization is the portion of low (green), medium (orange) and high (red) complexity claims processed in past period. For escalation review , the portion of escalated claims (dark blue) versus the claims that do not need escalation (light blue) is displayed. For fraud assessment , the fraud median score as well as the maximum score over the past period is displayed. A sample view of this dashboard is shown below. The dashboard is created following the steps in this tutorial , using the following ODM output parameters fields to build the search: Decision Target Kibana field BOM attribute Assess Fraud data.claim_processing.assess_fraud.out.fraud.score FraudAssessment.score Review Escalation data.claim_processing.review_escalation.out.escalation.required EscalationAssessment.required Segment Claim data.claim_processing.segment_claim.out.complexity.complexity ComplexityAssessment.complexity BAW dashboard Please check back later, as we design and implement custom BPMN and Case dashboards in a future iteration of Denim Compute.","title":"Insights"},{"location":"design/insights/#operational-intelligence","text":"The Business Automation Insights (BAI) component provides capabilities to visualize the business events that are generated by an automation solution, as well as feed these events to a data lake so that deeper insights can be derived about the solution, in particular by applying machine learning algorithms to the events. In the following sections, we describe how BAI is put to work for the different components of the Denim Compute solution.","title":"Operational intelligence"},{"location":"design/insights/#odm-dashboard","text":"The Denim Compute implementation uses ODM events collected by BAI to present a Kibana dashboard that shows the distribution of the different business decision outcomes over a period of time. For the claim segmentation decision, the visualization is the portion of low (green), medium (orange) and high (red) complexity claims processed in past period. For escalation review , the portion of escalated claims (dark blue) versus the claims that do not need escalation (light blue) is displayed. For fraud assessment , the fraud median score as well as the maximum score over the past period is displayed. A sample view of this dashboard is shown below. The dashboard is created following the steps in this tutorial , using the following ODM output parameters fields to build the search: Decision Target Kibana field BOM attribute Assess Fraud data.claim_processing.assess_fraud.out.fraud.score FraudAssessment.score Review Escalation data.claim_processing.review_escalation.out.escalation.required EscalationAssessment.required Segment Claim data.claim_processing.segment_claim.out.complexity.complexity ComplexityAssessment.complexity","title":"ODM dashboard"},{"location":"design/insights/#baw-dashboard","text":"Please check back later, as we design and implement custom BPMN and Case dashboards in a future iteration of Denim Compute.","title":"BAW dashboard"},{"location":"design/sundries/","text":"DBA solution design findings Case Design Leveraging case persistence The claims solution implements a Policy case which is persisted as a record to represent the insurance policy. The Policy case type stores the policy information and claims are generated from the policy by a create claim activity, which creates the new claim case and also transfer existing policy information into the claim. Case data model and business objects The Case data model currently does not fully support Business Objects, therefore an intering solution have been implemented to map properties. The interface of the Case Manager data model with other BAW components may require the implementation of distinct case properties to match each element of a business object. A new upcoming version of Case Manager will fully support business objects and allow better data integration with other BAW components.","title":"Sundries"},{"location":"design/sundries/#dba-solution-design-findings","text":"","title":"DBA solution design findings"},{"location":"design/sundries/#case-design","text":"","title":"Case Design"},{"location":"design/sundries/#leveraging-case-persistence","text":"The claims solution implements a Policy case which is persisted as a record to represent the insurance policy. The Policy case type stores the policy information and claims are generated from the policy by a create claim activity, which creates the new claim case and also transfer existing policy information into the claim.","title":"Leveraging case persistence"},{"location":"design/sundries/#case-data-model-and-business-objects","text":"The Case data model currently does not fully support Business Objects, therefore an intering solution have been implemented to map properties. The interface of the Case Manager data model with other BAW components may require the implementation of distinct case properties to match each element of a business object. A new upcoming version of Case Manager will fully support business objects and allow better data integration with other BAW components.","title":"Case data model and business objects"},{"location":"design/workflow/","text":"Workflow design This section covers the design of the Denim Compute workflow using the Business Automation Workflow (BAW) platform. It is recommended to be familiar with Scenario walkthrough in conjunction with this section. Case and Process collaboration Denim Compute uses the combined capabilities of Case Management and Process flow from Business Automation Workflow in the solution scenario. Case-oriented workflows are typically modeled in CMMN notation while Process-oriented ones use BPMN notation. In order to show the collaboration between the Case and Process aspects we show a BPMN collaboration diagram as a close approximation of how the separation of focus is achieved. The following two figures show the high level workflow as a BPMN Collaboration with Case ad-hoc activities in one pool and Process directed acyclic graph in the other pool and message exchanges between both as control passes back and forth. Within the Case activities there is one special one (highlighted in the image below) that has scope for the duration of the Process it invokes. This is because the manner in which other Case activities are invoked is via property updates that in turn trigger pre-conditions on the activities. In order to update a Case property the Process must be able to reference its parent Case activity which remains in scope. If we look at the Case Builder part of Business Automation Workflow, the highlighted activities match the earlier depiction whereby Gather Accident Information is a P8 Process implementation that updates a case property that then triggers the Initiate Claims Processing activity which is a BPM Process implementation. The Initiate Claims Processing Process shown below is then responsible for co-ordinating both Process steps and communication back to Case to request further ad-hoc activities. Highlighted shows an example of the earlier model's message exchange. In the outbound message it is achieved by using the Javascript API to update a Case property which then triggers the pre-condition of a Case activity. The intermediate receiving message event then represents the return inbound message signaling the ad-hoc activity has finished. Here in Case Builder again are the relevant activities with the first one ( Create Adjuster Report ) implemented as a P8 Process which then triggers the Signal Adjuster Report Created implemented as a BPM Process which will then send the message event to the main awaiting process. Note this was a design decision to use the message pair capabilities in BPM, an alternative would be to implement a step in Create Adjuster Report P8 that would need to make a REST call to send the message event. This is the BPM Process implementation of Signal Adjuster Report Created with the message send event highlighted.","title":"Workflow"},{"location":"design/workflow/#workflow-design","text":"This section covers the design of the Denim Compute workflow using the Business Automation Workflow (BAW) platform. It is recommended to be familiar with Scenario walkthrough in conjunction with this section.","title":"Workflow design"},{"location":"design/workflow/#case-and-process-collaboration","text":"Denim Compute uses the combined capabilities of Case Management and Process flow from Business Automation Workflow in the solution scenario. Case-oriented workflows are typically modeled in CMMN notation while Process-oriented ones use BPMN notation. In order to show the collaboration between the Case and Process aspects we show a BPMN collaboration diagram as a close approximation of how the separation of focus is achieved. The following two figures show the high level workflow as a BPMN Collaboration with Case ad-hoc activities in one pool and Process directed acyclic graph in the other pool and message exchanges between both as control passes back and forth. Within the Case activities there is one special one (highlighted in the image below) that has scope for the duration of the Process it invokes. This is because the manner in which other Case activities are invoked is via property updates that in turn trigger pre-conditions on the activities. In order to update a Case property the Process must be able to reference its parent Case activity which remains in scope. If we look at the Case Builder part of Business Automation Workflow, the highlighted activities match the earlier depiction whereby Gather Accident Information is a P8 Process implementation that updates a case property that then triggers the Initiate Claims Processing activity which is a BPM Process implementation. The Initiate Claims Processing Process shown below is then responsible for co-ordinating both Process steps and communication back to Case to request further ad-hoc activities. Highlighted shows an example of the earlier model's message exchange. In the outbound message it is achieved by using the Javascript API to update a Case property which then triggers the pre-condition of a Case activity. The intermediate receiving message event then represents the return inbound message signaling the ad-hoc activity has finished. Here in Case Builder again are the relevant activities with the first one ( Create Adjuster Report ) implemented as a P8 Process which then triggers the Signal Adjuster Report Created implemented as a BPM Process which will then send the message event to the main awaiting process. Note this was a design decision to use the message pair capabilities in BPM, an alternative would be to implement a step in Create Adjuster Report P8 that would need to make a REST call to send the message event. This is the BPM Process implementation of Signal Adjuster Report Created with the message send event highlighted.","title":"Case and Process collaboration"},{"location":"development/case-activities/","text":"Case activities Leveraging case persistence The claims solution implements a Policy case which is persisted as a record to represent the insurance policy. The Policy case type stores the policy information and claims are generated from the policy by a create claim activity, which creates the new claim case and also transfer existing policy information into the claim. Creating a Claim from Policy When the first notice of loss is received by the claim representative the first task is to search for the insured Policy, once the Policy is found the claim representative starts a discretionary activity that will create the new Claim case and also automatically transfer the insured policy data into the claim. The activity that creates the new claim case is implemented using FileNet workflow. FileNet Workflow and Process are both supported with BAW. FileNet workflow is used to implement this activity to leverage the standard CE_Operations and ICM_Operations. The equivalent of these two builtin operations are not yet available in BAW 19.0.0.2 but will be available for the process in a subsequent release. The details of the activity can be accessed from BAW Case Builder. The Builder does not provide the ability to open the details of each step of the workflow or to edit these steps. In order to edit the step is necessary to open this FileNet Workflow using \"FileNet Process Designer\" which is a tool available as java application. \"Filenet Process Design\" is a workflow design tool used to implement FiLeNet Workflows and even if the names are similar it should not be confused with \"BPM Process Designer\" . The FileNet Process Designer tool can usually be found on BAW installation directory: install_root/FileNet/ContentEngine/tools/PE/pedesigner.bat connection point Where is the the FileNet workflow system connection point configured in your environment. In the following is show the general view of the workflow and the details of the step and ICM_Operation to create a new claim case. Case data model and business objects The Case data model currently does not fully support Business Objects, therefore an interim solution has been implemented to map properties. The interface of the Case Manager data model with other BAW components may require the implementation of distinct case properties to match each element of a business object. A new upcoming version of Case Manager will fully support business objects and allow better data integration with other BAW components.","title":"Case activities"},{"location":"development/case-activities/#case-activities","text":"","title":"Case activities"},{"location":"development/case-activities/#leveraging-case-persistence","text":"The claims solution implements a Policy case which is persisted as a record to represent the insurance policy. The Policy case type stores the policy information and claims are generated from the policy by a create claim activity, which creates the new claim case and also transfer existing policy information into the claim.","title":"Leveraging case persistence"},{"location":"development/case-activities/#creating-a-claim-from-policy","text":"When the first notice of loss is received by the claim representative the first task is to search for the insured Policy, once the Policy is found the claim representative starts a discretionary activity that will create the new Claim case and also automatically transfer the insured policy data into the claim. The activity that creates the new claim case is implemented using FileNet workflow. FileNet Workflow and Process are both supported with BAW. FileNet workflow is used to implement this activity to leverage the standard CE_Operations and ICM_Operations. The equivalent of these two builtin operations are not yet available in BAW 19.0.0.2 but will be available for the process in a subsequent release. The details of the activity can be accessed from BAW Case Builder. The Builder does not provide the ability to open the details of each step of the workflow or to edit these steps. In order to edit the step is necessary to open this FileNet Workflow using \"FileNet Process Designer\" which is a tool available as java application. \"Filenet Process Design\" is a workflow design tool used to implement FiLeNet Workflows and even if the names are similar it should not be confused with \"BPM Process Designer\" . The FileNet Process Designer tool can usually be found on BAW installation directory: install_root/FileNet/ContentEngine/tools/PE/pedesigner.bat connection point Where is the the FileNet workflow system connection point configured in your environment. In the following is show the general view of the workflow and the details of the step and ICM_Operation to create a new claim case.","title":"Creating a Claim from Policy"},{"location":"development/case-activities/#case-data-model-and-business-objects","text":"The Case data model currently does not fully support Business Objects, therefore an interim solution has been implemented to map properties. The interface of the Case Manager data model with other BAW components may require the implementation of distinct case properties to match each element of a business object. A new upcoming version of Case Manager will fully support business objects and allow better data integration with other BAW components.","title":"Case data model and business objects"},{"location":"development/case-user-interface/","text":"Case User Interface Case property validation Case implements out of the box property validation in relation to type and format according template. However some more complex validations should be implemented with scripts. The solution implements a script action on Work Details page of the activity Gather Accident Information . The script action validates two date properties: Date Reported Date-Time of Loss These properties are compared against the current date and prevent a user to enter a date that is in future. The script action Validate and Complete replaces the original Complete action on the Work Details page. In order to implement the validation is necessary to create a custom Complete action using a Script Action added to the Work Details page associated with that particular step in the workflow. In the this case the Activity has on one step and has the same name of the Activity \"Gather Accident Information\": You can review the implementation by opening Cases pages and select under Work Details the page \"Gather Accident Information\" Once the page is open for editing select the Toolbar, open Settings (gear symbol on the right side), and check the new custom button in the Toolbar menu. Once the new button opens, verify that the Action \"Script Action\" is selected, the label is according to \"Validate and Complete\" and the script action code is in the Execution pane. The source code of the script action can be found in the validate-and-complete-workitem.js file under {denim-compute-repo}/source/case . Transfer case property to document When a user files a document into the case, for instance a Police Report, some case properties should be transferred and automatically updated into the document class (Case Document Type). The Claims solution implementation a custom script action to perform this property transfer and update. The script action is implemented and deployed into the work details page when an activity is created to file a police report. The following properties are read from the case and transferred into the document class: Policy Number Claim Number This script action requires the document class to have Policy Number and Claim Number as properties. Review the implementation of the code by opening the Work Details page \"Gather Accident Information\" like in the previous section and select this time the \"Case Information\" widget and open the Settings (gear on the right corner). Select the custom button \"Add Document and Set Property\" and review the settings of the script action and the code in the field Execute. The script action source code can be found in the file-police-report.js file under {denim-compute-repo}/source/case .","title":"Case user interface"},{"location":"development/case-user-interface/#case-user-interface","text":"","title":"Case User Interface"},{"location":"development/case-user-interface/#case-property-validation","text":"Case implements out of the box property validation in relation to type and format according template. However some more complex validations should be implemented with scripts. The solution implements a script action on Work Details page of the activity Gather Accident Information . The script action validates two date properties: Date Reported Date-Time of Loss These properties are compared against the current date and prevent a user to enter a date that is in future. The script action Validate and Complete replaces the original Complete action on the Work Details page. In order to implement the validation is necessary to create a custom Complete action using a Script Action added to the Work Details page associated with that particular step in the workflow. In the this case the Activity has on one step and has the same name of the Activity \"Gather Accident Information\": You can review the implementation by opening Cases pages and select under Work Details the page \"Gather Accident Information\" Once the page is open for editing select the Toolbar, open Settings (gear symbol on the right side), and check the new custom button in the Toolbar menu. Once the new button opens, verify that the Action \"Script Action\" is selected, the label is according to \"Validate and Complete\" and the script action code is in the Execution pane. The source code of the script action can be found in the validate-and-complete-workitem.js file under {denim-compute-repo}/source/case .","title":"Case property validation"},{"location":"development/case-user-interface/#transfer-case-property-to-document","text":"When a user files a document into the case, for instance a Police Report, some case properties should be transferred and automatically updated into the document class (Case Document Type). The Claims solution implementation a custom script action to perform this property transfer and update. The script action is implemented and deployed into the work details page when an activity is created to file a police report. The following properties are read from the case and transferred into the document class: Policy Number Claim Number This script action requires the document class to have Policy Number and Claim Number as properties. Review the implementation of the code by opening the Work Details page \"Gather Accident Information\" like in the previous section and select this time the \"Case Information\" widget and open the Settings (gear on the right corner). Select the custom button \"Add Document and Set Property\" and review the settings of the script action and the code in the field Execute. The script action source code can be found in the file-police-report.js file under {denim-compute-repo}/source/case .","title":"Transfer case property to document"},{"location":"development/decisions/","text":"Business decisions implementation Business Object Model The BOM is composed of two BOM entries, shown on the figure below The first one, claim-model , is built bottom-up from the Java object model. The second, util-model , holds purely virtual BOM classes that provides utilities to the decision service rules. The domains for the different attributes of the object model are defined using Excel-based dynamic domains. The domains.xls spreadsheet under the resources folder of the BOM defines the different domains. The domains are then associated with virtual classes defined under the types folder of claim-model . Finally, for each attribute which value is backed a domain, we create a corresponding virtual attribute which type is the domain class. We remove any verbalization from the attribute that maps to the Java model, and instead verbalize the virtual attribute. We add the suffix Virtual to the original attribute name as the naming convention for the virtual attribute. For example, in the figure below, we create the descriptionVirtual attribute corresponding to the description attributes that is derived from the Java model. Sample rule artifacts We show here a few rules from the segment-claim decision operation for illustration purpose. The rule flow for this operation is a simple sequence of 3 tasks. The first one computes a complexity score for different aspects of the accident: damages , injuries and others . The second one is aggregating the different component scores and normalizes them on a single scale. The third task derives a categorical complexity (low, medium, high) from the normalized score. Below are 2 examples of rules, respectively contributing to the damages and injuries component: single vehicle damage rule definition fatal injury rule definition Deployment configuration The claim-processing deployment configuration is defined to encapsulate the 3 decision operations segment-claim , assess-fraud and review-escalation . Running this deployment configuration generates and deploys the claim_processing RuleApp, which encapsulates 3 rulesets, exposed through the following decision end-points: http://{host}:{port}/DecisionService/rest/v1/claim_processing/1.0/segment_claim/1.0 http://{host}:{port}/DecisionService/rest/v1/claim_processing/1.0/assess_fraud/1.0 http://{host}:{port}/DecisionService/rest/v1/claim_processing/1.0/review_escalation/1.0 Connecting ruleset to BAI To enable an ODM ruleset to send events to the BAI component, the bai.emitter.enabled ruleset property must be set to true . The additional boolean ruleset properties below allow to control the scope of the events that are emitted: - bai.emitter.input - bai.emitter.output - bai.emitter.trace We have set-up the following properties in the deployment configuration. Once deployed, the ruleset property values can be adjusted in the Rule Execution Server console. Ruleset testing Once the claim processing decision service projects are deployed to Decision Center, you can exercise test scenarios on the different decision operations. Sample test scenarios, in the form of business users friendly Excel spreadsheets, are provided here: {denim-compute-repo}/solution/odm/test . To run a test suite, open the claim-processing decision service, and on the Tests tab, click the New Test Suite (+ sig) button, the select the decision operation you want to test, for example segment-claim : In the file to use of the Scenarios section of the page, select the Scenario File - Claim Segmentation.xlsx file, then click the Save and Run button: One the test suite run is complete, you will be switched to the Reports tab. You can then click on the newly generated test execution report to inspect the individual test scenarios execution:","title":"Decisions"},{"location":"development/decisions/#business-decisions-implementation","text":"","title":"Business decisions implementation"},{"location":"development/decisions/#business-object-model","text":"The BOM is composed of two BOM entries, shown on the figure below The first one, claim-model , is built bottom-up from the Java object model. The second, util-model , holds purely virtual BOM classes that provides utilities to the decision service rules. The domains for the different attributes of the object model are defined using Excel-based dynamic domains. The domains.xls spreadsheet under the resources folder of the BOM defines the different domains. The domains are then associated with virtual classes defined under the types folder of claim-model . Finally, for each attribute which value is backed a domain, we create a corresponding virtual attribute which type is the domain class. We remove any verbalization from the attribute that maps to the Java model, and instead verbalize the virtual attribute. We add the suffix Virtual to the original attribute name as the naming convention for the virtual attribute. For example, in the figure below, we create the descriptionVirtual attribute corresponding to the description attributes that is derived from the Java model.","title":"Business Object Model"},{"location":"development/decisions/#sample-rule-artifacts","text":"We show here a few rules from the segment-claim decision operation for illustration purpose. The rule flow for this operation is a simple sequence of 3 tasks. The first one computes a complexity score for different aspects of the accident: damages , injuries and others . The second one is aggregating the different component scores and normalizes them on a single scale. The third task derives a categorical complexity (low, medium, high) from the normalized score. Below are 2 examples of rules, respectively contributing to the damages and injuries component: single vehicle damage rule definition fatal injury rule definition","title":"Sample rule artifacts"},{"location":"development/decisions/#deployment-configuration","text":"The claim-processing deployment configuration is defined to encapsulate the 3 decision operations segment-claim , assess-fraud and review-escalation . Running this deployment configuration generates and deploys the claim_processing RuleApp, which encapsulates 3 rulesets, exposed through the following decision end-points: http://{host}:{port}/DecisionService/rest/v1/claim_processing/1.0/segment_claim/1.0 http://{host}:{port}/DecisionService/rest/v1/claim_processing/1.0/assess_fraud/1.0 http://{host}:{port}/DecisionService/rest/v1/claim_processing/1.0/review_escalation/1.0","title":"Deployment configuration"},{"location":"development/decisions/#connecting-ruleset-to-bai","text":"To enable an ODM ruleset to send events to the BAI component, the bai.emitter.enabled ruleset property must be set to true . The additional boolean ruleset properties below allow to control the scope of the events that are emitted: - bai.emitter.input - bai.emitter.output - bai.emitter.trace We have set-up the following properties in the deployment configuration. Once deployed, the ruleset property values can be adjusted in the Rule Execution Server console.","title":"Connecting ruleset to BAI"},{"location":"development/decisions/#ruleset-testing","text":"Once the claim processing decision service projects are deployed to Decision Center, you can exercise test scenarios on the different decision operations. Sample test scenarios, in the form of business users friendly Excel spreadsheets, are provided here: {denim-compute-repo}/solution/odm/test . To run a test suite, open the claim-processing decision service, and on the Tests tab, click the New Test Suite (+ sig) button, the select the decision operation you want to test, for example segment-claim : In the file to use of the Scenarios section of the page, select the Scenario File - Claim Segmentation.xlsx file, then click the Save and Run button: One the test suite run is complete, you will be switched to the Reports tab. You can then click on the newly generated test execution report to inspect the individual test scenarios execution:","title":"Ruleset testing"},{"location":"development/process-flow-control/","text":"Process flow control Process hierarchy The top level process that is called from the Gather Accident Information Case activity when the FNOL (First Notice of Loss) stage is complete is Initiate Claims Processing , shown below. The process diagram is color-coded to highlight specific patterns of processing as follows: Orange for integration between Case and Process Green for integration between Process and ODM (Operational Decision Manager) Purple for integration between process and ECM (Enterprise Content Management) The Initiate Claims Processing process includes a call to the linked process Loss Assessment . In the below image, we focus on the MVP path through that process (which matches the scenario in the Scenario walkthrough section). There are additional steps to handle the situation where the ODM service Review Escalation Conditions results in a path that needs review and potential rework which you can expect by opening up the process in the Process Designer tool. Loss Assessment in turn calls the linked process Estimate Damage which involves two user tasks (that are implemented in BAW with client-side human services that contain one or more coaches for user interaction) Tender for Estimates and Review and Select Estimate . In between these two user tasks is a message exchange (for more details on the message exchange pattern see the Process message exchange section in Process services ) which invokes a further process and receives the result. The Provide Repair Estimates process is a kind of batch processing paradigm as it needs to manage the soliciting and obtaining of multiple repair estimates which it does by invoking a further linked process, Provide Repair Estimates Per Vehicle , using a multi-instance loop construct. The Provide Repair Estimates Per Vehicle process shown below handles the processing for each vehicle in the claim. It then delegates to a further multi-instance loop to call the linked process Provide Repair Estimates Per Repairer . The Provide Repair Estimates Per Repairer process then handles the collecting of a repair estimate from an individual repairer for a single vehicle. Process synchronization with Case As discussed in the Workflow Design section, the workflow solution involves a collaboration between Case ad-hoc activities and process sequential activities. That collaboration is achieved in a number of ways, first the main process is implemented as a Case activity as highlighted below. Within Initiate Claims Processing , there is a need to synchronize the case properties by reading them from the parent Case and this is done in the service flow implementation Map Case Correlation Properties , which is part of the referenced Toolkit Denim Compute Auto Claims Toolkit . For more details on the synchronization framework, please see the Case properties synchronization section within Process services . When the process needs to pass control back to Case and invoke another case activity. It does that by updating one or more Case properties that are used as the pre-condition of that target activity. The logic to do this is illustrated by the activity with service flow implementation Request Claim Adjuster Report shown below, which again uses the synchronization framework. Here we see the target Case activity called Create Adjuster Report (which is implemented as a P8 process) and the highlighted start pre-condition which references a Case property that has been updated to trigger this activity. The above screen shot also shows the follow-on Case activity called Signal Adjuster Report Created which is triggered from a property update that is done within the previous Create Adjuster Report activity. That activity is implemented as a further process that then uses a Message Send Event (highlighted) to communicate to the main Initiate Claims Processing process. For further details on the mechanics of message event exchanges, see the Process message exchange section of Process services . To complete the message exchange pattern, here we see the awaiting Intermediate Message Event of type receiving inside Initiate Claims Processing which then \"awakens\" that quiesced process based on correlating information supplied to it from Signal Adjuster Report Created .","title":"Process flow control"},{"location":"development/process-flow-control/#process-flow-control","text":"","title":"Process flow control"},{"location":"development/process-flow-control/#process-hierarchy","text":"The top level process that is called from the Gather Accident Information Case activity when the FNOL (First Notice of Loss) stage is complete is Initiate Claims Processing , shown below. The process diagram is color-coded to highlight specific patterns of processing as follows: Orange for integration between Case and Process Green for integration between Process and ODM (Operational Decision Manager) Purple for integration between process and ECM (Enterprise Content Management) The Initiate Claims Processing process includes a call to the linked process Loss Assessment . In the below image, we focus on the MVP path through that process (which matches the scenario in the Scenario walkthrough section). There are additional steps to handle the situation where the ODM service Review Escalation Conditions results in a path that needs review and potential rework which you can expect by opening up the process in the Process Designer tool. Loss Assessment in turn calls the linked process Estimate Damage which involves two user tasks (that are implemented in BAW with client-side human services that contain one or more coaches for user interaction) Tender for Estimates and Review and Select Estimate . In between these two user tasks is a message exchange (for more details on the message exchange pattern see the Process message exchange section in Process services ) which invokes a further process and receives the result. The Provide Repair Estimates process is a kind of batch processing paradigm as it needs to manage the soliciting and obtaining of multiple repair estimates which it does by invoking a further linked process, Provide Repair Estimates Per Vehicle , using a multi-instance loop construct. The Provide Repair Estimates Per Vehicle process shown below handles the processing for each vehicle in the claim. It then delegates to a further multi-instance loop to call the linked process Provide Repair Estimates Per Repairer . The Provide Repair Estimates Per Repairer process then handles the collecting of a repair estimate from an individual repairer for a single vehicle.","title":"Process hierarchy"},{"location":"development/process-flow-control/#process-synchronization-with-case","text":"As discussed in the Workflow Design section, the workflow solution involves a collaboration between Case ad-hoc activities and process sequential activities. That collaboration is achieved in a number of ways, first the main process is implemented as a Case activity as highlighted below. Within Initiate Claims Processing , there is a need to synchronize the case properties by reading them from the parent Case and this is done in the service flow implementation Map Case Correlation Properties , which is part of the referenced Toolkit Denim Compute Auto Claims Toolkit . For more details on the synchronization framework, please see the Case properties synchronization section within Process services . When the process needs to pass control back to Case and invoke another case activity. It does that by updating one or more Case properties that are used as the pre-condition of that target activity. The logic to do this is illustrated by the activity with service flow implementation Request Claim Adjuster Report shown below, which again uses the synchronization framework. Here we see the target Case activity called Create Adjuster Report (which is implemented as a P8 process) and the highlighted start pre-condition which references a Case property that has been updated to trigger this activity. The above screen shot also shows the follow-on Case activity called Signal Adjuster Report Created which is triggered from a property update that is done within the previous Create Adjuster Report activity. That activity is implemented as a further process that then uses a Message Send Event (highlighted) to communicate to the main Initiate Claims Processing process. For further details on the mechanics of message event exchanges, see the Process message exchange section of Process services . To complete the message exchange pattern, here we see the awaiting Intermediate Message Event of type receiving inside Initiate Claims Processing which then \"awakens\" that quiesced process based on correlating information supplied to it from Signal Adjuster Report Created .","title":"Process synchronization with Case"},{"location":"development/process-services/","text":"Process services This section provides details on service flow implementation chains of process activities. To understand the processes themselves that this section builds upon, please see Process flow control . Process message exchange Processes communicate asynchronously via message events whereby one process will send an outbound message and the other receives it. In the figure below we see an example of this pattern whereby the sending process ( Signal Adjuster Report Created ) on the left side and the receiving process ( Initiate Claims Processing ) on the right side reference the same message Under Cover Agent (UCA). In the sending process, the outbound message event has Data Mapping setup with a mapping to the correlating data plus the event payload. The correlating data is important because there is a need to identify the specific Initiate Claims Processing process instance that is awaiting the response from Signale Adjuster Report Created . Here on the other side of the exchange you can see that the event data's claimCorrelationId is designated as the correlation variable and is used then to compare to the claimCaseId values of instances in order to match and resume processing in the correct instance. Case properties synchronization In order for Case properties to be referenced inside processes they need to be retrieved. Inside the process, an AutoClaim Business Object is modeled which represents those data items of interest to the process, a large majority of which are also Case properties. A framework of services has been developed to allow for obtaining Case properties and setting them on the various levels within the complex Business Object (BO) structure (and updating them as we will cover later in this section). The principle is that each BO within a higher level complex structure has a mapping service such as illustrated in the below Map Vehicle Input (which is inside the Denim Compute Auto Claim Toolkit ) for a vehicle object. That service in turn calls a generic service Map BO Input to perform the mapping. The data mapping on the service accepts as input an ANY type representing the BO to populate and then returns it populated with data from the corresponding Case properties. The other key input is a String list representing the set of property to attribute mappings. In the Set Vehicle Mappings script, the CSV list is created. This list has an entry for each target attribute in the BO and passes in for each entry a formatted string, an example of which is highlighted. To take that value DENI1_VehicleMake:make:String the first part represents the unique name of the case property, the second is the BO attribute name, and the third is the data type. Here is the corresponding case property named Vehicle Make showing where the unique identifier comes from inside Case Builder . The Map BO Input service is shown here, it makes calls to a number of helper services in order to interact with Case using the JavaScript API. Returning to the mapping, the higher level AutoClaim BO is mapped similarly in a service Map AutoClaim Input and also it delegates to lower level services (such as Map Vehicle Input shown earlier) to perform the mappings of the contained BOs within it. When data is required to be updated in Case the pattern is to set the inputs on and invoke a generic service Write Case Properties as illustrated in the service Update Claim Settlement Details (found in the Denim Compute Auto Claims Toolkit) shown here. The Map Properties script sets the casePropertyUpdateList variable used as the input mapping on the target Write Case Properties service. ODM integration BAW integration to ODM is by using a REST External Service . The service includes the three ODM operations used in the scenario, it is named Denim ODM Service and found in the Denim Compute Decision Services Toolkit Toolkit shown here: An example usage of the External Service is in Perform Claim Segmentation within Denim Compute Auto Claim Toolkit Toolkit. Note in the scenario that there is a boundary error event handler in case the ODM service is unavailable and this then sets the outputs to follow a default path. The inputs to the ODM operation are set from case properties using the pattern already described in the Case properties synchronization sub-section above. ECM integration BAW uses two main patterns to integrate to ECM, first it has to read existing documents from ECM in order to display them to the user inside client-side human service coaches and second it has to write a new JSON document to ECM with the complex data structures populated within BPM. For the first pattern a service flow named Get Claim ECM Documents in Denim Compute Auto Claims Toolkit Toolkit is called from the Loss Assessment process as shown here. In the Data Mapping inputs to that service we provide the claimCaseId and claimSubfolderPath which are used to find the Case ECM folder and the sub folder within it of interest (in this situation that is Damages Evidence ). Here is the Get Claim ECM Documents service flow where it uses the Content Integration feature to makes calls to Get Folder by Path and Get Documents in Folder . It then filters the returned documents to ensure that they match the Document Class of interest and that they have metadata for the Vehicle Plate that matches the vehicle that is bound to this particular service flow. The Loss Assessment process we saw earlier then invokes the Tender for Estimates Client Side Human Service . In the Coach layout a reusable View named Vehicle Documents (found in Denim Compute Auto Claims - Common Toolkit) is used and has the documents that were retrieved bound to it. In Vehicle Documents a Viewer control from the Content Management Toolkit is used to display the document content as shown here. Moving now to the second pattern, in the Initiate Claims Processing process one of the last activities is to store the JSON representation of the complex BO structure in the Case folder by invoking the service flow Write AutoClaim JSON Output . In Write AutoClaim JSON Output (within Denim Compute Auto Claims Toolkit) the steps include a call to a Content Integration step using the Create document operation. A previous step Convert to JSON is used to parse the AutoClaim BO and convert it into native JS equivalents and from that to extract the JSON formatted output.","title":"Process services"},{"location":"development/process-services/#process-services","text":"This section provides details on service flow implementation chains of process activities. To understand the processes themselves that this section builds upon, please see Process flow control .","title":"Process services"},{"location":"development/process-services/#process-message-exchange","text":"Processes communicate asynchronously via message events whereby one process will send an outbound message and the other receives it. In the figure below we see an example of this pattern whereby the sending process ( Signal Adjuster Report Created ) on the left side and the receiving process ( Initiate Claims Processing ) on the right side reference the same message Under Cover Agent (UCA). In the sending process, the outbound message event has Data Mapping setup with a mapping to the correlating data plus the event payload. The correlating data is important because there is a need to identify the specific Initiate Claims Processing process instance that is awaiting the response from Signale Adjuster Report Created . Here on the other side of the exchange you can see that the event data's claimCorrelationId is designated as the correlation variable and is used then to compare to the claimCaseId values of instances in order to match and resume processing in the correct instance.","title":"Process message exchange"},{"location":"development/process-services/#case-properties-synchronization","text":"In order for Case properties to be referenced inside processes they need to be retrieved. Inside the process, an AutoClaim Business Object is modeled which represents those data items of interest to the process, a large majority of which are also Case properties. A framework of services has been developed to allow for obtaining Case properties and setting them on the various levels within the complex Business Object (BO) structure (and updating them as we will cover later in this section). The principle is that each BO within a higher level complex structure has a mapping service such as illustrated in the below Map Vehicle Input (which is inside the Denim Compute Auto Claim Toolkit ) for a vehicle object. That service in turn calls a generic service Map BO Input to perform the mapping. The data mapping on the service accepts as input an ANY type representing the BO to populate and then returns it populated with data from the corresponding Case properties. The other key input is a String list representing the set of property to attribute mappings. In the Set Vehicle Mappings script, the CSV list is created. This list has an entry for each target attribute in the BO and passes in for each entry a formatted string, an example of which is highlighted. To take that value DENI1_VehicleMake:make:String the first part represents the unique name of the case property, the second is the BO attribute name, and the third is the data type. Here is the corresponding case property named Vehicle Make showing where the unique identifier comes from inside Case Builder . The Map BO Input service is shown here, it makes calls to a number of helper services in order to interact with Case using the JavaScript API. Returning to the mapping, the higher level AutoClaim BO is mapped similarly in a service Map AutoClaim Input and also it delegates to lower level services (such as Map Vehicle Input shown earlier) to perform the mappings of the contained BOs within it. When data is required to be updated in Case the pattern is to set the inputs on and invoke a generic service Write Case Properties as illustrated in the service Update Claim Settlement Details (found in the Denim Compute Auto Claims Toolkit) shown here. The Map Properties script sets the casePropertyUpdateList variable used as the input mapping on the target Write Case Properties service.","title":"Case properties synchronization"},{"location":"development/process-services/#odm-integration","text":"BAW integration to ODM is by using a REST External Service . The service includes the three ODM operations used in the scenario, it is named Denim ODM Service and found in the Denim Compute Decision Services Toolkit Toolkit shown here: An example usage of the External Service is in Perform Claim Segmentation within Denim Compute Auto Claim Toolkit Toolkit. Note in the scenario that there is a boundary error event handler in case the ODM service is unavailable and this then sets the outputs to follow a default path. The inputs to the ODM operation are set from case properties using the pattern already described in the Case properties synchronization sub-section above.","title":"ODM integration"},{"location":"development/process-services/#ecm-integration","text":"BAW uses two main patterns to integrate to ECM, first it has to read existing documents from ECM in order to display them to the user inside client-side human service coaches and second it has to write a new JSON document to ECM with the complex data structures populated within BPM. For the first pattern a service flow named Get Claim ECM Documents in Denim Compute Auto Claims Toolkit Toolkit is called from the Loss Assessment process as shown here. In the Data Mapping inputs to that service we provide the claimCaseId and claimSubfolderPath which are used to find the Case ECM folder and the sub folder within it of interest (in this situation that is Damages Evidence ). Here is the Get Claim ECM Documents service flow where it uses the Content Integration feature to makes calls to Get Folder by Path and Get Documents in Folder . It then filters the returned documents to ensure that they match the Document Class of interest and that they have metadata for the Vehicle Plate that matches the vehicle that is bound to this particular service flow. The Loss Assessment process we saw earlier then invokes the Tender for Estimates Client Side Human Service . In the Coach layout a reusable View named Vehicle Documents (found in Denim Compute Auto Claims - Common Toolkit) is used and has the documents that were retrieved bound to it. In Vehicle Documents a Viewer control from the Content Management Toolkit is used to display the document content as shown here. Moving now to the second pattern, in the Initiate Claims Processing process one of the last activities is to store the JSON representation of the complex BO structure in the Case folder by invoking the service flow Write AutoClaim JSON Output . In Write AutoClaim JSON Output (within Denim Compute Auto Claims Toolkit) the steps include a call to a Content Integration step using the Create document operation. A previous step Convert to JSON is used to parse the AutoClaim BO and convert it into native JS equivalents and from that to extract the JSON formatted output.","title":"ECM integration"},{"location":"development/process-user-interface/","text":"Process User Interface Referencing views The BPM UI API provides a number of ways to reference other controls (Views) depending on the context. The most simple example is to reference a control from itself: Next when you want to reference another control inside an event of a control you use the ${\\ controlId\\ } notation: You can use a shortcut notation of getSibling() to refer to a control at same level (i.e. inside the same view): From inside a Table column you need to use ${../\\ controlId\\ } to navigate up to reference controls at the same level as the Table (this principle also applies from inside a view to reference things that are siblings of the view on the parent Coach): (Note the above also shows an example of using internal Table shortcut references \u2013 in this case to reference a control from same row with ${txtSKU=} notation). To reference controls that are on the Coach (the page) you use page.ui.get() : To reference controls inside a view\u2019s logic use this.ui.get() : To reference view logic (functions) from a contained control inside a view use view.xxx : Using formulas The BPM UI API provides formulas where you reference other fields and when those fields change the formula is automatically recalculated. An example of that is in the Create Settlement Offer Coach where the Claim Settlement Amount is a calculated field that uses a formula to refer to other fields. Any time one of those referenced fields changes then the formula is re-evaluated: However sometimes setting a formula does not meet the requirements and you have to adopt an alternative approach. Here is an example of trying to set a formula that references the selected rows in some tables: The first issue is that it gives errors because the getSelectedRecord() at load returns null: The second issue (and the critical one in this case) is that an automatic update does not seem to get triggered when you select the row in the table. To work around this we take a different approach were we will use a function to perform the calculation and set the target field and place this function in a reusable view that we then add to the Coach: The view function is shown here: Now then the table row is selected we can call the view\u2019s function from the event: View events examples Events on a view (control) are the key to detecting a change and reacting to it (e.g. a data field is set or updated) with BPM UI API calls. A typical event is an on click of a button \u2013 here is an example of invoking a view\u2019s function and passing in as an argument a reference to another control\u2019s bound data property: Here is an example of reacting to a data change event and it also shows you can put multiple statements in the implementation (you can even add arbitrary JS logic like if() {} else {} ): Her is an example of detecting a table row selection: Table view API manipulation The BPM UI Table view API has a number of methods, snippets of the ones that were used most regularly in the processing are shown here: Editing table rows with Flyout The BPM UI Samples (and Salient Spark site Table control referencing a demo https://www.youtube.com/watch?v=ZxnvDCDY8CA ) recommend using a view inside a table column that edits a row (and uses Deferred Sections for lazy loading). The key to this is that the Modal section is placed within a table column and bound to the list\u2019s currentItem : While this allows for editing the table row, it makes direct edits to the underlying row and does not preserve the existing value. In our scenario we want to provide an enhanced user experience so they can Edit a row and then decide whether to commit those changes or cancel them. To support this we can not use the approach of binding to the list\u2019s currentItem . Instead we create a separate local variable of type VehiclePart which represents the currently editable Part and bind that to the Modal that will include the Part editing fields: When Edit is clicked the event then calls an editPart() method defined in a reusable view and passes in the SKU for the selected row: Here is the editPart() method within Provide Repair Estimate Functions view that then sets the bound data on the mobile from the selected item in the Parts table plus it also sets a hidden field for the part index as we will need to reference this in the case of the user choosing the Save Changes option on the Modal: The Save Changes boundary event is then used to transition to a client-side script where the partsList is updated at the saved index (and if the user selects to Cancel we just close the Modal and do not make any updates to the partsList ): Putting reusable functions in a view Formulas etc on controls are great for one time actions but you often find that you want to do multiple things on an event and while you can add many statements separated by \u201c;\u201d in an event it becomes unwieldly and difficult to maintain. Also if you find yourself having to do a lot of chained behavior on different controls (e.g. a table updates so you have to check button visibility, plus popup a model, plus re-initialize a variable, etc) then it is hard to keep track. (Another example is for a Table with a requirement to switch Button visibility depending on row count \u2013 the Table\u2019s on rows loaded event worked for most cases but it did not fire when you programmatically called table.add() to add a new row). So we considered it better to have a function that does the multiple things and call that function. However if you use Custom HTML on a Coach then you can\u2019t reference that function in client-side scripts so you would end up with duplicating the logic (where you had to transition out of Coach and call the same behavioural logic). To get round this we created a view that has no content (well we put a hidden 100px Spacer on it so we could see it visually on the parent Coach canvas): And an example of referencing the functions: The downside is that the view with functions has to refer to control Id names that are volatile (if they change on Coach then the code inside the view functions will break): However you would be in a similar situation if you used Custom HTML as again you would have to edit the code for every change to a Control ID . Transitioning to client-side scripts While the ability to encapsulate all UI logic in a view is desirable to allow for maximum reuse, sometimes it is not possible to self-contain everything on the Coach or in a view \u2013 you have to transition out to a client-side script to complete the necessary logic. The most common situation is where you need to refer to a variable that is not bound to anything on the coach so it does not have any DOM representation that you can make BPM UI API calls (e.g. page.ui.get(\\ controlId\\ ) ) against. Here is an example from the Tender For Estimates Coach where the vehicleList.repairerList does not have any binding to a visual control and so it cannot be referenced in BPM UI API calls: Here is another example where there is a reference to duplicatePartWarning which has no binding on the Coach: (Note: duplicatePartWarning is a String bound to the Help section of a Modal Alert view from UI Toolkit that is added on the Coach and there is no exposed API method on it that lets you set the Help value thus the need to manipulate the duplicatePartWarning variable): Note in both of the above cases it may be possible to avoid these transitions if really necessary by putting hidden controls on the Coach that are bound to the variables that need to be manipulated. That way the BPM UI API can be used to access and update the bindings on those hidden controls. An other example is where the event is not exposed \u2013 such as in a Modal with buttons encapsulated within it so you cannot get access to the button events: You have to then react to the boundary event instead and transition to a client-side script : You may also be able to avoid this situation in a number of (increasing complexity in terms of coding) ways. The easiest solution might be to not encapsulate the buttons inside the Modal but have them directly on the Coach / view that needs to react to their events. An alternative, and more complex, option would be to expose the inner button events to the interface of the containing view. This requires using BPM UI Coach Framework APIs to detect the button events programmatically and to fire an event option on the parent view. Coach views loading order The loading order of the tree of views contained in a coach is inside out \u2013 a section with controls loads after all the contained controls inside it have loaded and so on. We utilise a top level Vertical Layout container in each Coach and do initialization things that reference multiple controls in the onLoad() event of it \u2013 that way we can ensure all dependent controls are already loaded and can be referenced with page.ui.get(\\ controlId\\ ) / ${\\ controlId\\ } etc. Here is an example with the outermost Vertical Layout shown and in there it calls an embedded view with the JS functions to perform checking whether to enable the Complete button or not: Deferred section views and asynchronous loading A common pattern when using a Modal popup is to defer the loading of it until needed (especially if it is complex and put within the column of a table) using the Deferred Section (DS) view. You have to be very careful about the loading order \u2013 you will get errors showing up in the browser debugger such as \u201c page.ui.get(\\ controlId>) not defined \u201d if you are trying to reference DOM items that are not loaded yet. A way to avoid such issues is to encapsulate the actions in a function and ensure it is called only when the DS has loaded. Here is a view with a DS that inside has a Modal and a Table and other controls: An initializing function ( showModal() below) is called from parent Coach (say from a button click event) that checks whether the DS is loaded and if not it calls lazyLoad() on it otherwise it calls the function that prepares the view by setting visibility etc: This same prepareView() function should be called only when the DS has lazy-loaded: Forcing boundary event triggers Sometimes you want to react to an event on a control by executing some client-side script logic but the control does not have its own boundary event that fits the need. So you add a Navigation Event control: And in the source for the event (in this example the return from a Service Call ) you call fire() on it: Which then lets you transition out to the client-side script : Map control reacting to binding changes We use a BPM UI Map view in the Tender for Estimates Coach to show the location of a Vehicle . The BPM UI Map view has configuration options for the latitude / longitude settings: While these work on first load of the Map inside the popup Modal on the Coach, subsequent changing of these values (for example when the selected Vehicle is changed) did not trigger an update of the location shown on the Map. To get around this we need to use a specific Map API method. First let\u2019s look at the sequence of events, When a Vehicle is selected the ZIP is passed to a Service Call view ( Get Coordinates ) which invokes a service to translate the ZIP into Geo coordinates. On return from that Service Call we then need to force a transition and that is done by in turn calling the fire() method of a Navigation Event view: The Navigation Event then provides us a boundary event which we can use to transition to the Show Map client-side script : And in the script (among other things) we have to explicitly call the setCenter() method on the Map to ensure it refreshes: Note that while this results in the Map correctly showing the new coordinates, however the Marker is not shown (it may be possible to set this with the method https://support.salientprocess.com/docs/enterprise/Map.html#addMarker but we did not check this as it was not vital to the scenario). Access view configuration option complex BO There is a defined way to navigate through the structure of a complex BO ( Business Object ) bound as a configuration option of a view. In this case you get the top level BO ( searchableVehicleParts ) and then a property that is a list ( partsList ): Then a further call uses the index to get a specific entry from the list and then reference attributes of that entry (an instance of VehiclePart BO):","title":"Process user interface"},{"location":"development/process-user-interface/#process-user-interface","text":"","title":"Process User Interface"},{"location":"development/process-user-interface/#referencing-views","text":"The BPM UI API provides a number of ways to reference other controls (Views) depending on the context. The most simple example is to reference a control from itself: Next when you want to reference another control inside an event of a control you use the ${\\ controlId\\ } notation: You can use a shortcut notation of getSibling() to refer to a control at same level (i.e. inside the same view): From inside a Table column you need to use ${../\\ controlId\\ } to navigate up to reference controls at the same level as the Table (this principle also applies from inside a view to reference things that are siblings of the view on the parent Coach): (Note the above also shows an example of using internal Table shortcut references \u2013 in this case to reference a control from same row with ${txtSKU=} notation). To reference controls that are on the Coach (the page) you use page.ui.get() : To reference controls inside a view\u2019s logic use this.ui.get() : To reference view logic (functions) from a contained control inside a view use view.xxx :","title":"Referencing views"},{"location":"development/process-user-interface/#using-formulas","text":"The BPM UI API provides formulas where you reference other fields and when those fields change the formula is automatically recalculated. An example of that is in the Create Settlement Offer Coach where the Claim Settlement Amount is a calculated field that uses a formula to refer to other fields. Any time one of those referenced fields changes then the formula is re-evaluated: However sometimes setting a formula does not meet the requirements and you have to adopt an alternative approach. Here is an example of trying to set a formula that references the selected rows in some tables: The first issue is that it gives errors because the getSelectedRecord() at load returns null: The second issue (and the critical one in this case) is that an automatic update does not seem to get triggered when you select the row in the table. To work around this we take a different approach were we will use a function to perform the calculation and set the target field and place this function in a reusable view that we then add to the Coach: The view function is shown here: Now then the table row is selected we can call the view\u2019s function from the event:","title":"Using formulas"},{"location":"development/process-user-interface/#view-events-examples","text":"Events on a view (control) are the key to detecting a change and reacting to it (e.g. a data field is set or updated) with BPM UI API calls. A typical event is an on click of a button \u2013 here is an example of invoking a view\u2019s function and passing in as an argument a reference to another control\u2019s bound data property: Here is an example of reacting to a data change event and it also shows you can put multiple statements in the implementation (you can even add arbitrary JS logic like if() {} else {} ): Her is an example of detecting a table row selection:","title":"View events examples"},{"location":"development/process-user-interface/#table-view-api-manipulation","text":"The BPM UI Table view API has a number of methods, snippets of the ones that were used most regularly in the processing are shown here:","title":"Table view API manipulation"},{"location":"development/process-user-interface/#editing-table-rows-with-flyout","text":"The BPM UI Samples (and Salient Spark site Table control referencing a demo https://www.youtube.com/watch?v=ZxnvDCDY8CA ) recommend using a view inside a table column that edits a row (and uses Deferred Sections for lazy loading). The key to this is that the Modal section is placed within a table column and bound to the list\u2019s currentItem : While this allows for editing the table row, it makes direct edits to the underlying row and does not preserve the existing value. In our scenario we want to provide an enhanced user experience so they can Edit a row and then decide whether to commit those changes or cancel them. To support this we can not use the approach of binding to the list\u2019s currentItem . Instead we create a separate local variable of type VehiclePart which represents the currently editable Part and bind that to the Modal that will include the Part editing fields: When Edit is clicked the event then calls an editPart() method defined in a reusable view and passes in the SKU for the selected row: Here is the editPart() method within Provide Repair Estimate Functions view that then sets the bound data on the mobile from the selected item in the Parts table plus it also sets a hidden field for the part index as we will need to reference this in the case of the user choosing the Save Changes option on the Modal: The Save Changes boundary event is then used to transition to a client-side script where the partsList is updated at the saved index (and if the user selects to Cancel we just close the Modal and do not make any updates to the partsList ):","title":"Editing table rows with Flyout"},{"location":"development/process-user-interface/#putting-reusable-functions-in-a-view","text":"Formulas etc on controls are great for one time actions but you often find that you want to do multiple things on an event and while you can add many statements separated by \u201c;\u201d in an event it becomes unwieldly and difficult to maintain. Also if you find yourself having to do a lot of chained behavior on different controls (e.g. a table updates so you have to check button visibility, plus popup a model, plus re-initialize a variable, etc) then it is hard to keep track. (Another example is for a Table with a requirement to switch Button visibility depending on row count \u2013 the Table\u2019s on rows loaded event worked for most cases but it did not fire when you programmatically called table.add() to add a new row). So we considered it better to have a function that does the multiple things and call that function. However if you use Custom HTML on a Coach then you can\u2019t reference that function in client-side scripts so you would end up with duplicating the logic (where you had to transition out of Coach and call the same behavioural logic). To get round this we created a view that has no content (well we put a hidden 100px Spacer on it so we could see it visually on the parent Coach canvas): And an example of referencing the functions: The downside is that the view with functions has to refer to control Id names that are volatile (if they change on Coach then the code inside the view functions will break): However you would be in a similar situation if you used Custom HTML as again you would have to edit the code for every change to a Control ID .","title":"Putting reusable functions in a view"},{"location":"development/process-user-interface/#transitioning-to-client-side-scripts","text":"While the ability to encapsulate all UI logic in a view is desirable to allow for maximum reuse, sometimes it is not possible to self-contain everything on the Coach or in a view \u2013 you have to transition out to a client-side script to complete the necessary logic. The most common situation is where you need to refer to a variable that is not bound to anything on the coach so it does not have any DOM representation that you can make BPM UI API calls (e.g. page.ui.get(\\ controlId\\ ) ) against. Here is an example from the Tender For Estimates Coach where the vehicleList.repairerList does not have any binding to a visual control and so it cannot be referenced in BPM UI API calls: Here is another example where there is a reference to duplicatePartWarning which has no binding on the Coach: (Note: duplicatePartWarning is a String bound to the Help section of a Modal Alert view from UI Toolkit that is added on the Coach and there is no exposed API method on it that lets you set the Help value thus the need to manipulate the duplicatePartWarning variable): Note in both of the above cases it may be possible to avoid these transitions if really necessary by putting hidden controls on the Coach that are bound to the variables that need to be manipulated. That way the BPM UI API can be used to access and update the bindings on those hidden controls. An other example is where the event is not exposed \u2013 such as in a Modal with buttons encapsulated within it so you cannot get access to the button events: You have to then react to the boundary event instead and transition to a client-side script : You may also be able to avoid this situation in a number of (increasing complexity in terms of coding) ways. The easiest solution might be to not encapsulate the buttons inside the Modal but have them directly on the Coach / view that needs to react to their events. An alternative, and more complex, option would be to expose the inner button events to the interface of the containing view. This requires using BPM UI Coach Framework APIs to detect the button events programmatically and to fire an event option on the parent view.","title":"Transitioning to client-side scripts"},{"location":"development/process-user-interface/#coach-views-loading-order","text":"The loading order of the tree of views contained in a coach is inside out \u2013 a section with controls loads after all the contained controls inside it have loaded and so on. We utilise a top level Vertical Layout container in each Coach and do initialization things that reference multiple controls in the onLoad() event of it \u2013 that way we can ensure all dependent controls are already loaded and can be referenced with page.ui.get(\\ controlId\\ ) / ${\\ controlId\\ } etc. Here is an example with the outermost Vertical Layout shown and in there it calls an embedded view with the JS functions to perform checking whether to enable the Complete button or not:","title":"Coach views loading order"},{"location":"development/process-user-interface/#deferred-section-views-and-asynchronous-loading","text":"A common pattern when using a Modal popup is to defer the loading of it until needed (especially if it is complex and put within the column of a table) using the Deferred Section (DS) view. You have to be very careful about the loading order \u2013 you will get errors showing up in the browser debugger such as \u201c page.ui.get(\\ controlId>) not defined \u201d if you are trying to reference DOM items that are not loaded yet. A way to avoid such issues is to encapsulate the actions in a function and ensure it is called only when the DS has loaded. Here is a view with a DS that inside has a Modal and a Table and other controls: An initializing function ( showModal() below) is called from parent Coach (say from a button click event) that checks whether the DS is loaded and if not it calls lazyLoad() on it otherwise it calls the function that prepares the view by setting visibility etc: This same prepareView() function should be called only when the DS has lazy-loaded:","title":"Deferred section views and asynchronous loading"},{"location":"development/process-user-interface/#forcing-boundary-event-triggers","text":"Sometimes you want to react to an event on a control by executing some client-side script logic but the control does not have its own boundary event that fits the need. So you add a Navigation Event control: And in the source for the event (in this example the return from a Service Call ) you call fire() on it: Which then lets you transition out to the client-side script :","title":"Forcing boundary event triggers"},{"location":"development/process-user-interface/#map-control-reacting-to-binding-changes","text":"We use a BPM UI Map view in the Tender for Estimates Coach to show the location of a Vehicle . The BPM UI Map view has configuration options for the latitude / longitude settings: While these work on first load of the Map inside the popup Modal on the Coach, subsequent changing of these values (for example when the selected Vehicle is changed) did not trigger an update of the location shown on the Map. To get around this we need to use a specific Map API method. First let\u2019s look at the sequence of events, When a Vehicle is selected the ZIP is passed to a Service Call view ( Get Coordinates ) which invokes a service to translate the ZIP into Geo coordinates. On return from that Service Call we then need to force a transition and that is done by in turn calling the fire() method of a Navigation Event view: The Navigation Event then provides us a boundary event which we can use to transition to the Show Map client-side script : And in the script (among other things) we have to explicitly call the setCenter() method on the Map to ensure it refreshes: Note that while this results in the Map correctly showing the new coordinates, however the Marker is not shown (it may be possible to set this with the method https://support.salientprocess.com/docs/enterprise/Map.html#addMarker but we did not check this as it was not vital to the scenario).","title":"Map control reacting to binding changes"},{"location":"development/process-user-interface/#access-view-configuration-option-complex-bo","text":"There is a defined way to navigate through the structure of a complex BO ( Business Object ) bound as a configuration option of a view. In this case you get the top level BO ( searchableVehicleParts ) and then a property that is a list ( partsList ): Then a further call uses the index to get a specific entry from the list and then reference attributes of that entry (an instance of VehiclePart BO):","title":"Access view configuration option complex BO"},{"location":"development/sundries/","text":"DBA solution development findings About BAW Process and Case data integration BAW 19.0.0.1 offers JavaScript facilities (object and associated set of methods) to allow the data integration between Process and Case. These objects and methods allow to transfer case properties into the process and subsequently to save back into the case property data updated. In processes that implement case activities, you can interact with the JavaScript case operations through the new operations that have been added to the TWProcessInstance JavaScript API in Business Automation Workflow. Useful methods are: addCommentToParentActivity addCommentToParentCase completeParentCaseStage createCaseUsingSameCaseType createCaseUsingSpecifiedCaseType createDiscretionaryActivityInParentCase createDiscretionaryActivityInParentCaseWithWorkflowParams createParentCaseDiscretionaryActivityWithProps createSubfolderUnderParentCase getParentActivityPropertyNames getParentActivityPropertyValues getParentCaseCasePropertyNames getParentCaseCasePropertyValues getParentCaseStructure relateParentCase searchParentCaseActivities setParentActivityPropertyValues setParentCaseCasePropertyValues unrelateParentCase Case properties validation Case Manager delivers basic property validation OOTB. For instance, properties are validated according to type and whether they are required or not. Other more complex validation rules, with more complex logic require the implementation of scripts. The claims processing solution implements some of these script-based property validation to showcase the concept. About ODM Integrating multiple decision services based on a common BOM While ODM rulesets are grouped in a RuleApp deployment artifact, the corresponding service definitions are generated individually for each ruleset, as shown below: In our use case, we use one claim_processing RuleApp, and we have to generate three separate service definition files, one for each segment_claim , assess_fraud and review_escalation . Because all three decision services are sharing the same input object model , importing these separate service definition files in BAW will create three separate (but identical) versions of the business objects hierarchy. One solution, using e.g. the YAML definition files, is to rename each Request and Response definitions in the generated service definition files with a unique name, and then merge the multiple YAML files into a single one that reflects the services exposed by the RuleApp.","title":"Sundries"},{"location":"development/sundries/#dba-solution-development-findings","text":"","title":"DBA solution development findings"},{"location":"development/sundries/#about-baw","text":"","title":"About BAW"},{"location":"development/sundries/#process-and-case-data-integration","text":"BAW 19.0.0.1 offers JavaScript facilities (object and associated set of methods) to allow the data integration between Process and Case. These objects and methods allow to transfer case properties into the process and subsequently to save back into the case property data updated. In processes that implement case activities, you can interact with the JavaScript case operations through the new operations that have been added to the TWProcessInstance JavaScript API in Business Automation Workflow. Useful methods are: addCommentToParentActivity addCommentToParentCase completeParentCaseStage createCaseUsingSameCaseType createCaseUsingSpecifiedCaseType createDiscretionaryActivityInParentCase createDiscretionaryActivityInParentCaseWithWorkflowParams createParentCaseDiscretionaryActivityWithProps createSubfolderUnderParentCase getParentActivityPropertyNames getParentActivityPropertyValues getParentCaseCasePropertyNames getParentCaseCasePropertyValues getParentCaseStructure relateParentCase searchParentCaseActivities setParentActivityPropertyValues setParentCaseCasePropertyValues unrelateParentCase","title":"Process and Case data integration"},{"location":"development/sundries/#case-properties-validation","text":"Case Manager delivers basic property validation OOTB. For instance, properties are validated according to type and whether they are required or not. Other more complex validation rules, with more complex logic require the implementation of scripts. The claims processing solution implements some of these script-based property validation to showcase the concept.","title":"Case properties validation"},{"location":"development/sundries/#about-odm","text":"","title":"About ODM"},{"location":"development/sundries/#integrating-multiple-decision-services-based-on-a-common-bom","text":"While ODM rulesets are grouped in a RuleApp deployment artifact, the corresponding service definitions are generated individually for each ruleset, as shown below: In our use case, we use one claim_processing RuleApp, and we have to generate three separate service definition files, one for each segment_claim , assess_fraud and review_escalation . Because all three decision services are sharing the same input object model , importing these separate service definition files in BAW will create three separate (but identical) versions of the business objects hierarchy. One solution, using e.g. the YAML definition files, is to rename each Request and Response definitions in the generated service definition files with a unique name, and then merge the multiple YAML files into a single one that reflects the services exposed by the RuleApp.","title":"Integrating multiple decision services based on a common BOM"},{"location":"development/workflow-intro/","text":"Workflow development This section covers the BAW development items of interest. Before looking at this section, it is recommended to first be familiar with the workflow design . To follow along in this section, it helps to have the solution installed as described in the Deploy section. This section is sub-divided into the following chapters: Process flow control which covers the BPMN processes and how they co-ordinate the process logic. Process services which covers the process activities implemented as service flows . Process user interface which covers the user task implementations within a process that use client-side human services and coaches . Case activities which covers the case activity implementations. Case user interface which covers the user task implementations within case activities.","title":"Introduction"},{"location":"development/workflow-intro/#workflow-development","text":"This section covers the BAW development items of interest. Before looking at this section, it is recommended to first be familiar with the workflow design . To follow along in this section, it helps to have the solution installed as described in the Deploy section. This section is sub-divided into the following chapters: Process flow control which covers the BPMN processes and how they co-ordinate the process logic. Process services which covers the process activities implemented as service flows . Process user interface which covers the user task implementations within a process that use client-side human services and coaches . Case activities which covers the case activity implementations. Case user interface which covers the user task implementations within case activities.","title":"Workflow development"},{"location":"environment/create-vms/","text":"This chapter outlines how to create the private cloud cluster VMs, using VMware vSphere server as the sample virtualization platform. It covers VM creation for the RHEL and Ubuntu Linux distributions. Create RHEL VMs Go to the directory where you want your VM and right click to get the action menu. Select a creation type. Enter the name for the VM. Select the folder to use. Select a compute resource. Select a storage location. Select compatibility. Select a host operating system. You can select many operating systems. Here we are selecting RHEL. But, later you will see the screen shots when we selected Ubuntu for other VMs. Customize the hardware settings specifically CPU, Memory and Hard Disk size. Also, make sure the New Network is set to csplab. This shows you that you can select thick provisioning (all of the requested disk size is allocated) or thin provisioning (disk size is allocated when it is required). For ICP, its probably best to use thick provisioning especially for the Master nodes. Click the Finish button to begin VM creation. View the newly created VM. Install RHEL Once you boot the VM for the first time, you will load the RHEL operating system. Review this link for a sample of how to do this. Select Redhat. Select the operating system version. Select the language. Click on the Installation Destination because you want to make sure that you define the file system sizes recommended for the ICP node you are creating. Select the I will configure partitioning radio button and then click the Done button. Click the + button and add each file system individually. Depending on what type of ICP node you are provisionng, you can refer to this link for file system sizings. Once all of the file systems are defined, click the Done button. Click the Begin Installation button. Enter the root password and create an admin account. Here you should create the admin account. Once the admin account is entered, click the Done button. Click the Reboot button. Once the VM is rebooted, you can log into it using the IP address shown in the Summary page. Check the filesystem sizes with the df -h command. Create Ubuntu VMs Now that the VM is created, you can start it. Once the VM is started, you should open the VM. This will allow you to finish installing the operating system. Create VM from template You can convert a VM (if it stopped) to a template. If you are creating multiple Worker nodes for instance, its best to convert one VM to a template and clone all of your VMs from the template. To make a template, right click the VM to get the action menu where you can do this. Once you have a template, you can clone a VM with it. You right click the template definition and clone to do this. Enter the VM name and select a location for the VM. Select a compute resource. Select the storage location. On the Select Clone Options just click the Next button. Click the Finish button. Change the network type You can view your newly created VM. Notice that the network is still set to VM Network and not csplab. Right click the VM definition to get the Action menu. Select Edit Settings. Change Network Adapter 1 to csplab. Post-install Change hostname This command works for RHEL and Ubuntu: hostnamectl set-hostname name hostnamectl set-hostname dbamc-icp-proxy1.csplab.local Edit the hosts file Modify /etc/hosts so that the new hostname is added. Switch to static IP address On RHEL Determine which ifcfg configuration file to edit by running the command ip addr . Look for the entry that matches the current IP address, in this case, it's ens192 . Edit /etc/sysconfig/network-scripts/ifcfg-ens192 file to change these lines as follows: IPV6INIT=no BOOTPROTO=static Append these lines making sure you set IPADDR to the static IP address. PROXY_METHOD=none BROWSER_ONLY=no DEFROUTE=yes IPV4_FAILURE_FATAL=no IPV6_AUTOCONF=no IPV6_DEFROUTE=no IPV6_FAILURE_FATAL=no IPADDR= static IP Address PREFIX=16 GATEWAY=172.16.255.250 DNS1=172.16.0.11 DNS2=172.16.0.17 DOMAIN=csplab.local On Ubuntu Edit /etc/network/interfaces and replace line (your iface value may be different): iface ens160 inet dhcp with the following lines, making sure the parameters (address, broadcast, gateway, etc.) match your own system value: iface ens160 inet static address 172.16.52.220 netmask 255.255.0.0 broadcast 172.16.255.255 gateway 172.16.255.250 dns-nameservers 172.16.0.11 172.16.0.17 dns-search csplab.local","title":"Creating VMs"},{"location":"environment/create-vms/#create-rhel-vms","text":"Go to the directory where you want your VM and right click to get the action menu. Select a creation type. Enter the name for the VM. Select the folder to use. Select a compute resource. Select a storage location. Select compatibility. Select a host operating system. You can select many operating systems. Here we are selecting RHEL. But, later you will see the screen shots when we selected Ubuntu for other VMs. Customize the hardware settings specifically CPU, Memory and Hard Disk size. Also, make sure the New Network is set to csplab. This shows you that you can select thick provisioning (all of the requested disk size is allocated) or thin provisioning (disk size is allocated when it is required). For ICP, its probably best to use thick provisioning especially for the Master nodes. Click the Finish button to begin VM creation. View the newly created VM.","title":"Create RHEL VMs"},{"location":"environment/create-vms/#install-rhel","text":"Once you boot the VM for the first time, you will load the RHEL operating system. Review this link for a sample of how to do this. Select Redhat. Select the operating system version. Select the language. Click on the Installation Destination because you want to make sure that you define the file system sizes recommended for the ICP node you are creating. Select the I will configure partitioning radio button and then click the Done button. Click the + button and add each file system individually. Depending on what type of ICP node you are provisionng, you can refer to this link for file system sizings. Once all of the file systems are defined, click the Done button. Click the Begin Installation button. Enter the root password and create an admin account. Here you should create the admin account. Once the admin account is entered, click the Done button. Click the Reboot button. Once the VM is rebooted, you can log into it using the IP address shown in the Summary page. Check the filesystem sizes with the df -h command.","title":"Install RHEL"},{"location":"environment/create-vms/#create-ubuntu-vms","text":"Now that the VM is created, you can start it. Once the VM is started, you should open the VM. This will allow you to finish installing the operating system.","title":"Create Ubuntu VMs"},{"location":"environment/create-vms/#create-vm-from-template","text":"You can convert a VM (if it stopped) to a template. If you are creating multiple Worker nodes for instance, its best to convert one VM to a template and clone all of your VMs from the template. To make a template, right click the VM to get the action menu where you can do this. Once you have a template, you can clone a VM with it. You right click the template definition and clone to do this. Enter the VM name and select a location for the VM. Select a compute resource. Select the storage location. On the Select Clone Options just click the Next button. Click the Finish button.","title":"Create VM from template"},{"location":"environment/create-vms/#change-the-network-type","text":"You can view your newly created VM. Notice that the network is still set to VM Network and not csplab. Right click the VM definition to get the Action menu. Select Edit Settings. Change Network Adapter 1 to csplab.","title":"Change the network type"},{"location":"environment/create-vms/#post-install","text":"","title":"Post-install"},{"location":"environment/create-vms/#change-hostname","text":"This command works for RHEL and Ubuntu: hostnamectl set-hostname name hostnamectl set-hostname dbamc-icp-proxy1.csplab.local","title":"Change hostname"},{"location":"environment/create-vms/#edit-the-hosts-file","text":"Modify /etc/hosts so that the new hostname is added.","title":"Edit the hosts file"},{"location":"environment/create-vms/#switch-to-static-ip-address","text":"","title":"Switch to static IP address"},{"location":"environment/create-vms/#on-rhel","text":"Determine which ifcfg configuration file to edit by running the command ip addr . Look for the entry that matches the current IP address, in this case, it's ens192 . Edit /etc/sysconfig/network-scripts/ifcfg-ens192 file to change these lines as follows: IPV6INIT=no BOOTPROTO=static Append these lines making sure you set IPADDR to the static IP address. PROXY_METHOD=none BROWSER_ONLY=no DEFROUTE=yes IPV4_FAILURE_FATAL=no IPV6_AUTOCONF=no IPV6_DEFROUTE=no IPV6_FAILURE_FATAL=no IPADDR= static IP Address PREFIX=16 GATEWAY=172.16.255.250 DNS1=172.16.0.11 DNS2=172.16.0.17 DOMAIN=csplab.local","title":"On RHEL"},{"location":"environment/create-vms/#on-ubuntu","text":"Edit /etc/network/interfaces and replace line (your iface value may be different): iface ens160 inet dhcp with the following lines, making sure the parameters (address, broadcast, gateway, etc.) match your own system value: iface ens160 inet static address 172.16.52.220 netmask 255.255.0.0 broadcast 172.16.255.255 gateway 172.16.255.250 dns-nameservers 172.16.0.11 172.16.0.17 dns-search csplab.local","title":"On Ubuntu"},{"location":"environment/install-bai/","text":"This chapter details the install activities for the BAI component and the associated emitters for BPMN, BPEL, Case Management and ODM. Installing the BPMN event emitter This section provides a playback of the IBM Knowledge Center BAW emitter install documentation steps. Before starting on your install, make sure you have the following node information available: IBM Event Streams bootstrap host IP and port (e.g. 172.16.52.216:32254 ) IBM Event Streams broker-[0..n] host and port IBM Event Streams topicName, e.g. event-streams-test1-ibm-bai-ingress IBM Event Streams console URL BAI Kibana UI URL You will also need the credentials for the BPM deployment environment administrator. Next, check the install prerequisites, which are listed here . Run the EnableBAI script The documentation instructions for this task are here . The first step is to install the emitter application and create a set of JMS resources. Log in to the BAW deployment manager node and run the EnableBAI.py script. You will need to use the BPM deployment environment administrator credentials to run the script. cd /opt/IBM/BPM ./profiles/DmgrProfile/bin/wsadmin.sh -f ./BPM/Lombardi/tools/def/EnableBAI.py -enable You should see the following output: ADMA5016I: Installation of BPMEventEmitter_war_De1 started. ADMA5058I: Application and module versions are validated with versions of deployment targets. ADMA5005I: The application BPMEventEmitter_war_De1 is configured in the WebSphere Application Server repository. ADMA5005I: The application BPMEventEmitter_war_De1 is configured in the WebSphere Application Server repository. ADMA5081I: The bootstrap address for client module is configured in the WebSphere Application Server repository. ADMA5053I: The library references for the installed optional package are created. ADMA5005I: The application BPMEventEmitter_war_De1 is configured in the WebSphere Application Server repository. ADMA5001I: The application binaries are saved in /opt/IBM/BPM/profiles/DmgrProfile/wstemp/Script16bb8535ebd/workspace/cells/PCCell1/applications/BPMEventEmitter_war_De1.ear/BPMEventEmitter_war_De1.ear ADMA5005I: The application BPMEventEmitter_war_De1 is configured in the WebSphere Application Server repository. SECJ0400I: Successfully updated the application BPMEventEmitter_war_De1 with the appContextIDForSecurity information. ADMA5005I: The application BPMEventEmitter_war_De1 is configured in the WebSphere Application Server repository. ADMA5005I: The application BPMEventEmitter_war_De1 is configured in the WebSphere Application Server repository. ADMA5113I: Activation plan created successfully. ADMA5011I: The cleanup of the temp directory for application BPMEventEmitter_war_De1 is complete. ADMA5013I: Application BPMEventEmitter_war_De1 installed successfully. Saving Configuration Class loader order is changed to 'classes loaded with local class loader first'. Connection update skipped --------------------------------------------------------------- AdminNodeManagement: Synchronize the active nodes Usage: AdminNodeManagement.syncActiveNodes() Return: If the command is successfully invoked, a value of 1 is returned. --------------------------------------------------------------- Node1 Node2 Configuration synchronized. After the script execution completes, you will have: The BPMEventEmitter application installed: The monitorCF connection factory created: The monitorQueue created: The defAS activation spec is created: Once the script completes, you need to restart BAW. This step may take a while: the deployment manager node may be slow to stop, restart and communicate with the BAW cluster members. Prepare secure connection to IBM Event Streams The documentation instructions for this task are here . The steps are the following: Generate the API key Log in to the IBM Event Stream console as administrator. Connect to the cluster: Download the truststore and PEM certificate as shown below: Copy the truststore (e.g. es-cert.jks file) and PEM certificate (e.g. es-cert.pem file) in a folder (e.g. /opt/IBM/BPM/EventStreamKeyStore ) on the deployment manager node Import the certificates on the deployment manager by running the following command: /opt/IBM/BPM/java/bin/keytool -importcert -alias kafkaServer -keystore es-cert.jks -file es-cert.pem -noprompt Prepare secure connection for BPM event emitter Documentation instructions for this task are here . The steps are the following: Edit the configuration property file /opt/IBM/BPM/BPM/Lombardi/tools/def/BAIConfigure.properties DeploymentEnvironmentName is set to None as we only have one DE bootstrapServer s is set to the Event Stream bootstrap and 3 boot servers IP/port topicName is set to the kafka ingress topic that was configured when installing BAI bpmCellName is set to the name of the BAW BPM cell type is set to plain_ssl as this is the security configure by default on IBM Event Streams trustStore is set to the location of the trust store downloaded from IBM Event Stream trustStore.password is set to the IBM Event Stream trust store password username is set to token password is set to the API key generated from IBM Event Streams Below is an example of the configuration file: DeploymentEnvironmentName=None bootstrapServers=172.16.52.216:32254,172.16.52.216:31027,172.16.52.216:31571,172.16.52.216:30457 topicName=event-streams-test1-ibm-bai-ingress bpmCellName=PCCell1 type=plain_ssl # SSL trustStore=/opt/IBM/BPM/EventStreamKeyStore/es-cert.jks trustStore.password=password # Authentication username=token password=gbSA_jxUBeoVyqmvdoZvePoPWS_W1SaHOKmuQCk-xH93 Run the EnableBAI configuration script again as follows: cd /opt/IBM/BPM ./profiles/DmgrProfile/bin/wsadmin.sh -f ./BPM/Lombardi/tools/def/EnableBAI.py --update=connection --property=./BPM/Lombardi/tools/def/BAIConfigure.properties Start the BPM event emitter application. The log should show the following: [7/3/19 12:22:01:913 EDT] 000000a4 ConfigConnect I CWMCD1003I: The Kafka connection is created. [7/3/19 12:22:01:915 EDT] 000000a4 LifeCycleMana I CWMCD1014I: The BPMEventEmitter message-driven bean (MDB) is started. [7/3/19 12:22:01:936 EDT] 000000a4 ApplicationMg A WSVR0221I: Application started: BPMEventEmitter_war_De1 [7/3/19 12:22:01:937 EDT] 000000a4 CompositionUn A WSVR0191I: Composition unit WebSphere:cuname=BPMEventEmitter_war_De1 in BLA WebSphere:blaname=BPMEventEmitter_war_De1 started. Test the connection to BAI Test the install by running sample processes from the process portal. You should see the Workflow - Processes default dashboard in Kibana being updated, as shown below: Installing the Case event emitter TBD Installing the ODM event emitter Unlike for an on-prem install, where you have to manually configure the ODM emitter install, the emitter for ODM is installed as part of the ODM containers deployment through IBACC. You will need to have handy the information shown on the screen shot below during the ODM deployment.","title":"Installing BAI"},{"location":"environment/install-bai/#installing-the-bpmn-event-emitter","text":"This section provides a playback of the IBM Knowledge Center BAW emitter install documentation steps. Before starting on your install, make sure you have the following node information available: IBM Event Streams bootstrap host IP and port (e.g. 172.16.52.216:32254 ) IBM Event Streams broker-[0..n] host and port IBM Event Streams topicName, e.g. event-streams-test1-ibm-bai-ingress IBM Event Streams console URL BAI Kibana UI URL You will also need the credentials for the BPM deployment environment administrator. Next, check the install prerequisites, which are listed here .","title":"Installing the BPMN event emitter"},{"location":"environment/install-bai/#run-the-enablebai-script","text":"The documentation instructions for this task are here . The first step is to install the emitter application and create a set of JMS resources. Log in to the BAW deployment manager node and run the EnableBAI.py script. You will need to use the BPM deployment environment administrator credentials to run the script. cd /opt/IBM/BPM ./profiles/DmgrProfile/bin/wsadmin.sh -f ./BPM/Lombardi/tools/def/EnableBAI.py -enable You should see the following output: ADMA5016I: Installation of BPMEventEmitter_war_De1 started. ADMA5058I: Application and module versions are validated with versions of deployment targets. ADMA5005I: The application BPMEventEmitter_war_De1 is configured in the WebSphere Application Server repository. ADMA5005I: The application BPMEventEmitter_war_De1 is configured in the WebSphere Application Server repository. ADMA5081I: The bootstrap address for client module is configured in the WebSphere Application Server repository. ADMA5053I: The library references for the installed optional package are created. ADMA5005I: The application BPMEventEmitter_war_De1 is configured in the WebSphere Application Server repository. ADMA5001I: The application binaries are saved in /opt/IBM/BPM/profiles/DmgrProfile/wstemp/Script16bb8535ebd/workspace/cells/PCCell1/applications/BPMEventEmitter_war_De1.ear/BPMEventEmitter_war_De1.ear ADMA5005I: The application BPMEventEmitter_war_De1 is configured in the WebSphere Application Server repository. SECJ0400I: Successfully updated the application BPMEventEmitter_war_De1 with the appContextIDForSecurity information. ADMA5005I: The application BPMEventEmitter_war_De1 is configured in the WebSphere Application Server repository. ADMA5005I: The application BPMEventEmitter_war_De1 is configured in the WebSphere Application Server repository. ADMA5113I: Activation plan created successfully. ADMA5011I: The cleanup of the temp directory for application BPMEventEmitter_war_De1 is complete. ADMA5013I: Application BPMEventEmitter_war_De1 installed successfully. Saving Configuration Class loader order is changed to 'classes loaded with local class loader first'. Connection update skipped --------------------------------------------------------------- AdminNodeManagement: Synchronize the active nodes Usage: AdminNodeManagement.syncActiveNodes() Return: If the command is successfully invoked, a value of 1 is returned. --------------------------------------------------------------- Node1 Node2 Configuration synchronized. After the script execution completes, you will have: The BPMEventEmitter application installed: The monitorCF connection factory created: The monitorQueue created: The defAS activation spec is created: Once the script completes, you need to restart BAW. This step may take a while: the deployment manager node may be slow to stop, restart and communicate with the BAW cluster members.","title":"Run the EnableBAI script"},{"location":"environment/install-bai/#prepare-secure-connection-to-ibm-event-streams","text":"The documentation instructions for this task are here . The steps are the following: Generate the API key Log in to the IBM Event Stream console as administrator. Connect to the cluster: Download the truststore and PEM certificate as shown below: Copy the truststore (e.g. es-cert.jks file) and PEM certificate (e.g. es-cert.pem file) in a folder (e.g. /opt/IBM/BPM/EventStreamKeyStore ) on the deployment manager node Import the certificates on the deployment manager by running the following command: /opt/IBM/BPM/java/bin/keytool -importcert -alias kafkaServer -keystore es-cert.jks -file es-cert.pem -noprompt","title":"Prepare secure connection to IBM Event Streams"},{"location":"environment/install-bai/#prepare-secure-connection-for-bpm-event-emitter","text":"Documentation instructions for this task are here . The steps are the following: Edit the configuration property file /opt/IBM/BPM/BPM/Lombardi/tools/def/BAIConfigure.properties DeploymentEnvironmentName is set to None as we only have one DE bootstrapServer s is set to the Event Stream bootstrap and 3 boot servers IP/port topicName is set to the kafka ingress topic that was configured when installing BAI bpmCellName is set to the name of the BAW BPM cell type is set to plain_ssl as this is the security configure by default on IBM Event Streams trustStore is set to the location of the trust store downloaded from IBM Event Stream trustStore.password is set to the IBM Event Stream trust store password username is set to token password is set to the API key generated from IBM Event Streams Below is an example of the configuration file: DeploymentEnvironmentName=None bootstrapServers=172.16.52.216:32254,172.16.52.216:31027,172.16.52.216:31571,172.16.52.216:30457 topicName=event-streams-test1-ibm-bai-ingress bpmCellName=PCCell1 type=plain_ssl # SSL trustStore=/opt/IBM/BPM/EventStreamKeyStore/es-cert.jks trustStore.password=password # Authentication username=token password=gbSA_jxUBeoVyqmvdoZvePoPWS_W1SaHOKmuQCk-xH93 Run the EnableBAI configuration script again as follows: cd /opt/IBM/BPM ./profiles/DmgrProfile/bin/wsadmin.sh -f ./BPM/Lombardi/tools/def/EnableBAI.py --update=connection --property=./BPM/Lombardi/tools/def/BAIConfigure.properties Start the BPM event emitter application. The log should show the following: [7/3/19 12:22:01:913 EDT] 000000a4 ConfigConnect I CWMCD1003I: The Kafka connection is created. [7/3/19 12:22:01:915 EDT] 000000a4 LifeCycleMana I CWMCD1014I: The BPMEventEmitter message-driven bean (MDB) is started. [7/3/19 12:22:01:936 EDT] 000000a4 ApplicationMg A WSVR0221I: Application started: BPMEventEmitter_war_De1 [7/3/19 12:22:01:937 EDT] 000000a4 CompositionUn A WSVR0191I: Composition unit WebSphere:cuname=BPMEventEmitter_war_De1 in BLA WebSphere:blaname=BPMEventEmitter_war_De1 started.","title":"Prepare secure connection for BPM event emitter"},{"location":"environment/install-bai/#test-the-connection-to-bai","text":"Test the install by running sample processes from the process portal. You should see the Workflow - Processes default dashboard in Kibana being updated, as shown below:","title":"Test the connection to BAI"},{"location":"environment/install-bai/#installing-the-case-event-emitter","text":"TBD","title":"Installing the Case event emitter"},{"location":"environment/install-bai/#installing-the-odm-event-emitter","text":"Unlike for an on-prem install, where you have to manually configure the ODM emitter install, the emitter for ODM is installed as part of the ODM containers deployment through IBACC. You will need to have handy the information shown on the screen shot below during the ODM deployment.","title":"Installing the ODM event emitter"},{"location":"environment/install-baw/","text":"This chapter details the install activities for the BAW component. Software download Download BAW 18.0.0.1 (part number CNTA0ML, CNTA1ML, and CNTA2ML). Download the BAW 19.0.0.1 delta. Download the WAS 8.5.5.15 fixpack. Installation The IBM Knowledge Center BAW install documentation can be found here . Linux settings Add the following content in the file /etc/security/limits.conf on all servers: # - stack - maximum stack size (KB) root soft stack 32768 root hard stack 32768 # - nofile - maximum number of open files root soft nofile 65536 root hard nofile 65536 # - nproc - maximum number of processes root soft nproc 16384 root hard nproc 16384 # - fsize - maximum file size root soft fsize 6291453 root hard fsize 6291453 Execute the following commands on all the servers: echo 3000 /proc/sys/net/core/netdev_max_backlog echo 3000 /proc/sys/net/core/somaxconn echo 15 /proc/sys/net/ipv4/tcp_keepalive_intvl echo 5 /proc/sys/net/ipv4/tcp_keepalive_probes Install software Unpack the installation files: gunzip BAW_18_0_0_1_Linux_x86_1_of_3.tar.gz gunzip BAW_18_0_0_1_Linux_x86_2_of_3.tar.gz gunzip BAW_18_0_0_1_Linux_x86_3_of_3.tar.gz tar xvf BAW_18_0_0_1_Linux_x86_1_of_3.tar tar xvf BAW_18_0_0_1_Linux_x86_2_of_3.tar tar xvf BAW_18_0_0_1_Linux_x86_3_of_3.tar unzip 8.5.5-WS-WAS-FP015-part1.zip unzip 8.5.5-WS-WAS-FP015-part2.zip unzip 8.5.5-WS-WAS-FP015-part3.zip Install the Installation Manager: cd /downloads/BAW18001/IM64/tools ./imcl install com.ibm.cic.agent -repositories /downloads/BAW18001/IM64/repository.config -installationDirectory /opt/ibm/IM/eclipse -showVerboseProgress -log IM_Installation.log -acceptLicense Install BAW cd /opt/ibm/IM/eclipse/tools/ # Install BAW 18.0.0.1 ./imcl install com.ibm.bpm.ADV.v85,WorkflowEnterprise.NonProduction com.ibm.websphere.ND.v85,core.feature,ejbdeploy,thinclient,embeddablecontainer,samples,com.ibm.sdk.6_64bit -acceptLicense -installationDirectory /opt/IBM/BPM -repositories /downloads/BAW18001/repository/repos_64bit -properties user.wasjava=java8 -showVerboseProgress -log silentinstall.log # Install BAW 19.0.0.1 and WAS 8.5.5.15 fix pack ./imcl install com.ibm.websphere.ND.v85 com.ibm.bpm.ADV.v85,WorkflowEnterprise.NonProduction -acceptLicense -installationDirectory /opt/IBM/BPM -repositories /downloads/WAS85515/repository.config,/downloads/BAW19001/workflow.19001.delta.repository.zip -properties user.wasjava=java8 -showVerboseProgress -log silent_update.txt Verify installation /opt/IBM/BPM/bin/versionInfo.sh -maintenancePackages Create shared folder Create a shared folder for Case Management on the NFS server (shared services VM): mkdir -p /data/casemanagement Mount the shared folder on all the servers: mount {shared_services_host}:/data/casemanagement /data/casemanagement Create profiles Make sure the property bpm.de.caseManager.networkSharedDirectory has been set to the shared folder /data/casemanagement # Login to the DE server cd /opt/IBM/BPM/bin ./BPMConfig.sh -create -de Advanced-PC-ThreeClusters-DB2.properties /opt/IBM/BPM/profiles/DmgrProfile/bin/startManager.sh # Login to the node servers cd /opt/IBM/BPM/bin ./BPMConfig.sh -create -de Advanced-PC-ThreeClusters-DB2.properties /opt/IBM/BPM/profiles/Node1Profile/bin/startNode.sh /opt/IBM/BPM/profiles/Node2Profile/bin/startNode.sh Create the database The database scripts can be found in /opt/IBM/BPM/profiles/DmgrProfile/dbscripts . db2 -stf CMNDB-Cell/createDatabase.sql db2 connect to CMNDB db2 -tvf CMNDB-Cell/createSchema_Advanced.sql db2 -tvf CMNDB/createSchema_Advanced.sql db2 -tvf CMNDB/createSchema_Messaging.sql db2 connect reset db2 -stf BPMDB/createDatabase.sql db2 connect to BPMDB db2 -tvf BPMDB/createSchema_Advanced.sql db2 -tdGO -vf BPMDB/createProcedure_Advanced.sql db2 connect reset db2 -stf PDWDB/createDatabase.sql db2 connect to PDWDB db2 -tvf PDWDB/createSchema_Advanced.sql db2 connect reset mkdir -p /home/db2inst1/db2inst1/CPEDB/DOSSA/datafs1 mkdir -p /home/db2inst1/db2inst1/CPEDB/DOSSA/datafs2 mkdir -p /home/db2inst1/db2inst1/CPEDB/DOSSA/datafs3 mkdir -p /home/db2inst1/db2inst1/CPEDB/DOSSA/indexfs1 mkdir -p /home/db2inst1/db2inst1/CPEDB/DOSSA/indexfs2 mkdir -p /home/db2inst1/db2inst1/CPEDB/DOSSA/lobfs1 mkdir -p /home/db2inst1/db2inst1/CPEDB/TOSSA/datafs1 mkdir -p /home/db2inst1/db2inst1/CPEDB/TOSSA/datafs2 mkdir -p /home/db2inst1/db2inst1/CPEDB/TOSSA/datafs3 mkdir -p /home/db2inst1/db2inst1/CPEDB/TOSSA/indexfs1 mkdir -p /home/db2inst1/db2inst1/CPEDB/TOSSA/indexfs2 mkdir -p /home/db2inst1/db2inst1/CPEDB/TOSSA/lobfs1 mkdir -p /home/db2inst1/db2inst1/CPEDB/sys mkdir -p /home/db2inst1/db2inst1/CPEDB/systmp mkdir -p /home/db2inst1/db2inst1/CPEDB/usr mkdir -p /home/db2inst1/db2inst1/CPEDB/log chmod -R 777 /home/db2inst1/db2inst1/CPEDB # replace @DB_DIR@ with /home/db2inst1/db2inst1 in CPEDB/createDatabase_ECM.sql and CPEDB/createTablespace_Advanced.sql db2 -stf CPEDB/createDatabase_ECM.sql db2 connect to CPEDB db2 -tvf CPEDB/createTablespace_Advanced.sql db2 connect reset db2 connect to CPEDB db2 -tvf CPEDB/createSchema_Advanced.sql db2 connect reset Bootstrap process server data /opt/IBM/BPM/profiles/DmgrProfile/bin/bootstrapProcessServerData.sh -clusterName AppCluster Create CM object store Create a group named caseAdmin in the WAS console, and assign deadmin to the group, then execute the following script: /opt/IBM/BPM/profiles/DmgrProfile/bin/wsadmin.sh -user deadmin -password deadmin -host dbamc-icp-ubuntu-baw3.csplab.local -port 8880 -lang jython print AdminTask.createObjectStoreForContent(['-clusterName', 'AppCluster', '-PEWorkflowSystemAdminGroup', 'caseAdmin','-creationUser','deadmin','-password','deadmin']) Case Management configuration Prepare access the Case Config Manager UI Install xrdp to be able remote desktop connection to the DMgr node. Commands for Ubuntu are the following: apt-get install xrdp apt-get install xfce4 echo xfce4-session ~/.xsession Remove the line ./etc/X11/Xsession from file /etc/xrdp/startwm.sh Add the line startxfce4 in file /etc/xrdp/startwm.sh Restart the xrdp service: service xrdp restart Update timeouts Update the timeout settings to at least 600 seconds: Servers Server Types WebSphere application servers Configuration tab Container Settings Container Services Transaction service Total transaction lifetime timeout Servers Server Types WebSphere application servers Configuration tab Container Settings Container Services Transaction service Maximum transaction lifetime timeout Servers Server Types WebSphere application servers Configuration tab Container Settings Container Services ORB service Request timeout Servers Server Types WebSphere application servers Configuration tab Container Settings Container Services ORB service Locate request timeout Resources JDBC Data sources [Content Engine or Case Manager data source name] Connection Pool properties Connection timeout Resources JDBC Data sources [Content Engine or Case Manager XA data source name] Connection Pool properties Connection timeout Run the Case ConfigManager Run /opt/IBM/BPM/CaseManagement/configure/configmgr through the remote desktop connection, then run all tasks in sequence according to this document : Register the Administration Console for Content Platform Engine (ACCE) Plug-in Configure the Case Management Object Stores Define the Default Project Area Configure Case Integration with IBM Business Automation Workflow Deploy the Content Platform Engine Workflow Service Register the IBM Business Automation Workflow Plug-in Register the Case Management Services Plug-in Register the Case Widgets Package Register the IBM Business Automation Workflow Case Administration Client Plug-in Register Project Area Configure Business Rules Register the Case Monitor Widgets Package Uninstall Drop databases su - db2inst1 db2 drop database CMNDB; db2 drop database BPMDB; db2 drop database PDWDB; rm -rf /home/db2inst1/db2inst1/CPEDB db2 drop database CPEDB; Stop DE and nodes /opt/IBM/BPM/profiles/Node1Profile/bin/stopNode.sh -username deadmin -password deadmin /opt/IBM/BPM/profiles/Node2Profile/bin/stopNode.sh -username deadmin -password deadmin /opt/IBM/BPM/profiles/DmgrProfile/bin/stopManager.sh -username deadmin -password deadmin Delete profiles on all servers cd /opt/IBM/BPM/bin ./BPMConfig.sh -delete -profiles Advanced-PC-ThreeClusters-DB2.properties rm -rf /opt/IBM/BPM/profiles Troubleshooting The error \"The plug-in JAR file was not found at the specified location\" occurs when the case management folder is not set to the network directory. To fix the issue, modify the configuration using the following commands (see this support document ). cd /opt/IBM/BPM/bin ./BPMConfig.sh -export -profile DmgrProfile -de De1 -outputDir bawconfig/ ./BPMConfig.sh -update -profile DmgrProfile -de De1 -component ContentNavigator -networkDirectory /data/casemanagement ./BPMConfig.sh -update -profile DmgrProfile -de De1 -component CaseManager -networkDirectory /data/casemanagement","title":"Installing BAW"},{"location":"environment/install-baw/#software-download","text":"Download BAW 18.0.0.1 (part number CNTA0ML, CNTA1ML, and CNTA2ML). Download the BAW 19.0.0.1 delta. Download the WAS 8.5.5.15 fixpack.","title":"Software download"},{"location":"environment/install-baw/#installation","text":"The IBM Knowledge Center BAW install documentation can be found here .","title":"Installation"},{"location":"environment/install-baw/#linux-settings","text":"Add the following content in the file /etc/security/limits.conf on all servers: # - stack - maximum stack size (KB) root soft stack 32768 root hard stack 32768 # - nofile - maximum number of open files root soft nofile 65536 root hard nofile 65536 # - nproc - maximum number of processes root soft nproc 16384 root hard nproc 16384 # - fsize - maximum file size root soft fsize 6291453 root hard fsize 6291453 Execute the following commands on all the servers: echo 3000 /proc/sys/net/core/netdev_max_backlog echo 3000 /proc/sys/net/core/somaxconn echo 15 /proc/sys/net/ipv4/tcp_keepalive_intvl echo 5 /proc/sys/net/ipv4/tcp_keepalive_probes","title":"Linux settings"},{"location":"environment/install-baw/#install-software","text":"Unpack the installation files: gunzip BAW_18_0_0_1_Linux_x86_1_of_3.tar.gz gunzip BAW_18_0_0_1_Linux_x86_2_of_3.tar.gz gunzip BAW_18_0_0_1_Linux_x86_3_of_3.tar.gz tar xvf BAW_18_0_0_1_Linux_x86_1_of_3.tar tar xvf BAW_18_0_0_1_Linux_x86_2_of_3.tar tar xvf BAW_18_0_0_1_Linux_x86_3_of_3.tar unzip 8.5.5-WS-WAS-FP015-part1.zip unzip 8.5.5-WS-WAS-FP015-part2.zip unzip 8.5.5-WS-WAS-FP015-part3.zip Install the Installation Manager: cd /downloads/BAW18001/IM64/tools ./imcl install com.ibm.cic.agent -repositories /downloads/BAW18001/IM64/repository.config -installationDirectory /opt/ibm/IM/eclipse -showVerboseProgress -log IM_Installation.log -acceptLicense Install BAW cd /opt/ibm/IM/eclipse/tools/ # Install BAW 18.0.0.1 ./imcl install com.ibm.bpm.ADV.v85,WorkflowEnterprise.NonProduction com.ibm.websphere.ND.v85,core.feature,ejbdeploy,thinclient,embeddablecontainer,samples,com.ibm.sdk.6_64bit -acceptLicense -installationDirectory /opt/IBM/BPM -repositories /downloads/BAW18001/repository/repos_64bit -properties user.wasjava=java8 -showVerboseProgress -log silentinstall.log # Install BAW 19.0.0.1 and WAS 8.5.5.15 fix pack ./imcl install com.ibm.websphere.ND.v85 com.ibm.bpm.ADV.v85,WorkflowEnterprise.NonProduction -acceptLicense -installationDirectory /opt/IBM/BPM -repositories /downloads/WAS85515/repository.config,/downloads/BAW19001/workflow.19001.delta.repository.zip -properties user.wasjava=java8 -showVerboseProgress -log silent_update.txt Verify installation /opt/IBM/BPM/bin/versionInfo.sh -maintenancePackages","title":"Install software"},{"location":"environment/install-baw/#create-shared-folder","text":"Create a shared folder for Case Management on the NFS server (shared services VM): mkdir -p /data/casemanagement Mount the shared folder on all the servers: mount {shared_services_host}:/data/casemanagement /data/casemanagement","title":"Create shared folder"},{"location":"environment/install-baw/#create-profiles","text":"Make sure the property bpm.de.caseManager.networkSharedDirectory has been set to the shared folder /data/casemanagement # Login to the DE server cd /opt/IBM/BPM/bin ./BPMConfig.sh -create -de Advanced-PC-ThreeClusters-DB2.properties /opt/IBM/BPM/profiles/DmgrProfile/bin/startManager.sh # Login to the node servers cd /opt/IBM/BPM/bin ./BPMConfig.sh -create -de Advanced-PC-ThreeClusters-DB2.properties /opt/IBM/BPM/profiles/Node1Profile/bin/startNode.sh /opt/IBM/BPM/profiles/Node2Profile/bin/startNode.sh","title":"Create profiles"},{"location":"environment/install-baw/#create-the-database","text":"The database scripts can be found in /opt/IBM/BPM/profiles/DmgrProfile/dbscripts . db2 -stf CMNDB-Cell/createDatabase.sql db2 connect to CMNDB db2 -tvf CMNDB-Cell/createSchema_Advanced.sql db2 -tvf CMNDB/createSchema_Advanced.sql db2 -tvf CMNDB/createSchema_Messaging.sql db2 connect reset db2 -stf BPMDB/createDatabase.sql db2 connect to BPMDB db2 -tvf BPMDB/createSchema_Advanced.sql db2 -tdGO -vf BPMDB/createProcedure_Advanced.sql db2 connect reset db2 -stf PDWDB/createDatabase.sql db2 connect to PDWDB db2 -tvf PDWDB/createSchema_Advanced.sql db2 connect reset mkdir -p /home/db2inst1/db2inst1/CPEDB/DOSSA/datafs1 mkdir -p /home/db2inst1/db2inst1/CPEDB/DOSSA/datafs2 mkdir -p /home/db2inst1/db2inst1/CPEDB/DOSSA/datafs3 mkdir -p /home/db2inst1/db2inst1/CPEDB/DOSSA/indexfs1 mkdir -p /home/db2inst1/db2inst1/CPEDB/DOSSA/indexfs2 mkdir -p /home/db2inst1/db2inst1/CPEDB/DOSSA/lobfs1 mkdir -p /home/db2inst1/db2inst1/CPEDB/TOSSA/datafs1 mkdir -p /home/db2inst1/db2inst1/CPEDB/TOSSA/datafs2 mkdir -p /home/db2inst1/db2inst1/CPEDB/TOSSA/datafs3 mkdir -p /home/db2inst1/db2inst1/CPEDB/TOSSA/indexfs1 mkdir -p /home/db2inst1/db2inst1/CPEDB/TOSSA/indexfs2 mkdir -p /home/db2inst1/db2inst1/CPEDB/TOSSA/lobfs1 mkdir -p /home/db2inst1/db2inst1/CPEDB/sys mkdir -p /home/db2inst1/db2inst1/CPEDB/systmp mkdir -p /home/db2inst1/db2inst1/CPEDB/usr mkdir -p /home/db2inst1/db2inst1/CPEDB/log chmod -R 777 /home/db2inst1/db2inst1/CPEDB # replace @DB_DIR@ with /home/db2inst1/db2inst1 in CPEDB/createDatabase_ECM.sql and CPEDB/createTablespace_Advanced.sql db2 -stf CPEDB/createDatabase_ECM.sql db2 connect to CPEDB db2 -tvf CPEDB/createTablespace_Advanced.sql db2 connect reset db2 connect to CPEDB db2 -tvf CPEDB/createSchema_Advanced.sql db2 connect reset","title":"Create the database"},{"location":"environment/install-baw/#bootstrap-process-server-data","text":"/opt/IBM/BPM/profiles/DmgrProfile/bin/bootstrapProcessServerData.sh -clusterName AppCluster","title":"Bootstrap process server data"},{"location":"environment/install-baw/#create-cm-object-store","text":"Create a group named caseAdmin in the WAS console, and assign deadmin to the group, then execute the following script: /opt/IBM/BPM/profiles/DmgrProfile/bin/wsadmin.sh -user deadmin -password deadmin -host dbamc-icp-ubuntu-baw3.csplab.local -port 8880 -lang jython print AdminTask.createObjectStoreForContent(['-clusterName', 'AppCluster', '-PEWorkflowSystemAdminGroup', 'caseAdmin','-creationUser','deadmin','-password','deadmin'])","title":"Create CM object store"},{"location":"environment/install-baw/#case-management-configuration","text":"","title":"Case Management configuration"},{"location":"environment/install-baw/#prepare-access-the-case-config-manager-ui","text":"Install xrdp to be able remote desktop connection to the DMgr node. Commands for Ubuntu are the following: apt-get install xrdp apt-get install xfce4 echo xfce4-session ~/.xsession Remove the line ./etc/X11/Xsession from file /etc/xrdp/startwm.sh Add the line startxfce4 in file /etc/xrdp/startwm.sh Restart the xrdp service: service xrdp restart","title":"Prepare access the Case Config Manager UI"},{"location":"environment/install-baw/#update-timeouts","text":"Update the timeout settings to at least 600 seconds: Servers Server Types WebSphere application servers Configuration tab Container Settings Container Services Transaction service Total transaction lifetime timeout Servers Server Types WebSphere application servers Configuration tab Container Settings Container Services Transaction service Maximum transaction lifetime timeout Servers Server Types WebSphere application servers Configuration tab Container Settings Container Services ORB service Request timeout Servers Server Types WebSphere application servers Configuration tab Container Settings Container Services ORB service Locate request timeout Resources JDBC Data sources [Content Engine or Case Manager data source name] Connection Pool properties Connection timeout Resources JDBC Data sources [Content Engine or Case Manager XA data source name] Connection Pool properties Connection timeout","title":"Update timeouts"},{"location":"environment/install-baw/#run-the-case-configmanager","text":"Run /opt/IBM/BPM/CaseManagement/configure/configmgr through the remote desktop connection, then run all tasks in sequence according to this document : Register the Administration Console for Content Platform Engine (ACCE) Plug-in Configure the Case Management Object Stores Define the Default Project Area Configure Case Integration with IBM Business Automation Workflow Deploy the Content Platform Engine Workflow Service Register the IBM Business Automation Workflow Plug-in Register the Case Management Services Plug-in Register the Case Widgets Package Register the IBM Business Automation Workflow Case Administration Client Plug-in Register Project Area Configure Business Rules Register the Case Monitor Widgets Package","title":"Run the Case ConfigManager"},{"location":"environment/install-baw/#uninstall","text":"","title":"Uninstall"},{"location":"environment/install-baw/#drop-databases","text":"su - db2inst1 db2 drop database CMNDB; db2 drop database BPMDB; db2 drop database PDWDB; rm -rf /home/db2inst1/db2inst1/CPEDB db2 drop database CPEDB;","title":"Drop databases"},{"location":"environment/install-baw/#stop-de-and-nodes","text":"/opt/IBM/BPM/profiles/Node1Profile/bin/stopNode.sh -username deadmin -password deadmin /opt/IBM/BPM/profiles/Node2Profile/bin/stopNode.sh -username deadmin -password deadmin /opt/IBM/BPM/profiles/DmgrProfile/bin/stopManager.sh -username deadmin -password deadmin","title":"Stop DE and nodes"},{"location":"environment/install-baw/#delete-profiles-on-all-servers","text":"cd /opt/IBM/BPM/bin ./BPMConfig.sh -delete -profiles Advanced-PC-ThreeClusters-DB2.properties rm -rf /opt/IBM/BPM/profiles","title":"Delete profiles on all servers"},{"location":"environment/install-baw/#troubleshooting","text":"The error \"The plug-in JAR file was not found at the specified location\" occurs when the case management folder is not set to the network directory. To fix the issue, modify the configuration using the following commands (see this support document ). cd /opt/IBM/BPM/bin ./BPMConfig.sh -export -profile DmgrProfile -de De1 -outputDir bawconfig/ ./BPMConfig.sh -update -profile DmgrProfile -de De1 -component ContentNavigator -networkDirectory /data/casemanagement ./BPMConfig.sh -update -profile DmgrProfile -de De1 -component CaseManager -networkDirectory /data/casemanagement","title":"Troubleshooting"},{"location":"environment/install-cam/","text":"Introduction IBM Cloud Automation Manager (CAM) is a cloud management solution in IBM Cloud Private for deploying cloud infrastructure in multiple clouds with an optimized user experience. Cloud Automation Manager uses open source Terraform to manage and deliver cloud infrastructure as code. Cloud infrastructure that is delivered as code is reusable, is able to be placed under version control, can be shared across distributed teams, and can be used to easily replicate environments. In relation to ICP4A, CAM is used to deploy IBM Business Automation Workflow (BAW) to traditional WebSphere nodes. Eventually, IBM BAW will be deployed as containers which will mean that CAM would not be used in this case. CAM will still be strategic for deploying any VM based offering. CAM will also soon support OpenShift. As long as there are WAS-ND based offerings, CAM will be used to deploy them. Even after IBM BAW is delivered in containers, customers will still want to or need to use VM (WAS-ND) based versions if they: Are using IID/BPEL content, which is almost 50% of Workflow clients They are dependent upon deprecated code which will be removed in the Container version of Workflow and can't yet refactor their apps. There are several steps to install CAM, described in the IBM CAM Knowledge Center documentation. Notice that we did an offline installation as opposed to an online installation. The difference is that we will load the CAM software into our ICP catalog. Online installations download the CAM software from a public Docker registry. Load CAM software Download software First, download icp-cam-x86_64-3.1.2.1.tar.gz from Fix Central. Place the tar file into the directory /download s an a Master node: $ ls -l total 10025452 -rw-r--r-- 1 root root 10266055420 May 31 20:22 icp-cam-x86_64-3.1.2.1.tar.gz Log into ICP and Docker $ cloudctl login -a https://dbamc.icp:8443 --skip-ssl-validation -u admin -p admin -n services Authenticating... OK Targeted account dbamc Account (id-dbamc-account) Targeted namespace services Configuring kubectl ... Property clusters.dbamc unset. Property users.dbamc-user unset. Property contexts.dbamc-context unset. Cluster dbamc set. User dbamc-user set. Context dbamc-context created. Switched to context dbamc-context . OK Configuring helm: /root/.helm OK root@dbamc-icp-ubuntu-master3:/downloads# docker login dbamc.icp:8500 Authenticating with existing credentials... WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded Load the software into the catalog cloudctl catalog load-archive --archive /downloads/icp-cam-x86_64-3.1.2.1.tar.gz Generate API Key You will need to generate an API key to install CAM because its required as one of the configuration parameters. Here are the commands to do this. export serviceIDName='service-deploy' export serviceApiKeyName='service-deploy-api-key' cloudctl login -a https://dbamc.icp:8443 --skip-ssl-validation -u admin -p admin -n services cloudctl iam service-id-create ${serviceIDName} -d 'Service ID for service-deploy' cloudctl iam service-policy-create ${serviceIDName} -r Administrator,ClusterAdministrator --service-name 'idmgmt' cloudctl iam service-policy-create ${serviceIDName} -r Administrator,ClusterAdministrator --service-name 'identity' cloudctl iam service-api-key-create ${serviceApiKeyName} ${serviceIDName} -d 'Api key for service-deploy' The output is: Name service-deploy-api-key Description Api key for service-deploy Bound To crn:v1:icp:private:iam-identity:dbamc:n/services::serviceid:ServiceId-bce5616f-b7b5-4d8c-9098-e7ffc61a7125 Created At 2019-06-06T05:39+0000 API Key gWWji7Nf0qJCgSWOHAyzwCkD-86nRk7scPbuHz5Hp2Vw You need to save this API Key somewhere. You will never be able to view it again, and you will need it later as a parameter when installing CAM into ICP. NOTE: One other thing that is happening when you run the above commands is that you are creating a Service ID for CAM. If you log into the ICP console, navigate to: Manage - Identity Access - Service IDs And there you will see a Service ID called service-deploy. Create PV folders in NFS CAM needs multiple persistent volumes (PV) to be configured. The first thing you need to do is create directories that the Persistent Volumes will bind to. Most likely, you will create these directories on a shared server so that the directories can be shared by multiple instances of CAM. Via command line: login to the NFS server and execute: export exportdir=/data/export mkdir -p \\ ${exportdir}/CAM_db ${exportdir}/CAM_terraform/ \\ cam-provider-terraform \\ ${exportdir}/CAM_logs/cam-provider-terraform \\ ${exportdir}/CAM_BPD_appdata/mysql \\ ${exportdir}/CAM_BPD_appdata/repositories \\ ${exportdir}/CAM_BPD_appdata/workspace chmod -R 2775 ${exportdir}/CAM_db ${exportdir}/CAM_logs ${exportdir}/CAM_terraform ${exportdir}/CAM_BPD_appdata chown -R root:1000 ${exportdir}/CAM_logs ${exportdir}/CAM_BPD_appdata chown -R root:1111 ${exportdir}/CAM_terraform ${exportdir}/CAM_logs/cam-provider-terraform chown -R 999:999 ${exportdir}/CAM_BPD_appdata/mysql ${exportdir}/CAM_db Install CAM Create persistent volumes You can create the PVs in the ICP console or via Kubernetes commands. Make sure you set the server and path attributes to match your environment. Log into the boot node Authenticate to ICP from the command line: cloudctl login -a https://dbamc.icp:8443 --skip-ssl-validation -u admin -p admin -n services Kubernetes command for cam-mongo-pv kubectl create -f - EOF kind : PersistentVolume apiVersion : v1 metadata : name : cam-mongo-pv labels : type : cam-mongo spec : capacity : storage : 15Gi accessModes : - ReadWriteMany nfs : server : 172.16.52.212 path : /data/export/CAM_db EOF Kubernetes command for cam-logs-pv kubectl create -f - EOF kind : PersistentVolume apiVersion : v1 metadata : name : cam-logs-pv labels : type : cam-logs spec : capacity : storage : 10Gi accessModes : - ReadWriteMany nfs : server : 172.16.52.212 path : /data/export/CAM_logs EOF Kubernetes command for cam-terraform-pv kubectl create -f - EOF kind : PersistentVolume apiVersion : v1 metadata : name : cam-terraform-pv labels : type : cam-terraform spec : capacity : storage : 15Gi accessModes : - ReadWriteMany nfs : server : 172.16.52.212 path : /data/export/CAM_terraform EOF Kubernetes command for cam-bpd-appdata-pv kubectl create -f - EOF kind : PersistentVolume apiVersion : v1 metadata : name : cam-bpd-appdata-pv labels : type : cam-bpd-appdata spec : capacity : storage : 20Gi accessModes : - ReadWriteMany nfs : server : 172.16.52.212 path : /data/export/CAM_BPD_appdata EOF Install interactively Log into the ICP Console and navigate to Manage - Helm Repositories: Click Sync Repositories on the right: Again, on ICP home page, select catalog in the upper-right hand side. Using the search bar, search for CAM. Select the ibm-cam Helm Chart from the local-charts repository (there will be two ibm-cam icons, one for the online version and one for the local version that you installed above. Pick the local version.) Click the Configuration tab and use the following settings: Helm release name: cam Target namespace: services Check the license agreement IAM service API key: use the key copied in task above. Click Install Check the status of the deployment from the command line: kubectl get po -n services Installl with command line You can also install the helm chart via the command line. Download the Helm chart curl -kLo /root/cam/ibm-cam-3.1.3.tgz https://dbamc.icp:8443/helm-repo/requiredAssets/ibm-cam-3.1.3.tgz Install CAM Helm chart helm install /root/cam/ibm-cam-3.1.3.tgz --name cam --namespace services --set license=accept,global.iam.deployApiKey=gWWji7Nf0qJCgSWOHAyzwCkD-86nRk7scPbuHz5Hp2Vw,image.repository=dbamc.icp:8500/services/,auditService.image.repository=dbamc.icp:8500/ibmcom/ --tls Uninstall Purge Helm Release kubectl delete clusterservicebrokers.servicecatalog.k8s.io cam-broker helm del cam --purge --tls kubectl delete pod --grace-period=0 --force --namespace services -l release=cam kubectl delete secret cam-docker-secret -n services Delete PV and PVC kubectl delete PersistentVolumeClaim cam-terraform-pv -n services kubectl delete PersistentVolumeClaim cam-bpd-appdata-pv -n services kubectl delete PersistentVolumeClaim cam-logs-pv -n services kubectl delete PersistentVolumeClaim cam-mongo-pv -n services cd /root/cam kubectl delete -f pv.yaml Clean PVs NFS folders ssh nfs rm -Rf /data/export/CAM_db/* Troubleshooting cam-bpd-ui not available See this documentation reference # Get into the MariaDB pod kubectl exec -it -n services cam-bpd-mariadb-d84b5b9d8-lpdg7 -- bash # Get the MariaDB root password env | grep MYSQL_ROOT_PASSWORD # Run the mysql command line tool mysql -u root -p password_found_above # for example: mysql -u root -pbcb19ee3dee0 # you might only need mysql without any credentials # Show the databases show databases; # Verify database ibm_ucdp exists. If it does, then use ibm_ucdp; show tables; # Verify there are many tables (should show around 61) # Verify the user ucdpadmin exists SELECT User,Host FROM mysql.user; drop database ibm_ucdp; CREATE DATABASE ibm_ucdp; CREATE USER 'ucdpadmin'@'%' IDENTIFIED BY 'bcb19ee3dee0'; GRANT ALL ON ibm_ucdp.* TO 'ucdpadmin'@'%' ; FLUSH PRIVILEGES; kubectl delete pod cam-bpd-ui-77dd68957f-jxvb8 -n services","title":"Installing CAM"},{"location":"environment/install-cam/#introduction","text":"IBM Cloud Automation Manager (CAM) is a cloud management solution in IBM Cloud Private for deploying cloud infrastructure in multiple clouds with an optimized user experience. Cloud Automation Manager uses open source Terraform to manage and deliver cloud infrastructure as code. Cloud infrastructure that is delivered as code is reusable, is able to be placed under version control, can be shared across distributed teams, and can be used to easily replicate environments. In relation to ICP4A, CAM is used to deploy IBM Business Automation Workflow (BAW) to traditional WebSphere nodes. Eventually, IBM BAW will be deployed as containers which will mean that CAM would not be used in this case. CAM will still be strategic for deploying any VM based offering. CAM will also soon support OpenShift. As long as there are WAS-ND based offerings, CAM will be used to deploy them. Even after IBM BAW is delivered in containers, customers will still want to or need to use VM (WAS-ND) based versions if they: Are using IID/BPEL content, which is almost 50% of Workflow clients They are dependent upon deprecated code which will be removed in the Container version of Workflow and can't yet refactor their apps. There are several steps to install CAM, described in the IBM CAM Knowledge Center documentation. Notice that we did an offline installation as opposed to an online installation. The difference is that we will load the CAM software into our ICP catalog. Online installations download the CAM software from a public Docker registry.","title":"Introduction"},{"location":"environment/install-cam/#load-cam-software","text":"","title":"Load CAM software"},{"location":"environment/install-cam/#download-software","text":"First, download icp-cam-x86_64-3.1.2.1.tar.gz from Fix Central. Place the tar file into the directory /download s an a Master node: $ ls -l total 10025452 -rw-r--r-- 1 root root 10266055420 May 31 20:22 icp-cam-x86_64-3.1.2.1.tar.gz","title":"Download software"},{"location":"environment/install-cam/#log-into-icp-and-docker","text":"$ cloudctl login -a https://dbamc.icp:8443 --skip-ssl-validation -u admin -p admin -n services Authenticating... OK Targeted account dbamc Account (id-dbamc-account) Targeted namespace services Configuring kubectl ... Property clusters.dbamc unset. Property users.dbamc-user unset. Property contexts.dbamc-context unset. Cluster dbamc set. User dbamc-user set. Context dbamc-context created. Switched to context dbamc-context . OK Configuring helm: /root/.helm OK root@dbamc-icp-ubuntu-master3:/downloads# docker login dbamc.icp:8500 Authenticating with existing credentials... WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded","title":"Log into ICP and Docker"},{"location":"environment/install-cam/#load-the-software-into-the-catalog","text":"cloudctl catalog load-archive --archive /downloads/icp-cam-x86_64-3.1.2.1.tar.gz","title":"Load the software into the catalog"},{"location":"environment/install-cam/#generate-api-key","text":"You will need to generate an API key to install CAM because its required as one of the configuration parameters. Here are the commands to do this. export serviceIDName='service-deploy' export serviceApiKeyName='service-deploy-api-key' cloudctl login -a https://dbamc.icp:8443 --skip-ssl-validation -u admin -p admin -n services cloudctl iam service-id-create ${serviceIDName} -d 'Service ID for service-deploy' cloudctl iam service-policy-create ${serviceIDName} -r Administrator,ClusterAdministrator --service-name 'idmgmt' cloudctl iam service-policy-create ${serviceIDName} -r Administrator,ClusterAdministrator --service-name 'identity' cloudctl iam service-api-key-create ${serviceApiKeyName} ${serviceIDName} -d 'Api key for service-deploy' The output is: Name service-deploy-api-key Description Api key for service-deploy Bound To crn:v1:icp:private:iam-identity:dbamc:n/services::serviceid:ServiceId-bce5616f-b7b5-4d8c-9098-e7ffc61a7125 Created At 2019-06-06T05:39+0000 API Key gWWji7Nf0qJCgSWOHAyzwCkD-86nRk7scPbuHz5Hp2Vw You need to save this API Key somewhere. You will never be able to view it again, and you will need it later as a parameter when installing CAM into ICP. NOTE: One other thing that is happening when you run the above commands is that you are creating a Service ID for CAM. If you log into the ICP console, navigate to: Manage - Identity Access - Service IDs And there you will see a Service ID called service-deploy.","title":"Generate API Key"},{"location":"environment/install-cam/#create-pv-folders-in-nfs","text":"CAM needs multiple persistent volumes (PV) to be configured. The first thing you need to do is create directories that the Persistent Volumes will bind to. Most likely, you will create these directories on a shared server so that the directories can be shared by multiple instances of CAM. Via command line: login to the NFS server and execute: export exportdir=/data/export mkdir -p \\ ${exportdir}/CAM_db ${exportdir}/CAM_terraform/ \\ cam-provider-terraform \\ ${exportdir}/CAM_logs/cam-provider-terraform \\ ${exportdir}/CAM_BPD_appdata/mysql \\ ${exportdir}/CAM_BPD_appdata/repositories \\ ${exportdir}/CAM_BPD_appdata/workspace chmod -R 2775 ${exportdir}/CAM_db ${exportdir}/CAM_logs ${exportdir}/CAM_terraform ${exportdir}/CAM_BPD_appdata chown -R root:1000 ${exportdir}/CAM_logs ${exportdir}/CAM_BPD_appdata chown -R root:1111 ${exportdir}/CAM_terraform ${exportdir}/CAM_logs/cam-provider-terraform chown -R 999:999 ${exportdir}/CAM_BPD_appdata/mysql ${exportdir}/CAM_db","title":"Create PV folders in NFS"},{"location":"environment/install-cam/#install-cam","text":"","title":"Install CAM"},{"location":"environment/install-cam/#create-persistent-volumes","text":"You can create the PVs in the ICP console or via Kubernetes commands. Make sure you set the server and path attributes to match your environment. Log into the boot node Authenticate to ICP from the command line: cloudctl login -a https://dbamc.icp:8443 --skip-ssl-validation -u admin -p admin -n services Kubernetes command for cam-mongo-pv kubectl create -f - EOF kind : PersistentVolume apiVersion : v1 metadata : name : cam-mongo-pv labels : type : cam-mongo spec : capacity : storage : 15Gi accessModes : - ReadWriteMany nfs : server : 172.16.52.212 path : /data/export/CAM_db EOF Kubernetes command for cam-logs-pv kubectl create -f - EOF kind : PersistentVolume apiVersion : v1 metadata : name : cam-logs-pv labels : type : cam-logs spec : capacity : storage : 10Gi accessModes : - ReadWriteMany nfs : server : 172.16.52.212 path : /data/export/CAM_logs EOF Kubernetes command for cam-terraform-pv kubectl create -f - EOF kind : PersistentVolume apiVersion : v1 metadata : name : cam-terraform-pv labels : type : cam-terraform spec : capacity : storage : 15Gi accessModes : - ReadWriteMany nfs : server : 172.16.52.212 path : /data/export/CAM_terraform EOF Kubernetes command for cam-bpd-appdata-pv kubectl create -f - EOF kind : PersistentVolume apiVersion : v1 metadata : name : cam-bpd-appdata-pv labels : type : cam-bpd-appdata spec : capacity : storage : 20Gi accessModes : - ReadWriteMany nfs : server : 172.16.52.212 path : /data/export/CAM_BPD_appdata EOF","title":"Create persistent volumes"},{"location":"environment/install-cam/#install-interactively","text":"Log into the ICP Console and navigate to Manage - Helm Repositories: Click Sync Repositories on the right: Again, on ICP home page, select catalog in the upper-right hand side. Using the search bar, search for CAM. Select the ibm-cam Helm Chart from the local-charts repository (there will be two ibm-cam icons, one for the online version and one for the local version that you installed above. Pick the local version.) Click the Configuration tab and use the following settings: Helm release name: cam Target namespace: services Check the license agreement IAM service API key: use the key copied in task above. Click Install Check the status of the deployment from the command line: kubectl get po -n services","title":"Install interactively"},{"location":"environment/install-cam/#installl-with-command-line","text":"You can also install the helm chart via the command line. Download the Helm chart curl -kLo /root/cam/ibm-cam-3.1.3.tgz https://dbamc.icp:8443/helm-repo/requiredAssets/ibm-cam-3.1.3.tgz Install CAM Helm chart helm install /root/cam/ibm-cam-3.1.3.tgz --name cam --namespace services --set license=accept,global.iam.deployApiKey=gWWji7Nf0qJCgSWOHAyzwCkD-86nRk7scPbuHz5Hp2Vw,image.repository=dbamc.icp:8500/services/,auditService.image.repository=dbamc.icp:8500/ibmcom/ --tls","title":"Installl with command line"},{"location":"environment/install-cam/#uninstall","text":"","title":"Uninstall"},{"location":"environment/install-cam/#purge-helm-release","text":"kubectl delete clusterservicebrokers.servicecatalog.k8s.io cam-broker helm del cam --purge --tls kubectl delete pod --grace-period=0 --force --namespace services -l release=cam kubectl delete secret cam-docker-secret -n services","title":"Purge Helm Release"},{"location":"environment/install-cam/#delete-pv-and-pvc","text":"kubectl delete PersistentVolumeClaim cam-terraform-pv -n services kubectl delete PersistentVolumeClaim cam-bpd-appdata-pv -n services kubectl delete PersistentVolumeClaim cam-logs-pv -n services kubectl delete PersistentVolumeClaim cam-mongo-pv -n services cd /root/cam kubectl delete -f pv.yaml","title":"Delete PV and PVC"},{"location":"environment/install-cam/#clean-pvs-nfs-folders","text":"ssh nfs rm -Rf /data/export/CAM_db/*","title":"Clean PVs NFS folders"},{"location":"environment/install-cam/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"environment/install-cam/#cam-bpd-ui-not-available","text":"See this documentation reference # Get into the MariaDB pod kubectl exec -it -n services cam-bpd-mariadb-d84b5b9d8-lpdg7 -- bash # Get the MariaDB root password env | grep MYSQL_ROOT_PASSWORD # Run the mysql command line tool mysql -u root -p password_found_above # for example: mysql -u root -pbcb19ee3dee0 # you might only need mysql without any credentials # Show the databases show databases; # Verify database ibm_ucdp exists. If it does, then use ibm_ucdp; show tables; # Verify there are many tables (should show around 61) # Verify the user ucdpadmin exists SELECT User,Host FROM mysql.user; drop database ibm_ucdp; CREATE DATABASE ibm_ucdp; CREATE USER 'ucdpadmin'@'%' IDENTIFIED BY 'bcb19ee3dee0'; GRANT ALL ON ibm_ucdp.* TO 'ucdpadmin'@'%' ; FLUSH PRIVILEGES; kubectl delete pod cam-bpd-ui-77dd68957f-jxvb8 -n services","title":"cam-bpd-ui not available"},{"location":"environment/install-icp/","text":"The components involved in building the Denim Compute environment for IBM Cloud Private are shown on the figure below: Topology and sizing One of the goal of Denim Compute is to create a highly available environment to run solutions with the IBM Cloud Pak for Automation. This section reviews a set of documents that provide configuration and sizing recommendations towards that goal. ICP4A sizing This report provides hardware requirements (CPU, RAM, disk) for the different ICP4A components, but it is difficult to derive an aggregated requirement for a cluster that will support multiple capabilities. ICP sizing For ICP sizing, this ICP system requirement document from the IBM Cloud Private Reference Architecture is very useful, especially the \"Summary of production system requirements\" section. Selected topology Based on various sizing guidance, we settled on the following topology for our HA cluster: We created: (1) boot node (3) master nodes (2) proxy nodes (2) management nodes (4) worker nodes (BAI pushed us to four nodes) (1) shared services node (DB2, LDAP, NFS server) (3) BAW nodes (one Deployment Manager and two nodes) Install We use these instructions from the from the IBM Cloud Private Reference Architecture as the primary reference. Create VM templates We created our environment in a VMware vSphere environment. To start, we created two templates that we would use to clone all of the ICP and BAW nodes. The shared services node was a one-off and we didn't create a template to clone it. Both templates used this operating system: Distributor ID: Ubuntu Description: Ubuntu 16.04.6 LTS Release: 16.04 Codename: xenial Here is the worker template: The worker template was used for boot, proxy, worker and BAW nodes. Here is the master template: The master template was used for master and management nodes. NB : The shared services node uses Red Hat Enterprise Linux Server release 7.6 (Maipo) and has 8 CPU, 32 GB RAM and 300 GB disk space. No template was used to create it. Configure VM templates Before we cloned our nodes from the templates, we followed some configuration steps. This is better because if we cloned first, we would have to do the same step on every clone. Allow root login Execute the following commands on both template VMs. This enables root login remotely via ssh . Set a password for the root user sudo su - # provide your user password to get to the root shell passwd # Set the root password Enable remote login as root sed -i 's/prohibit-password/yes/' /etc/ssh sshd_config systemctl restart ssh Update NTP ICP requires all nodes be in time synchronization, so we need to update the NTP (Network Time Protocol) settings to make sure time stays in sync. Get the latest apt updates and install ntp apt-get update apt-get install -y ntp If using an internal NTP server, edit /etc/ntp.conf and add your internal server to the list and then restart the ntp server. In the following configuration, the server is configured to use a local NTP server ( ntp.{your-network-name}.local ) and fall back to public servers if that server is unavailable. # Specify one or more NTP servers. pool ntp.{your-network-name}.local After making configuration changes restart the NTP server with the command systemctl restart ntp Test the status of your NTP servers to make sure they are working. $ ntpq -p remote refid st t when poll reach delay offset jitter ======================================================================================== *ntp.{your-network-name}.local 129.6.15.30 2 u 183 1024 377 0.770 -0.140 1.026 If the ufw firewall is enabled, disable it. ICP will install iptables. $ ufw disable Firewall stopped and disabled on system startup $ ufw status Status: inactive Configure Virtual Memory This configuration is required for ELK. Update the vm.max_map_count setting to 262144: sysctl -w vm.max_map_count=262144 Make the changes permanent by adding the following line to the bottom of the /etc/sysctl.conf file: vm.max_map_count=262144 To check the current value, use the following command: $ sysctl vm.max_map_count vm.max_map_count = 262144 Install NFS common packages apt-get install -y nfs-common Install Python apt-get install -y python-setuptools Install Docker Update your ubuntu repositories: apt-get update Install Linux image extra packages apt-get install -y linux-image-extra-$(uname -r) linux-image-extra-virtual Install additional needed packages apt-get install -y apt-transport-https ca-certificates curl software-properties-common N.B. : These packages may all exist depending on what packages were included when the operating system installed. If they already exist, you will just see output indicating they already exist. If you assume they exist, however, and do not do this step and they are not there, the installation will fail. Add Docker\u2019s official GPG key curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add - Verify that the key fingerprint is 9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88 $ apt-key fingerprint 0EBFCD88 pub 4096R/0EBFCD88 2017-02-22 Key fingerprint = 9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88 uid Docker Release (CE deb) docker@docker.com sub 4096R/F273FCD8 2017-02-22 Setup the docker stable repository and update the local cache add-apt-repository deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable apt-get update Install docker apt-get install -y docker-ce Make sure docker is running and pulling correctly from the internet docker run hello-world This should downloaded the latest hello-world docker image version and put some text on the screen indicating a working installation. Clone cluster VMs Shutdown your VMs: shutdown -h now Using those VMs, convert them to templates. Clone your templates for each of the needed nodes in your cluster as specified below. Note that you can create a minimal, non-HA environment by just using a single node for each below (a Vulnerability Advisor (VA) is not required): master template - icp-master1, icp-master2, icp-master3 master template - icp-mgmt1, icp-mgmt2 worker template - icp-proxy1, icp-proxy2 worker template - icp-worker1, icp-worker2, icp-worker3, icp-worker4 worker template - icp-boot worker template - baw1, baw2, baw3 N.B. : We created the shared services node as a one-off and used RHEL. For High Availability and to provide for workload persistent storage, you will need an NFS node which does not need to be configured with any of the packages used by the above server. If an enterprise NFS server already exists which can be used for this environment it can be used, otherwise, an additional NFS node will need to be created to support this environment. Configure your newly cloned VMs and set their hostnames and static IP addresses. N.B. : The VMs can use DHCP assigned IP addresses, however, DHCP addresses are subject to change and if any addresses change your environment will break. For any kind of permanent environment, static IP addresses should be used. If your network interface is configured for DHCP, boot all of the newly provisioned nodes and then, using the VMware console, login to each VM and reconfigure the hostname and add the appropriate static IP address. IMPORTANT: If you do not have a DHCP server and configured your original VMs with a static IPs, you will need to boot each VM in turn, configuring each with its new IP address before booting the next to prevent having duplicate IP addresses on your network. Perform the following tasks to change the IP address and hostname on each respective node. Change the hostname using hostnamectl set-hostname node_name , replacing node_nam e with the new hostname for your node. Modify /etc/network/interfaces to configure a static IP address. Your file should look something like this on Ubuntu 16.04 (other OS may be slightly different): # This file describes the network interfaces available on your system # and how to activate them. For more information, see interfaces(5). source /etc/network/interfaces.d/* # The loopback network interface auto lo iface lo inet loopback # The primary network interface auto ens160 iface ens160 inet static address 172.16.53.220 netmask 255.255.0.0 broadcast 172.16.255.255 gateway 172.16.255.250 dns-nameservers 172.16.0.11 172.16.0.17 dns-search {your-network-name}.local Shut down the VM: shutdown -r now Configure passwordless SSH from the master node to all other nodes. You should now have all of your hosts prepared, named properly, and containing the proper IP addresses. The next step is to configure passwordless SSH between the boot-master node and the other nodes. You first need to create a passwordless SSH key that can be used across the implementation. Login as to the boot node as root From the root\u2019s home directory ( cd ~ ), execute: ssh-keygen -t rsa -P '' # Upper case P and two single quotes for no password Accept the default location of /root/.ssh/id_rsa for the new key file Executing ls ~/.ssh should show three files: id_rsa, id_rsa.pub and known_hosts Copy the resulting id_rsa key file from the boot node to each node in the cluster ssh-copy-id -i .ssh/id_rsa root@node_name You will be required to type the root password for each node during this process, but not thereafter. When this is complete you should be able to ssh from the boot node to each of the other nodes without having to provide a password. Test this now by executing the following for each node in the cluster, using ssh root@node_name Now update the /etc/hosts file on your boot node and add entries for each of your nodes and propagate that file to all of your other nodes. For example: # /etc/hosts 172.16.52.220 dbamc-icp-ubuntu-boot.{your-network-name}.local dbamc-icp-ubuntu-boot boot 172.16.52.221 dbamc-icp-ubuntu-master1.{your-network-name}.local dbamc-icp-ubuntu-master1 master1 172.16.52.222 dbamc-icp-ubuntu-master2.{your-network-name}.local dbamc-icp-ubuntu-master2 master2 172.16.52.223 dbamc-icp-ubuntu-master3.{your-network-name}.local dbamc-icp-ubuntu-master3 master3 172.16.52.224 dbamc-icp-ubuntu-mgmt1.{your-network-name}.local dbamc-icp-ubuntu-mgmt1 mgmt1 172.16.52.225 dbamc-icp-ubuntu-mgmt2.{your-network-name}.local dbamc-icp-ubuntu-mgmt2 mgmt2 172.16.52.226 dbamc-icp-ubuntu-proxy1.{your-network-name}.local dbamc-icp-ubuntu-proxy1 proxy1 172.16.52.227 dbamc-icp-ubuntu-proxy2.{your-network-name}.local dbamc-icp-ubuntu-proxy2 proxy2 172.16.52.228 dbamc-icp-ubuntu-worker1.{your-network-name}.local dbamc-icp-ubuntu-worker1 worker1 172.16.52.229 dbamc-icp-ubuntu-worker2.{your-network-name}.local dbamc-icp-ubuntu-worker2 worker2 172.16.52.230 dbamc-icp-ubuntu-worker3.{your-network-name}.local dbamc-icp-ubuntu-worker3 worker3 172.16.52.231 dbamc-icp-ubuntu-worker4.{your-network-name}.local dbamc-icp-ubuntu-worker4 worker4 172.16.52.212 dbamc-icp-shared-services.{your-network-name}.local dbamc-icp-shared-services With your /etc/hosts file configured correctly on the boot node, propagate it to all the other nodes in your cluster with remote copy: scp /etc/hosts node_name:/etc/hosts Your VMs are now ready to install ICP. Install ICP Load your ICP tarball into your boot node's docker registry. Only on the boot node, load the inception image from the ICP tarball into boot node's local docker registry. tar -xvf /downloads/ibm-cloud-private-x86_64-3.1.2.tar.gz ibm-inception-amd64-3.1.2.tar -O |docker load Note that this will take quite some time and a lot of disk space and memory because tar has to gunzip the entire tar file before it can extract any images. This takes memory, filespace in /tmp, and quite a lot of time. You can check the name and tag of the current inception image. To find your release information execute the following command on the boot node: $ docker images -a |grep inception ibmcom/icp-inception-amd64 3.1.2-ee 4dbdab8a176e 5 months ago 779MB In this case, the inception image name is ibmcom/icp-inception-amd64 and the version tag is 3.1.2-ee . You will replace the image name and version tag in the next commands with the ones you got when you issued the command above. Create a directory to hold installation configuration files mkdir /opt/icp cd /opt/icp Extract the installation configuration files docker run -e LICENSE=accept --rm -v /opt/icp:/data ibmcom/icp-inception-amd64:3.1.2-ee cp -r cluster /data After this command, you should have a folder named /opt/icp/cluster . Because you are installing the Enterprise Edition, you have to move the ICP tarball to the /opt/icp/cluster/images directory. mkdir -p /opt/icp/cluster/images mv /downloads/ibm-cloud-private-x86_64-3.1.2.tar.gz /opt/icp/cluster/images/ Copy the ssh key to the installation directory cp ~/.ssh/id_rsa /opt/icp/cluster/ssh_key chmod 400 /opt/icp/cluster/ssh_key Edit the /opt/icp/cluster/hosts file and enter the IP addresses of all nodes. The result should look something like this: [master] 172.16.52.221 172.16.52.222 172.16.52.223 [worker] 172.16.52.228 172.16.52.229 172.16.52.230 172.16.52.231 [proxy] 172.16.52.226 172.16.52.227 [management] 172.16.52.224 172.16.52.225 #[va] #5.5.5.5 Share NFS directories. In an HA environment, there are three directories which must be shared by all master nodes. These are /var/lib/registry , /var/lib/icp/audit , and /var/log/audit . On your NFS server, create directories for each of these paths e.g. /data/registry , /data/audit , and /data/log/audit . mkdir -p /data/registry mkdir -p /data/audit mkdir -p /data/log/audit Your NFS server should export the directories with the sync parameter set. Add these lines to the /etc/exports file on the NFS server. For example: /data/registry *(rw,sync,no_root_squash) /data/audit *(rw,sync,no_root_squash) /data/log/audit *(rw,sync,no_root_squash) Execute the exportfs command to make sure the directories are exported. exportfs -ra On each of the master nodes, edit the /etc/fstab file so that the mounts will be reestablished after a reboot. The /etc/fstab entries should look something like this: 172.16.52.212:/data/registry /var/lib/registry nfs rw,suid,dev,exec,auto,nouser,async,soft 1 2 172.16.52.212:/data/audit /var/lib/icp/audit nfs rw,suid,dev,exec,auto,nouser,async,soft 1 2 172.16.52.212:/data/log/audit /var/log/audit nfs rw,suid,dev,exec,auto,nouser,async,soft 1 2 Mount the directories in the current environment mount /var/lib/registry mount /var/lib/icp/audit mount /var/log/audit N.B. : You can also run this command: mount -a which will remount everything in the /etc/fstab file. You can validate that you successfully mounted a remote directory by running this command: $ df -P -T /var/log/audit | tail -n +2 | awk '{print $2}' nfs4 If you see nfs4 , then you are pointing to the remote directory. If you see ext4 , you are still pointing to a local directory. Deploy the ICP environment On the boot node, run these commands: cd /opt/icp/cluster docker run --rm -t -e LICENSE=accept --net=host -v /opt/icp/cluster:/installer/cluster ibmcom/icp-inception-amd64:3.1.2-ee install -vvv |tee install.log Some time later (about 30 minutes), you should have a deployed IBM Cloud private implementation. You can login to your cluster with a browser with credentials admin/admin.","title":"IBM Cloud Private"},{"location":"environment/install-icp/#topology-and-sizing","text":"One of the goal of Denim Compute is to create a highly available environment to run solutions with the IBM Cloud Pak for Automation. This section reviews a set of documents that provide configuration and sizing recommendations towards that goal.","title":"Topology and sizing"},{"location":"environment/install-icp/#icp4a-sizing","text":"This report provides hardware requirements (CPU, RAM, disk) for the different ICP4A components, but it is difficult to derive an aggregated requirement for a cluster that will support multiple capabilities.","title":"ICP4A sizing"},{"location":"environment/install-icp/#icp-sizing","text":"For ICP sizing, this ICP system requirement document from the IBM Cloud Private Reference Architecture is very useful, especially the \"Summary of production system requirements\" section.","title":"ICP sizing"},{"location":"environment/install-icp/#selected-topology","text":"Based on various sizing guidance, we settled on the following topology for our HA cluster: We created: (1) boot node (3) master nodes (2) proxy nodes (2) management nodes (4) worker nodes (BAI pushed us to four nodes) (1) shared services node (DB2, LDAP, NFS server) (3) BAW nodes (one Deployment Manager and two nodes)","title":"Selected topology"},{"location":"environment/install-icp/#install","text":"We use these instructions from the from the IBM Cloud Private Reference Architecture as the primary reference.","title":"Install"},{"location":"environment/install-icp/#create-vm-templates","text":"We created our environment in a VMware vSphere environment. To start, we created two templates that we would use to clone all of the ICP and BAW nodes. The shared services node was a one-off and we didn't create a template to clone it. Both templates used this operating system: Distributor ID: Ubuntu Description: Ubuntu 16.04.6 LTS Release: 16.04 Codename: xenial Here is the worker template: The worker template was used for boot, proxy, worker and BAW nodes. Here is the master template: The master template was used for master and management nodes. NB : The shared services node uses Red Hat Enterprise Linux Server release 7.6 (Maipo) and has 8 CPU, 32 GB RAM and 300 GB disk space. No template was used to create it.","title":"Create VM templates"},{"location":"environment/install-icp/#configure-vm-templates","text":"Before we cloned our nodes from the templates, we followed some configuration steps. This is better because if we cloned first, we would have to do the same step on every clone.","title":"Configure VM templates"},{"location":"environment/install-icp/#allow-root-login","text":"Execute the following commands on both template VMs. This enables root login remotely via ssh . Set a password for the root user sudo su - # provide your user password to get to the root shell passwd # Set the root password Enable remote login as root sed -i 's/prohibit-password/yes/' /etc/ssh sshd_config systemctl restart ssh","title":"Allow root login"},{"location":"environment/install-icp/#update-ntp","text":"ICP requires all nodes be in time synchronization, so we need to update the NTP (Network Time Protocol) settings to make sure time stays in sync. Get the latest apt updates and install ntp apt-get update apt-get install -y ntp If using an internal NTP server, edit /etc/ntp.conf and add your internal server to the list and then restart the ntp server. In the following configuration, the server is configured to use a local NTP server ( ntp.{your-network-name}.local ) and fall back to public servers if that server is unavailable. # Specify one or more NTP servers. pool ntp.{your-network-name}.local After making configuration changes restart the NTP server with the command systemctl restart ntp Test the status of your NTP servers to make sure they are working. $ ntpq -p remote refid st t when poll reach delay offset jitter ======================================================================================== *ntp.{your-network-name}.local 129.6.15.30 2 u 183 1024 377 0.770 -0.140 1.026 If the ufw firewall is enabled, disable it. ICP will install iptables. $ ufw disable Firewall stopped and disabled on system startup $ ufw status Status: inactive","title":"Update NTP"},{"location":"environment/install-icp/#configure-virtual-memory","text":"This configuration is required for ELK. Update the vm.max_map_count setting to 262144: sysctl -w vm.max_map_count=262144 Make the changes permanent by adding the following line to the bottom of the /etc/sysctl.conf file: vm.max_map_count=262144 To check the current value, use the following command: $ sysctl vm.max_map_count vm.max_map_count = 262144","title":"Configure Virtual Memory"},{"location":"environment/install-icp/#install-nfs-common-packages","text":"apt-get install -y nfs-common","title":"Install NFS common packages"},{"location":"environment/install-icp/#install-python","text":"apt-get install -y python-setuptools","title":"Install Python"},{"location":"environment/install-icp/#install-docker","text":"Update your ubuntu repositories: apt-get update Install Linux image extra packages apt-get install -y linux-image-extra-$(uname -r) linux-image-extra-virtual Install additional needed packages apt-get install -y apt-transport-https ca-certificates curl software-properties-common N.B. : These packages may all exist depending on what packages were included when the operating system installed. If they already exist, you will just see output indicating they already exist. If you assume they exist, however, and do not do this step and they are not there, the installation will fail. Add Docker\u2019s official GPG key curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add - Verify that the key fingerprint is 9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88 $ apt-key fingerprint 0EBFCD88 pub 4096R/0EBFCD88 2017-02-22 Key fingerprint = 9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88 uid Docker Release (CE deb) docker@docker.com sub 4096R/F273FCD8 2017-02-22 Setup the docker stable repository and update the local cache add-apt-repository deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable apt-get update Install docker apt-get install -y docker-ce Make sure docker is running and pulling correctly from the internet docker run hello-world This should downloaded the latest hello-world docker image version and put some text on the screen indicating a working installation.","title":"Install Docker"},{"location":"environment/install-icp/#clone-cluster-vms","text":"Shutdown your VMs: shutdown -h now Using those VMs, convert them to templates. Clone your templates for each of the needed nodes in your cluster as specified below. Note that you can create a minimal, non-HA environment by just using a single node for each below (a Vulnerability Advisor (VA) is not required): master template - icp-master1, icp-master2, icp-master3 master template - icp-mgmt1, icp-mgmt2 worker template - icp-proxy1, icp-proxy2 worker template - icp-worker1, icp-worker2, icp-worker3, icp-worker4 worker template - icp-boot worker template - baw1, baw2, baw3 N.B. : We created the shared services node as a one-off and used RHEL. For High Availability and to provide for workload persistent storage, you will need an NFS node which does not need to be configured with any of the packages used by the above server. If an enterprise NFS server already exists which can be used for this environment it can be used, otherwise, an additional NFS node will need to be created to support this environment. Configure your newly cloned VMs and set their hostnames and static IP addresses. N.B. : The VMs can use DHCP assigned IP addresses, however, DHCP addresses are subject to change and if any addresses change your environment will break. For any kind of permanent environment, static IP addresses should be used. If your network interface is configured for DHCP, boot all of the newly provisioned nodes and then, using the VMware console, login to each VM and reconfigure the hostname and add the appropriate static IP address. IMPORTANT: If you do not have a DHCP server and configured your original VMs with a static IPs, you will need to boot each VM in turn, configuring each with its new IP address before booting the next to prevent having duplicate IP addresses on your network. Perform the following tasks to change the IP address and hostname on each respective node. Change the hostname using hostnamectl set-hostname node_name , replacing node_nam e with the new hostname for your node. Modify /etc/network/interfaces to configure a static IP address. Your file should look something like this on Ubuntu 16.04 (other OS may be slightly different): # This file describes the network interfaces available on your system # and how to activate them. For more information, see interfaces(5). source /etc/network/interfaces.d/* # The loopback network interface auto lo iface lo inet loopback # The primary network interface auto ens160 iface ens160 inet static address 172.16.53.220 netmask 255.255.0.0 broadcast 172.16.255.255 gateway 172.16.255.250 dns-nameservers 172.16.0.11 172.16.0.17 dns-search {your-network-name}.local Shut down the VM: shutdown -r now Configure passwordless SSH from the master node to all other nodes. You should now have all of your hosts prepared, named properly, and containing the proper IP addresses. The next step is to configure passwordless SSH between the boot-master node and the other nodes. You first need to create a passwordless SSH key that can be used across the implementation. Login as to the boot node as root From the root\u2019s home directory ( cd ~ ), execute: ssh-keygen -t rsa -P '' # Upper case P and two single quotes for no password Accept the default location of /root/.ssh/id_rsa for the new key file Executing ls ~/.ssh should show three files: id_rsa, id_rsa.pub and known_hosts Copy the resulting id_rsa key file from the boot node to each node in the cluster ssh-copy-id -i .ssh/id_rsa root@node_name You will be required to type the root password for each node during this process, but not thereafter. When this is complete you should be able to ssh from the boot node to each of the other nodes without having to provide a password. Test this now by executing the following for each node in the cluster, using ssh root@node_name Now update the /etc/hosts file on your boot node and add entries for each of your nodes and propagate that file to all of your other nodes. For example: # /etc/hosts 172.16.52.220 dbamc-icp-ubuntu-boot.{your-network-name}.local dbamc-icp-ubuntu-boot boot 172.16.52.221 dbamc-icp-ubuntu-master1.{your-network-name}.local dbamc-icp-ubuntu-master1 master1 172.16.52.222 dbamc-icp-ubuntu-master2.{your-network-name}.local dbamc-icp-ubuntu-master2 master2 172.16.52.223 dbamc-icp-ubuntu-master3.{your-network-name}.local dbamc-icp-ubuntu-master3 master3 172.16.52.224 dbamc-icp-ubuntu-mgmt1.{your-network-name}.local dbamc-icp-ubuntu-mgmt1 mgmt1 172.16.52.225 dbamc-icp-ubuntu-mgmt2.{your-network-name}.local dbamc-icp-ubuntu-mgmt2 mgmt2 172.16.52.226 dbamc-icp-ubuntu-proxy1.{your-network-name}.local dbamc-icp-ubuntu-proxy1 proxy1 172.16.52.227 dbamc-icp-ubuntu-proxy2.{your-network-name}.local dbamc-icp-ubuntu-proxy2 proxy2 172.16.52.228 dbamc-icp-ubuntu-worker1.{your-network-name}.local dbamc-icp-ubuntu-worker1 worker1 172.16.52.229 dbamc-icp-ubuntu-worker2.{your-network-name}.local dbamc-icp-ubuntu-worker2 worker2 172.16.52.230 dbamc-icp-ubuntu-worker3.{your-network-name}.local dbamc-icp-ubuntu-worker3 worker3 172.16.52.231 dbamc-icp-ubuntu-worker4.{your-network-name}.local dbamc-icp-ubuntu-worker4 worker4 172.16.52.212 dbamc-icp-shared-services.{your-network-name}.local dbamc-icp-shared-services With your /etc/hosts file configured correctly on the boot node, propagate it to all the other nodes in your cluster with remote copy: scp /etc/hosts node_name:/etc/hosts Your VMs are now ready to install ICP.","title":"Clone cluster VMs"},{"location":"environment/install-icp/#install-icp","text":"Load your ICP tarball into your boot node's docker registry. Only on the boot node, load the inception image from the ICP tarball into boot node's local docker registry. tar -xvf /downloads/ibm-cloud-private-x86_64-3.1.2.tar.gz ibm-inception-amd64-3.1.2.tar -O |docker load Note that this will take quite some time and a lot of disk space and memory because tar has to gunzip the entire tar file before it can extract any images. This takes memory, filespace in /tmp, and quite a lot of time. You can check the name and tag of the current inception image. To find your release information execute the following command on the boot node: $ docker images -a |grep inception ibmcom/icp-inception-amd64 3.1.2-ee 4dbdab8a176e 5 months ago 779MB In this case, the inception image name is ibmcom/icp-inception-amd64 and the version tag is 3.1.2-ee . You will replace the image name and version tag in the next commands with the ones you got when you issued the command above. Create a directory to hold installation configuration files mkdir /opt/icp cd /opt/icp Extract the installation configuration files docker run -e LICENSE=accept --rm -v /opt/icp:/data ibmcom/icp-inception-amd64:3.1.2-ee cp -r cluster /data After this command, you should have a folder named /opt/icp/cluster . Because you are installing the Enterprise Edition, you have to move the ICP tarball to the /opt/icp/cluster/images directory. mkdir -p /opt/icp/cluster/images mv /downloads/ibm-cloud-private-x86_64-3.1.2.tar.gz /opt/icp/cluster/images/ Copy the ssh key to the installation directory cp ~/.ssh/id_rsa /opt/icp/cluster/ssh_key chmod 400 /opt/icp/cluster/ssh_key Edit the /opt/icp/cluster/hosts file and enter the IP addresses of all nodes. The result should look something like this: [master] 172.16.52.221 172.16.52.222 172.16.52.223 [worker] 172.16.52.228 172.16.52.229 172.16.52.230 172.16.52.231 [proxy] 172.16.52.226 172.16.52.227 [management] 172.16.52.224 172.16.52.225 #[va] #5.5.5.5 Share NFS directories. In an HA environment, there are three directories which must be shared by all master nodes. These are /var/lib/registry , /var/lib/icp/audit , and /var/log/audit . On your NFS server, create directories for each of these paths e.g. /data/registry , /data/audit , and /data/log/audit . mkdir -p /data/registry mkdir -p /data/audit mkdir -p /data/log/audit Your NFS server should export the directories with the sync parameter set. Add these lines to the /etc/exports file on the NFS server. For example: /data/registry *(rw,sync,no_root_squash) /data/audit *(rw,sync,no_root_squash) /data/log/audit *(rw,sync,no_root_squash) Execute the exportfs command to make sure the directories are exported. exportfs -ra On each of the master nodes, edit the /etc/fstab file so that the mounts will be reestablished after a reboot. The /etc/fstab entries should look something like this: 172.16.52.212:/data/registry /var/lib/registry nfs rw,suid,dev,exec,auto,nouser,async,soft 1 2 172.16.52.212:/data/audit /var/lib/icp/audit nfs rw,suid,dev,exec,auto,nouser,async,soft 1 2 172.16.52.212:/data/log/audit /var/log/audit nfs rw,suid,dev,exec,auto,nouser,async,soft 1 2 Mount the directories in the current environment mount /var/lib/registry mount /var/lib/icp/audit mount /var/log/audit N.B. : You can also run this command: mount -a which will remount everything in the /etc/fstab file. You can validate that you successfully mounted a remote directory by running this command: $ df -P -T /var/log/audit | tail -n +2 | awk '{print $2}' nfs4 If you see nfs4 , then you are pointing to the remote directory. If you see ext4 , you are still pointing to a local directory.","title":"Install ICP"},{"location":"environment/install-icp/#deploy-the-icp-environment","text":"On the boot node, run these commands: cd /opt/icp/cluster docker run --rm -t -e LICENSE=accept --net=host -v /opt/icp/cluster:/installer/cluster ibmcom/icp-inception-amd64:3.1.2-ee install -vvv |tee install.log Some time later (about 30 minutes), you should have a deployed IBM Cloud private implementation. You can login to your cluster with a browser with credentials admin/admin.","title":"Deploy the ICP environment"},{"location":"environment/install-openshift/","text":"Coming ASAP...","title":"RedHat OpenShift"},{"location":"environment/install-openshift/#coming-asap","text":"","title":"Coming ASAP..."},{"location":"environment/intro/","text":"Solution environment The target environment for the deployment of Denim Compute is the IBM Cloud Pak for Automation on top of a Kubernetes application management platform. A requirement for the environment is to provide minimum high-availability configuration. In the next sections, we detail how to: Create the required VMs, using VMware vSphere as the virtualization platform. Install the Kubernetes application management platform. Deploy the IBACC container to manage the installation of the Cloud Pak components. Install the individual DBA components (BAW, ODM, BAI, ...). Scripted install and deployment A collection of Ansible scripts to configure and install platforms, and deploy the ICP4A components are available in the ibm-cloud-architecture/icp4a-deployment GitHub repository.","title":"Introduction"},{"location":"environment/intro/#solution-environment","text":"The target environment for the deployment of Denim Compute is the IBM Cloud Pak for Automation on top of a Kubernetes application management platform. A requirement for the environment is to provide minimum high-availability configuration. In the next sections, we detail how to: Create the required VMs, using VMware vSphere as the virtualization platform. Install the Kubernetes application management platform. Deploy the IBACC container to manage the installation of the Cloud Pak components. Install the individual DBA components (BAW, ODM, BAI, ...).","title":"Solution environment"},{"location":"environment/intro/#scripted-install-and-deployment","text":"A collection of Ansible scripts to configure and install platforms, and deploy the ICP4A components are available in the ibm-cloud-architecture/icp4a-deployment GitHub repository.","title":"Scripted install and deployment"},{"location":"environment/sundries/","text":"Miscellaneous notes and findings Below are a list of random notes (recommendations, gotchas, etc.) regarding the setup of the environment and deployment of the IBM Cloud Pak for Automation. About VMs Nodes IP address range allocation When assigning IP addresses to the nodes in your cluster, double-check that the range you are using does not overlap with an range that is already allocated to another cluster. It seems obvious, but nothing usually prevents from using an IP that is already used by another VM. So, when that happens, the issue may be difficult to detect as the cluster will seem to operate off and on, in an erratic fashion. About ICP CIDR overlap The ICP configuration allows the specification of the cluster CIDR range. The lines below show the default definition in the config.yaml file. ## Network in IPv4 CIDR format network_cidr: 10.1.0.0/16 You must ensure that the CIDR range is not already used by the virtualization manager. Using the same CIDR will likely not cause any issue as long as you don't communicate back to the virtualization manager. However, this is exactly what CAM needs to do to support the provisioning of the BAW VMs. Note that once ICP has been installed, you will not be able to dynamically change the CIDR (at least in an easy way). You will thus have to re-install ICP with the updated CIDR. About CAM Even after BAW is containerized, CAM will still strategic for any VM based offering and will also support OpenShift. And as long as clients have WAS-ND based offerings, they will need CAM. Even after Workflow containers are available, customers will still want to or need to use VM (WAS-ND) based versions if they: Are using IID/BPEL content [almost 50% of Workflow customers] or They are dependent upon deprecated code which will be removed in the Container version of Workflow and can't yet refactor their apps. About BAW Installing through CAM (ICP4A 19.0.1 and before) The install of BAW involves the use of the IBM Cloud Automation Manager (CAM). While BAW can be installed on manually provisioned VMs, the use of CAM brings the following benefits: It facilitates the replication of the install process to multiple environments (e.g. DEV, TEST, PROD, etc.) It allows to have the same metering in ICP that the Docker containers use and go to the same dashboard. This means that VPC usage can be tracked for all Docker and non-Docker based offerings within ICP4A. BAW/CPE compatibility (BAW 19.0.02 and before) On-prem BAW is not compatible with the containerized Content Platform Engine (CPE) for various reasons linked to transactionality (e.g. use of Web Service Reliable Messaging). On-prem BAW workflows should thus use the on-prem CPE.","title":"Sundries"},{"location":"environment/sundries/#miscellaneous-notes-and-findings","text":"Below are a list of random notes (recommendations, gotchas, etc.) regarding the setup of the environment and deployment of the IBM Cloud Pak for Automation.","title":"Miscellaneous notes and findings"},{"location":"environment/sundries/#about-vms","text":"","title":"About VMs"},{"location":"environment/sundries/#nodes-ip-address-range-allocation","text":"When assigning IP addresses to the nodes in your cluster, double-check that the range you are using does not overlap with an range that is already allocated to another cluster. It seems obvious, but nothing usually prevents from using an IP that is already used by another VM. So, when that happens, the issue may be difficult to detect as the cluster will seem to operate off and on, in an erratic fashion.","title":"Nodes IP address range allocation"},{"location":"environment/sundries/#about-icp","text":"","title":"About ICP"},{"location":"environment/sundries/#cidr-overlap","text":"The ICP configuration allows the specification of the cluster CIDR range. The lines below show the default definition in the config.yaml file. ## Network in IPv4 CIDR format network_cidr: 10.1.0.0/16 You must ensure that the CIDR range is not already used by the virtualization manager. Using the same CIDR will likely not cause any issue as long as you don't communicate back to the virtualization manager. However, this is exactly what CAM needs to do to support the provisioning of the BAW VMs. Note that once ICP has been installed, you will not be able to dynamically change the CIDR (at least in an easy way). You will thus have to re-install ICP with the updated CIDR.","title":"CIDR overlap"},{"location":"environment/sundries/#about-cam","text":"Even after BAW is containerized, CAM will still strategic for any VM based offering and will also support OpenShift. And as long as clients have WAS-ND based offerings, they will need CAM. Even after Workflow containers are available, customers will still want to or need to use VM (WAS-ND) based versions if they: Are using IID/BPEL content [almost 50% of Workflow customers] or They are dependent upon deprecated code which will be removed in the Container version of Workflow and can't yet refactor their apps.","title":"About CAM"},{"location":"environment/sundries/#about-baw","text":"","title":"About BAW"},{"location":"environment/sundries/#installing-through-cam-icp4a-1901-and-before","text":"The install of BAW involves the use of the IBM Cloud Automation Manager (CAM). While BAW can be installed on manually provisioned VMs, the use of CAM brings the following benefits: It facilitates the replication of the install process to multiple environments (e.g. DEV, TEST, PROD, etc.) It allows to have the same metering in ICP that the Docker containers use and go to the same dashboard. This means that VPC usage can be tracked for all Docker and non-Docker based offerings within ICP4A.","title":"Installing through CAM (ICP4A 19.0.1 and before)"},{"location":"environment/sundries/#bawcpe-compatibility-baw-19002-and-before","text":"On-prem BAW is not compatible with the containerized Content Platform Engine (CPE) for various reasons linked to transactionality (e.g. use of Web Service Reliable Messaging). On-prem BAW workflows should thus use the on-prem CPE.","title":"BAW/CPE compatibility (BAW 19.0.02 and before)"},{"location":"showtime/deploy-solution/","text":"Deploying the solution This chapter shows you how to deploy the different components artifacts of the Denim Compute solution to the ICP cluster where your DBA components are deployed. Scripted deployment Under the {denim_compute_repo}/solution/scripts directory, you can find Ansible scripts that automate the deployment of the different solution artifacts (BAW solution, ODM decision service, BAI dashboards. To deploy the decision service, use: ansible-playbook main.yaml -i inventory --tags \"deploy_ds\" To load the BPM/Case application, use: ansible-playbook main.yaml -i inventory --tags \"load_case\" To import the custom BAI dashboards, use: ansible-playbook main.yaml -i inventory --tags \"import_dashboard\"","title":"Deploy"},{"location":"showtime/deploy-solution/#deploying-the-solution","text":"This chapter shows you how to deploy the different components artifacts of the Denim Compute solution to the ICP cluster where your DBA components are deployed.","title":"Deploying the solution"},{"location":"showtime/deploy-solution/#scripted-deployment","text":"Under the {denim_compute_repo}/solution/scripts directory, you can find Ansible scripts that automate the deployment of the different solution artifacts (BAW solution, ODM decision service, BAI dashboards. To deploy the decision service, use: ansible-playbook main.yaml -i inventory --tags \"deploy_ds\" To load the BPM/Case application, use: ansible-playbook main.yaml -i inventory --tags \"load_case\" To import the custom BAI dashboards, use: ansible-playbook main.yaml -i inventory --tags \"import_dashboard\"","title":"Scripted deployment"},{"location":"showtime/run-solution/","text":"Running the solution Deploy the case solution in Workflow Center Login to Workflow Center and open the Case Solution named Denim Compute Auto Claims : Click the deploy icon in the upper-right corner of the Case Builder: Create Case security configuration In the Workflow Center select the solution and click the contextual menu and then Advanced . With the Denim Compute solution selected click Actions , then Manage and then Security Configuration . Select the option to Create a security configuration and click Next . Provide a Security manifest name and click Next . Set permissions against the security Roles (in this example all permissions are assigned to each Role ) and click Next . You can then set the adminstrators, if you want you can Add others. In the Add Users and Groups modal dialogue you can add users or groups. You start typing partial names and then click the magnifying glass icon to find matching users that appear in the Available section. You can then use the arrows to move between Available and Selected and when done you click Add to complete. When done with this section click Next to continue. Next you map groups and users to the Case Roles . Select each Role and click Add then follow the earlier instructions for how to find and assign users and groups. In this example we have just added a default user to the Claim Intake Services Role . You repeat these steps for the other Roles . Here is the final situation were example users are added to each Role (if you want a more realistic scenario you should setup different users and groups and assign them to the Roles ). Click Next when done with this section. Now you can check the box next to Apply the security configuration and click Apply . You should get confirmation that the security configuration was successfully applied (you can then Close this dialog). Note if you ever need to review or change the security configuration settings you can launch it again and choose the Edit a security configuration option as shown here. Create BPM user groups The solution has a number of BPM Teams defined that need to map to users and groups. To do that launch the Process Admin Console and then select Server Admin section. Within that select User Management and Group Management and type denim in Select Group to Modify and you should see the groups that have been created as a result of the Team definitions. You then need to assign users and groups for your environment against those pre-defined groups. Here is an example were we have assigned a number of users to the denim-adjusters group. Configure servers The solution integrates to ODM and ECM by using defined Servers . By default these are mapped to the environment the IBM team used in testing. You have to now re-map these to your own environment based on how you configured it in the Environment section. To do this you must first ensure the deployed BAW solition is activated. Select it in Workflow Center and click the View Details icon in the lower left corner of the tile. Next choose the Snapshots section, select your snapshot and from the contextual menu you Activate it. After this you can now go to the Process Admin Console and you should see the snapshot in the list of Installed Apps . You click on the snapshot and then select the Servers section and you should see the two server definitions used ( DenimODMServer for the referenced ODM Sever and IBMBAW for the referenced ECM server). You should change the settings for the respective servers to match how you installed your environment (the Hostname , Port and user credentials need to be configured).","title":"Run"},{"location":"showtime/run-solution/#running-the-solution","text":"","title":"Running the solution"},{"location":"showtime/run-solution/#deploy-the-case-solution-in-workflow-center","text":"Login to Workflow Center and open the Case Solution named Denim Compute Auto Claims : Click the deploy icon in the upper-right corner of the Case Builder:","title":"Deploy the case solution in Workflow Center"},{"location":"showtime/run-solution/#create-case-security-configuration","text":"In the Workflow Center select the solution and click the contextual menu and then Advanced . With the Denim Compute solution selected click Actions , then Manage and then Security Configuration . Select the option to Create a security configuration and click Next . Provide a Security manifest name and click Next . Set permissions against the security Roles (in this example all permissions are assigned to each Role ) and click Next . You can then set the adminstrators, if you want you can Add others. In the Add Users and Groups modal dialogue you can add users or groups. You start typing partial names and then click the magnifying glass icon to find matching users that appear in the Available section. You can then use the arrows to move between Available and Selected and when done you click Add to complete. When done with this section click Next to continue. Next you map groups and users to the Case Roles . Select each Role and click Add then follow the earlier instructions for how to find and assign users and groups. In this example we have just added a default user to the Claim Intake Services Role . You repeat these steps for the other Roles . Here is the final situation were example users are added to each Role (if you want a more realistic scenario you should setup different users and groups and assign them to the Roles ). Click Next when done with this section. Now you can check the box next to Apply the security configuration and click Apply . You should get confirmation that the security configuration was successfully applied (you can then Close this dialog). Note if you ever need to review or change the security configuration settings you can launch it again and choose the Edit a security configuration option as shown here.","title":"Create Case security configuration"},{"location":"showtime/run-solution/#create-bpm-user-groups","text":"The solution has a number of BPM Teams defined that need to map to users and groups. To do that launch the Process Admin Console and then select Server Admin section. Within that select User Management and Group Management and type denim in Select Group to Modify and you should see the groups that have been created as a result of the Team definitions. You then need to assign users and groups for your environment against those pre-defined groups. Here is an example were we have assigned a number of users to the denim-adjusters group.","title":"Create BPM user groups"},{"location":"showtime/run-solution/#configure-servers","text":"The solution integrates to ODM and ECM by using defined Servers . By default these are mapped to the environment the IBM team used in testing. You have to now re-map these to your own environment based on how you configured it in the Environment section. To do this you must first ensure the deployed BAW solition is activated. Select it in Workflow Center and click the View Details icon in the lower left corner of the tile. Next choose the Snapshots section, select your snapshot and from the contextual menu you Activate it. After this you can now go to the Process Admin Console and you should see the snapshot in the list of Installed Apps . You click on the snapshot and then select the Servers section and you should see the two server definitions used ( DenimODMServer for the referenced ODM Sever and IBMBAW for the referenced ECM server). You should change the settings for the respective servers to match how you installed your environment (the Hostname , Port and user credentials need to be configured).","title":"Configure servers"},{"location":"usecase/use-case-definition/","text":"Use case definition Our reference implementation use case is automobile insurance claim processing. The different components we're using in the context of the use case are shown on the figure below. The capabilities involved in the first MVP are in gray, while the dashed boxes are the ones that will be included in future MVPs. Identifying personas Our standard DBA implementation methodology leverages IBM Design Thinking and in particular its focus on users and stakeholders' perspectives when designing business solutions. Here is an illustration of the use of one Design Thinking activity consisting in creating Empathy Maps, that is, looking at what a typical representative of a stakeholder role think, say, feel and do. Process discovery Process discovery and modeling is another key practice in our DBA methodology. We typically conduct it in a collaborative manner by leveraging our IBM Blueworks Live process mapping software available on the cloud as Software as a Service. Here is an example of the Process Map we defined to support the elaboration of the Denim Compute use case: The color coding is the following: Orange : an activity assumed on the Case Management side Purple : an activity requiring a BPM/ECM integration Green : a decision activity, invoking a ruleset Blue : a regular BPM activity. The overall process diagram for our use case is shown below. Since it involves ad-hoc activities, the diagram is not quite semantically correct. However, it gives a view of the intended sequence of events, along with the possible paths across the workflow. The first activity, Report Accident , represents the manual initial stimulus to start processing. It is not implemented either as a Case Activity or a BPMN Process. The first step under automated control is the following one, Gather Accident Information , which can either be initiated manually by a user starting a Claim case (say on receipt of a telephone call from the Insured to report accident) or by some document being added to the Claim folder. The following activities are not in scope of the current MVP: Assign Claim Adjuster Investigate Fraud Assess Escalated CLaim Pay Provider Invoice Process Simple Claim Reject Claim","title":"Use case definition"},{"location":"usecase/use-case-definition/#use-case-definition","text":"Our reference implementation use case is automobile insurance claim processing. The different components we're using in the context of the use case are shown on the figure below. The capabilities involved in the first MVP are in gray, while the dashed boxes are the ones that will be included in future MVPs.","title":"Use case definition"},{"location":"usecase/use-case-definition/#identifying-personas","text":"Our standard DBA implementation methodology leverages IBM Design Thinking and in particular its focus on users and stakeholders' perspectives when designing business solutions. Here is an illustration of the use of one Design Thinking activity consisting in creating Empathy Maps, that is, looking at what a typical representative of a stakeholder role think, say, feel and do.","title":"Identifying personas"},{"location":"usecase/use-case-definition/#process-discovery","text":"Process discovery and modeling is another key practice in our DBA methodology. We typically conduct it in a collaborative manner by leveraging our IBM Blueworks Live process mapping software available on the cloud as Software as a Service. Here is an example of the Process Map we defined to support the elaboration of the Denim Compute use case: The color coding is the following: Orange : an activity assumed on the Case Management side Purple : an activity requiring a BPM/ECM integration Green : a decision activity, invoking a ruleset Blue : a regular BPM activity. The overall process diagram for our use case is shown below. Since it involves ad-hoc activities, the diagram is not quite semantically correct. However, it gives a view of the intended sequence of events, along with the possible paths across the workflow. The first activity, Report Accident , represents the manual initial stimulus to start processing. It is not implemented either as a Case Activity or a BPMN Process. The first step under automated control is the following one, Gather Accident Information , which can either be initiated manually by a user starting a Claim case (say on receipt of a telephone call from the Insured to report accident) or by some document being added to the Claim folder. The following activities are not in scope of the current MVP: Assign Claim Adjuster Investigate Fraud Assess Escalated CLaim Pay Provider Invoice Process Simple Claim Reject Claim","title":"Process discovery"}]}