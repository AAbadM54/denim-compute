{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction The goal of Denim Compute is to provide a reference implementation for a Digital Business Automation (DBA) solution on Kubernetes application platforms. Denim Compute relies on the automobile insurance claim processing use case, a poster child of business automation applications, and focuses on the capabilities available in the IBM Cloud Pak for Automation to implement it. As of version 19.0.1 of the Cloud Pak, these capabilities are: Workflow , with the Business Automation Workflow (BAW) component. Content Management , with the FileNet Content Manager component. Decisions , with the Operational Decision Manager (ODM) component. Operational Intelligence , with the Business Automation Insights (BAI) component. Capture , with the Business Automation Content Analyzer (BACA) component. With this reference implementation, we intend to illustrate how the different capabilities of the platform come together through the sample use case to create a holistic automation solution. This includes demonstrating: How to install the different components of the Cloud Pak for Automation on a private cloud environment. When to use a mix of ad-hoc (Case Management) and sequential activities (BPMN). How to implement the hand-shake between Case and BPMN, and the exchange of data elements. How a machine learning model can be used in conjunction with ODM to render decisions. How to use BAI to monitor business events from decisions and process execution. How to use BACA to capture data from unstructured documents. Since the Cloud Pak for Automation is on a continuous release schedule, the architecture and implementation of Denim Compute will be aligned with the new features and capabilities of the pak as they become available. So check back often for improvements and extensions in the What's new section below. What's new On 08/30/19 The following features have been added: BACA integration scenario : The Denim Compute implementation now demonstrates automatic detection of an uploaded repair shop estimate and extraction of data from it using BACA. On 08/08/19 The following features have been added: Cloud Pak deployment on managed OpenShift : The Denim Compute has been ported and tested on a managed OpenShift cluster on IBM Cloud, and installation of the different components on OpenShift is documented. BAI dashboards for BPMN : The Denim Compute implementation now demonstrates the monitoring of BPM business events with complex BAI dashboards. On 07/12/19 Published the initial release. Documentation This documentation is built and published MkDocs. To publish the doc locally, you can install MkDocs and the Material theme, then follow these steps: Clone this repository: git clone git@github.com:ibm-cloud-architecture/denim-compute.git Go to the documentation folder: cd book Build and deploy the documentation: mkdocs serve View the published documentation in your browser at http://127.0.0.1:8000/","title":"Introduction"},{"location":"#introduction","text":"The goal of Denim Compute is to provide a reference implementation for a Digital Business Automation (DBA) solution on Kubernetes application platforms. Denim Compute relies on the automobile insurance claim processing use case, a poster child of business automation applications, and focuses on the capabilities available in the IBM Cloud Pak for Automation to implement it. As of version 19.0.1 of the Cloud Pak, these capabilities are: Workflow , with the Business Automation Workflow (BAW) component. Content Management , with the FileNet Content Manager component. Decisions , with the Operational Decision Manager (ODM) component. Operational Intelligence , with the Business Automation Insights (BAI) component. Capture , with the Business Automation Content Analyzer (BACA) component. With this reference implementation, we intend to illustrate how the different capabilities of the platform come together through the sample use case to create a holistic automation solution. This includes demonstrating: How to install the different components of the Cloud Pak for Automation on a private cloud environment. When to use a mix of ad-hoc (Case Management) and sequential activities (BPMN). How to implement the hand-shake between Case and BPMN, and the exchange of data elements. How a machine learning model can be used in conjunction with ODM to render decisions. How to use BAI to monitor business events from decisions and process execution. How to use BACA to capture data from unstructured documents. Since the Cloud Pak for Automation is on a continuous release schedule, the architecture and implementation of Denim Compute will be aligned with the new features and capabilities of the pak as they become available. So check back often for improvements and extensions in the What's new section below.","title":"Introduction"},{"location":"#whats-new","text":"","title":"What's new "},{"location":"#on-083019","text":"The following features have been added: BACA integration scenario : The Denim Compute implementation now demonstrates automatic detection of an uploaded repair shop estimate and extraction of data from it using BACA.","title":"On 08/30/19"},{"location":"#on-080819","text":"The following features have been added: Cloud Pak deployment on managed OpenShift : The Denim Compute has been ported and tested on a managed OpenShift cluster on IBM Cloud, and installation of the different components on OpenShift is documented. BAI dashboards for BPMN : The Denim Compute implementation now demonstrates the monitoring of BPM business events with complex BAI dashboards.","title":"On 08/08/19"},{"location":"#on-071219","text":"Published the initial release.","title":"On 07/12/19"},{"location":"#documentation","text":"This documentation is built and published MkDocs. To publish the doc locally, you can install MkDocs and the Material theme, then follow these steps: Clone this repository: git clone git@github.com:ibm-cloud-architecture/denim-compute.git Go to the documentation folder: cd book Build and deploy the documentation: mkdocs serve View the published documentation in your browser at http://127.0.0.1:8000/","title":"Documentation"},{"location":"references/","text":"References DBA Architecture Center","title":"References"},{"location":"references/#references","text":"DBA Architecture Center","title":"References"},{"location":"design/decisions/","text":"Business decisions The claim processing scenario currently involves three rule-based decisions: Perform claim segmentation which evaluates the information available from the initial claim intake and uses it to compute a complexity score for the claim. This score will be used to route the claim to an adjuster with the right experience or for simple cases, bypass the need to involve the adjuster. Assess fraud potential is also using the the information from the initial claim intake as well as information available on the driver as well as the policy holder if they are different persons. The decision returns a fraud propensity score that is then used to decide whether a detailed fraud investigation is warranted. Review for escalation uses information from the claim case to determine whether the claim should be reviewed by the claim manager before a claim settlement is created and proposed to the claimant. It returns a yes/no decision, with a set of justifications when the decision is to escalate. Object model The different decision are built using a common object model based on the concepts of Loss, Claim and Policy. A high-level diagram of the input model is shown below: Output of the three decisions is captured by the classes below: Rule projects The rule projects are organized as a decision service in a standard fashion, placing the BOM in a separate project that is referenced by the individual rule projects. Each rule project depending directly on the BOM project is directly associated with a specific decision operation. The claim-processing main project does not contain any rule artifact and is mainly used to define the decision operations and the deployment configuration that will allow the rulesets deployment to the Rule Execution Server. Injecting machine learning The claim fraud assessment decision service is a good candidate to inject a fraud detection service based on a machine learning (ML) model. The typical division of responsibility is that the ML model excels in detecting patterns of fraud, while rules can flag outliers, marginal or heuristic cases that have been detected but do not yet represent a pattern. NB : We are planning to integrate the ML-based scoring model for fraud in a future Denim Compute iteration. For now, we just have a clean set of labeled data (see the denim-insurance-fraud-data.csv file). This sample fraud dataset is a good fit for the AutoAI feature of Watson Studio : AutoAI will automatically determine a set of ML pipelines that perform well given the dataset and the dependent feature (in our case, the Fraudulent column). After the experiment execution completes, Watson AutoAI presents the possible pipelines to choose from, each with their associated KPIs. Once the desired pipeline is selected, it can be simply operationalized by creating a deployment, which exposes a scoring end-point: From there, the scoring service can be manually tested: Sample input: { input_data : [ { fields : [ Claim Amount , Coverage , Education , Employment Status , Income , Marital Status , Monthly Premium Auto , Months Since Last Claim , Months Since Policy Inception , Claim Reason , Sales Channel ], values : [ [ 276, Basic , Bachelor , Employed , 56274, Married , 69, 32, 5, Collision , Agent ], [ 265, Basic , High School , Unemployed , 0, Married , 70, 7, Scratch or Dent , CallCenter ] ] } ] } Sample output: { predictions : [ { fields : [ prediction , probability ], values : [ [ 0, [1, 9.530106164762553e-22]], [ 1, [0.0001811385154724121, 0.9998188614845276]] ] } ] }","title":"Decisions"},{"location":"design/decisions/#business-decisions","text":"The claim processing scenario currently involves three rule-based decisions: Perform claim segmentation which evaluates the information available from the initial claim intake and uses it to compute a complexity score for the claim. This score will be used to route the claim to an adjuster with the right experience or for simple cases, bypass the need to involve the adjuster. Assess fraud potential is also using the the information from the initial claim intake as well as information available on the driver as well as the policy holder if they are different persons. The decision returns a fraud propensity score that is then used to decide whether a detailed fraud investigation is warranted. Review for escalation uses information from the claim case to determine whether the claim should be reviewed by the claim manager before a claim settlement is created and proposed to the claimant. It returns a yes/no decision, with a set of justifications when the decision is to escalate.","title":"Business decisions"},{"location":"design/decisions/#rule-projects","text":"The rule projects are organized as a decision service in a standard fashion, placing the BOM in a separate project that is referenced by the individual rule projects. Each rule project depending directly on the BOM project is directly associated with a specific decision operation. The claim-processing main project does not contain any rule artifact and is mainly used to define the decision operations and the deployment configuration that will allow the rulesets deployment to the Rule Execution Server.","title":"Rule projects"},{"location":"design/decisions/#injecting-machine-learning","text":"The claim fraud assessment decision service is a good candidate to inject a fraud detection service based on a machine learning (ML) model. The typical division of responsibility is that the ML model excels in detecting patterns of fraud, while rules can flag outliers, marginal or heuristic cases that have been detected but do not yet represent a pattern. NB : We are planning to integrate the ML-based scoring model for fraud in a future Denim Compute iteration. For now, we just have a clean set of labeled data (see the denim-insurance-fraud-data.csv file). This sample fraud dataset is a good fit for the AutoAI feature of Watson Studio : AutoAI will automatically determine a set of ML pipelines that perform well given the dataset and the dependent feature (in our case, the Fraudulent column). After the experiment execution completes, Watson AutoAI presents the possible pipelines to choose from, each with their associated KPIs. Once the desired pipeline is selected, it can be simply operationalized by creating a deployment, which exposes a scoring end-point: From there, the scoring service can be manually tested: Sample input: { input_data : [ { fields : [ Claim Amount , Coverage , Education , Employment Status , Income , Marital Status , Monthly Premium Auto , Months Since Last Claim , Months Since Policy Inception , Claim Reason , Sales Channel ], values : [ [ 276, Basic , Bachelor , Employed , 56274, Married , 69, 32, 5, Collision , Agent ], [ 265, Basic , High School , Unemployed , 0, Married , 70, 7, Scratch or Dent , CallCenter ] ] } ] } Sample output: { predictions : [ { fields : [ prediction , probability ], values : [ [ 0, [1, 9.530106164762553e-22]], [ 1, [0.0001811385154724121, 0.9998188614845276]] ] } ] }","title":"Injecting machine learning"},{"location":"design/insights/","text":"Operational intelligence The Business Automation Insights (BAI) component provides capabilities to visualize the business events that are generated by an automation solution, as well as feed these events to a data lake so that deeper insights can be derived about the solution, in particular by applying machine learning algorithms to the events. In the following sections, we describe how BAI is put to work for the different components of the Denim Compute solution. ODM dashboard The Denim Compute implementation uses ODM events collected by BAI to present a Kibana dashboard that shows the distribution of the different business decision outcomes over a period of time. For the claim segmentation decision, the visualization is the portion of low (green), medium (orange) and high (red) complexity claims processed in past period. For escalation review , the portion of escalated claims (dark blue) versus the claims that do not need escalation (light blue) is displayed. For fraud assessment , the fraud median score as well as the maximum score over the past period is displayed. A sample view of this dashboard is shown below. The dashboard is created following the steps in this tutorial , using the following ODM output parameters fields to build the search: Decision Target Kibana field BOM attribute Assess Fraud data.claim_processing.assess_fraud.out.fraud.score FraudAssessment.score Review Escalation data.claim_processing.review_escalation.out.escalation.required EscalationAssessment.required Segment Claim data.claim_processing.segment_claim.out.complexity.complexity ComplexityAssessment.complexity BAW dashboard The BAW data sent to BAI as Dynamic Event Framework (DEF) events is then visualized on a set of Kibana dashboards, this is illustrated in detail in BAI scenario walkthrough . That scenario shows how to batch load the data in order to see realistic trends in the dashboards, however the concepts are the same for the main scenario described in Main scenario walkthrough . The core concept for sending DEF events to BAI is using tracking group definitions in BAW which emits data of relevance at critical points in the workflow (see Workflow design to familiarize yourself with the workflow). This can be seen in the fragment of Initiate Claims Processing below where a selection of the various tracking points are highlighted. Two different tracking groups are used in the workflow, one for fairly static data that is available at the start (such as information from the Policy and the First Notice of Loss) and one for data that is dynamically adjusted as the workflow progresses (e.g. the various amounts of estimates and adjustments). Below you can see that the tracking has been configured to use the tracking group named AutoClaimTG and the various state of workflow instance variables are mapped to the corresponding properties of AutoClaimTG . And here we see an example from downstream in the workflow where data updates are available after Claim Settlement and they are mapped to the corresponding properties in the tracking group named AutoClaimUpdatesTG . In the Kibana console you then see the results of those DEF events where they are aggregated into process summaries and then exposed as various visualizations on a dashboard. In the highlighted example is a specific visualization that shows various aggregated metrics. Here you can see the definition of that visualization and a specific metric is highlighted where it averages the values from the claimSettlementAmount property in AutoClaimUpdatesTG . The visualization in turn uses an Elasticsearch search shown below. It has various filters applied against the index pattern named process-s* in order to find specific summaries of interest to our scenario. The columns have been configured to highlight the base data that then feeds into the aggregation that calculated the average that we saw previously in the visualization. For further details on how the BAI integration with BAW was achieved, please see the insights development section.","title":"Insights"},{"location":"design/insights/#operational-intelligence","text":"The Business Automation Insights (BAI) component provides capabilities to visualize the business events that are generated by an automation solution, as well as feed these events to a data lake so that deeper insights can be derived about the solution, in particular by applying machine learning algorithms to the events. In the following sections, we describe how BAI is put to work for the different components of the Denim Compute solution.","title":"Operational intelligence"},{"location":"design/insights/#odm-dashboard","text":"The Denim Compute implementation uses ODM events collected by BAI to present a Kibana dashboard that shows the distribution of the different business decision outcomes over a period of time. For the claim segmentation decision, the visualization is the portion of low (green), medium (orange) and high (red) complexity claims processed in past period. For escalation review , the portion of escalated claims (dark blue) versus the claims that do not need escalation (light blue) is displayed. For fraud assessment , the fraud median score as well as the maximum score over the past period is displayed. A sample view of this dashboard is shown below. The dashboard is created following the steps in this tutorial , using the following ODM output parameters fields to build the search: Decision Target Kibana field BOM attribute Assess Fraud data.claim_processing.assess_fraud.out.fraud.score FraudAssessment.score Review Escalation data.claim_processing.review_escalation.out.escalation.required EscalationAssessment.required Segment Claim data.claim_processing.segment_claim.out.complexity.complexity ComplexityAssessment.complexity","title":"ODM dashboard"},{"location":"design/insights/#baw-dashboard","text":"The BAW data sent to BAI as Dynamic Event Framework (DEF) events is then visualized on a set of Kibana dashboards, this is illustrated in detail in BAI scenario walkthrough . That scenario shows how to batch load the data in order to see realistic trends in the dashboards, however the concepts are the same for the main scenario described in Main scenario walkthrough . The core concept for sending DEF events to BAI is using tracking group definitions in BAW which emits data of relevance at critical points in the workflow (see Workflow design to familiarize yourself with the workflow). This can be seen in the fragment of Initiate Claims Processing below where a selection of the various tracking points are highlighted. Two different tracking groups are used in the workflow, one for fairly static data that is available at the start (such as information from the Policy and the First Notice of Loss) and one for data that is dynamically adjusted as the workflow progresses (e.g. the various amounts of estimates and adjustments). Below you can see that the tracking has been configured to use the tracking group named AutoClaimTG and the various state of workflow instance variables are mapped to the corresponding properties of AutoClaimTG . And here we see an example from downstream in the workflow where data updates are available after Claim Settlement and they are mapped to the corresponding properties in the tracking group named AutoClaimUpdatesTG . In the Kibana console you then see the results of those DEF events where they are aggregated into process summaries and then exposed as various visualizations on a dashboard. In the highlighted example is a specific visualization that shows various aggregated metrics. Here you can see the definition of that visualization and a specific metric is highlighted where it averages the values from the claimSettlementAmount property in AutoClaimUpdatesTG . The visualization in turn uses an Elasticsearch search shown below. It has various filters applied against the index pattern named process-s* in order to find specific summaries of interest to our scenario. The columns have been configured to highlight the base data that then feeds into the aggregation that calculated the average that we saw previously in the visualization. For further details on how the BAI integration with BAW was achieved, please see the insights development section.","title":"BAW dashboard"},{"location":"design/sundries/","text":"DBA solution design findings Case Design Leveraging case persistence The claims solution implements a Policy case which is persisted as a record to represent the insurance policy. The Policy case type stores the policy information and claims are generated from the policy by a create claim activity, which creates the new claim case and also transfer existing policy information into the claim. Case data model and business objects The Case data model currently does not fully support Business Objects, therefore an interim solution have been implemented to map properties. The interface of the Case Manager data model with other BAW components may require the implementation of distinct case properties to match each element of a business object. A new upcoming version of Case Manager will fully support business objects and allow better data integration with other BAW components.","title":"Sundries"},{"location":"design/sundries/#dba-solution-design-findings","text":"","title":"DBA solution design findings"},{"location":"design/sundries/#case-design","text":"","title":"Case Design"},{"location":"design/sundries/#leveraging-case-persistence","text":"The claims solution implements a Policy case which is persisted as a record to represent the insurance policy. The Policy case type stores the policy information and claims are generated from the policy by a create claim activity, which creates the new claim case and also transfer existing policy information into the claim.","title":"Leveraging case persistence"},{"location":"design/sundries/#case-data-model-and-business-objects","text":"The Case data model currently does not fully support Business Objects, therefore an interim solution have been implemented to map properties. The interface of the Case Manager data model with other BAW components may require the implementation of distinct case properties to match each element of a business object. A new upcoming version of Case Manager will fully support business objects and allow better data integration with other BAW components.","title":"Case data model and business objects"},{"location":"design/workflow/","text":"Workflow design This section covers the design of the Denim Compute workflow using the Business Automation Workflow (BAW) platform. It is recommended to be familiar with the scenario walkthrough in conjunction with this section. Case and process collaboration Denim Compute uses the combined capabilities of Case Management and Process flow from Business Automation Workflow in the solution scenario. Case-oriented workflows are typically modeled in CMMN notation while Process-oriented ones use BPMN notation. In order to show the collaboration between the Case and Process aspects we show a BPMN collaboration diagram as a close approximation of how the separation of focus is achieved. The following two figures show the high level workflow as a BPMN Collaboration with Case ad-hoc activities in one pool and Process directed acyclic graph in the other pool and message exchanges between both as control passes back and forth. Within the case activities, there is one special one (highlighted in the image below) that has scope for the duration of the Process it invokes. This is because the manner in which other Case activities are invoked is via property updates that in turn trigger pre-conditions on the activities. In order to update a Case property the Process must be able to reference its parent Case activity which remains in scope. If we look at the Case Builder part of Business Automation Workflow, the highlighted activities match the earlier depiction whereby Gather Accident Information is a P8 Process implementation that updates a case property that then triggers the Initiate Claims Processing activity which is a BPM Process implementation. The Initiate Claims Processing Process shown below is then responsible for co-ordinating both Process steps and communication back to Case to request further ad-hoc activities. Highlighted shows an example of the earlier model's message exchange. In the outbound message it is achieved by using the Javascript API to update a Case property which then triggers the pre-condition of a Case activity. The intermediate receiving message event then represents the return inbound message signaling the ad-hoc activity has finished. Here in Case Builder again are the relevant activities with the first one ( Create Adjuster Report ) implemented as a P8 Process which then triggers the Signal Adjuster Report Created implemented as a BPM Process which will then send the message event to the main awaiting process. Note this was a design decision to use the message pair capabilities in BPM, an alternative would be to implement a step in Create Adjuster Report P8 that would need to make a REST call to send the message event. This is the BPM Process implementation of Signal Adjuster Report Created with the message send event highlighted. Mediated BACA integration A side scenario of the main scenario involves integration with Business Automation Content Analyzer (BACA) in order to automatically detect an uploaded document representing a vehicle repair estimate provided by a repair shop. The document is parsed by BACA and the data extracted to build up an estimate data structure that includes the set of vehicle parts needed to repair the damaged vehicle. In the Case, a case activity named Process Repair Estimate is configured to be triggered by the precondition of a document of class Auto Repair Estimate being added to the case. The case activity implementation shown below involves integrating to BACA and deciding whether the resulting parsed data is a valid estimate or not. If valid it is then sent as a message event using a publish-subscribe paradigm for interested subscribers to react to the newly available repair estimate. The integration to BACA has to be mediated because BACA provides a generic interface and the captured data contains a lot of details that our BAW business scenario is not interested in. A typical pattern for implementing a mediation between such business and technical concerns would be to use features of API Connect (APIC) and Application Connect Enterprise (ACE) from the IBM Cloud Pak for Integration . In this scenario, we have opted to use a different approach to the mediation for two reasons: To avoid readers having to obtain licensing for and install another IBM Cloud Pak. To illustrate how to develop and deploy a cloud native micro-service to the Red Hat OpenShift Container Platform and then integrate that with components from IBM Cloud Pak for Automation. After the message is published the subscriber is an in-flight instance of the Provide Estimates Per Repairer process. In that the user task has an interrupting boundary message event configured with correlation details set (such as the specific Auto Claim and the Vehicle VIN ) so that the specific instance of this process can be identified and the message delivered to it.","title":"Workflow"},{"location":"design/workflow/#workflow-design","text":"This section covers the design of the Denim Compute workflow using the Business Automation Workflow (BAW) platform. It is recommended to be familiar with the scenario walkthrough in conjunction with this section.","title":"Workflow design"},{"location":"design/workflow/#case-and-process-collaboration","text":"Denim Compute uses the combined capabilities of Case Management and Process flow from Business Automation Workflow in the solution scenario. Case-oriented workflows are typically modeled in CMMN notation while Process-oriented ones use BPMN notation. In order to show the collaboration between the Case and Process aspects we show a BPMN collaboration diagram as a close approximation of how the separation of focus is achieved. The following two figures show the high level workflow as a BPMN Collaboration with Case ad-hoc activities in one pool and Process directed acyclic graph in the other pool and message exchanges between both as control passes back and forth. Within the case activities, there is one special one (highlighted in the image below) that has scope for the duration of the Process it invokes. This is because the manner in which other Case activities are invoked is via property updates that in turn trigger pre-conditions on the activities. In order to update a Case property the Process must be able to reference its parent Case activity which remains in scope. If we look at the Case Builder part of Business Automation Workflow, the highlighted activities match the earlier depiction whereby Gather Accident Information is a P8 Process implementation that updates a case property that then triggers the Initiate Claims Processing activity which is a BPM Process implementation. The Initiate Claims Processing Process shown below is then responsible for co-ordinating both Process steps and communication back to Case to request further ad-hoc activities. Highlighted shows an example of the earlier model's message exchange. In the outbound message it is achieved by using the Javascript API to update a Case property which then triggers the pre-condition of a Case activity. The intermediate receiving message event then represents the return inbound message signaling the ad-hoc activity has finished. Here in Case Builder again are the relevant activities with the first one ( Create Adjuster Report ) implemented as a P8 Process which then triggers the Signal Adjuster Report Created implemented as a BPM Process which will then send the message event to the main awaiting process. Note this was a design decision to use the message pair capabilities in BPM, an alternative would be to implement a step in Create Adjuster Report P8 that would need to make a REST call to send the message event. This is the BPM Process implementation of Signal Adjuster Report Created with the message send event highlighted.","title":"Case and process collaboration"},{"location":"design/workflow/#mediated-baca-integration","text":"A side scenario of the main scenario involves integration with Business Automation Content Analyzer (BACA) in order to automatically detect an uploaded document representing a vehicle repair estimate provided by a repair shop. The document is parsed by BACA and the data extracted to build up an estimate data structure that includes the set of vehicle parts needed to repair the damaged vehicle. In the Case, a case activity named Process Repair Estimate is configured to be triggered by the precondition of a document of class Auto Repair Estimate being added to the case. The case activity implementation shown below involves integrating to BACA and deciding whether the resulting parsed data is a valid estimate or not. If valid it is then sent as a message event using a publish-subscribe paradigm for interested subscribers to react to the newly available repair estimate. The integration to BACA has to be mediated because BACA provides a generic interface and the captured data contains a lot of details that our BAW business scenario is not interested in. A typical pattern for implementing a mediation between such business and technical concerns would be to use features of API Connect (APIC) and Application Connect Enterprise (ACE) from the IBM Cloud Pak for Integration . In this scenario, we have opted to use a different approach to the mediation for two reasons: To avoid readers having to obtain licensing for and install another IBM Cloud Pak. To illustrate how to develop and deploy a cloud native micro-service to the Red Hat OpenShift Container Platform and then integrate that with components from IBM Cloud Pak for Automation. After the message is published the subscriber is an in-flight instance of the Provide Estimates Per Repairer process. In that the user task has an interrupting boundary message event configured with correlation details set (such as the specific Auto Claim and the Vehicle VIN ) so that the specific instance of this process can be identified and the message delivered to it.","title":"Mediated BACA integration "},{"location":"development/bai-emulated-workflow/","text":"Workflow for emulated BAI Introduction and rationale The integration with Business Automation Insights (BAI) is best illustrated in the special scenario BAI scenario walkthrough . While the main scenario BAW workflow as seen in Main scenario walkthrough is also integrated with BAI, the amount of human interaction in that workflow makes it hard in a short space of elapsed time to demonstrate a realistic BAI business situation where there would be 100s, 1000s or more claim instances processed. Therefore an additional workflow was designed and implemented in order to provide an emulated close approximation of the main workflow while also allowing for batch loading of large amounts of instances so that the resulting BAI Dashboards would have interesting and more true-to-life aggregated data to gain insights from. Emulated auto claims workflow The BAI emulation workflow is accessed from the Process Apps section of BAW Workflow Center as shown below. The Emulate Auto Claim Processing process is shown below. It matches the main steps with tracking enabled in the main scenario workflow (in terms of the activity names and the flow of control). All the steps are system steps with no human interaction so that the emulation can run without any need for intervention. The color-coding shows that there are Straight Through Processing (STP) activities (in green) and emulated long-running activities (in orange). The data passed around in variables is a EmulatedAutoClaim Business Object (BO) which contains controlData of type ClaimControl and processData of type ClaimData . The ClaimControl BO shown expanded here contains settings that are used to determine what kind of an Auto Claim instance this will be and what emulated wait times to implement for each of the long-running activities in scope. The ClaimData BO contains attributes that map one-to-one to the tracked fields that are defined in the Tracking Groups used to send business event data over the Dynamic Event Framework (DEF) to BAI. Looking a little further at the process activities, here we see an example of an STP activity Claim Initial Preparation which is implemented as a Service Flow named Activity Controller (this is a generic service that implements all the activity logic). In the data mapping for the activity the autoClaim.processData and autoClaim.controlData are passed in and out of the service along with the activityName . The initial instantiation and population of these BOs is done in the emulation batch generator discussed in Emulation batch generator later. In the Activity Controller Service Flow the script step contains various helper functions and in the main logic it has a switch statement to provide specific processing according to he passed in activityName . It updates the business data in claimData using values of other business data and control settings in control (which is the ClaimControl BO). Back in Emulate Auto Claim Processing the activity Estimate Damage is an example of a long-running activity (in the main scenario it would have one or more human interactions). It is implemented by a linked process named Emulate Long Running Activity . The Emulate Long Running Activity process also calls the Activity Controller Service Flow , however it does it after a timer delay where the amount of time is controlled by activityWaitTime which has been passed in to this process from the calling process Emulate Auto Claim Processing . Here again in Activity Controller you can see the relevant part of the script where the processing for the Estimate Damage activity takes place. The estimates are derived by calling a function getVehicleDamage . The called function within the script is shown here, it uses various random assignments of values according to the relative perceived values of the various vehicle makes (for example repairs to a Mercedes are likely to be most expensive relative to the configured maximum claim amount). Emulation batch generator The emulation batch generator is a process that allows the user to set control values that control a batch run of between 100 and 10000 instances of an emulated auto claim. The process definition Denim Compute Auto Claims - Emulation Generator is shown below. The user interaction to allow setting the emulation control parameters is an activity Set Emulation Controls which is implemented as a Client-Side Human Service with the same name. The data collected is then passed as output into the process (to the tw.local.control variable). The variable is an instance of EmulationControl BO shown below which stores the user settings to be later used in setting up the batch of instances and configuring the control data on each instance in the batch. The next activity in the process ( Configure Batch Control ) uses this data to then set up how the batches are to be controlled. The rest of the Denim Compute Auto Claims - Emulation Generator process involves looping for the entire requested set of instances and within that setting up a batch (with a time delay between them in Set Batch Delay ) and looping within that batch to configure ( Configure Emulation Instances ) and start ( Start Auto Claim Instance ) emulated claim instances. The activity Set Batch Delay is implemented by a Linked Process named Emulate - Set Batch Delay which uses a Timer to delay a few minutes between each batch submission. The Configure Emulation Instances Service Flow implements the activity Configure Emulation Instances and is shown below. This service uses the over control data for the emulation and the batch parameters to create a list of EmulatedAutoClaim BOs where each will be sent as the start data for creating and instance of Emulate Auto Claim Processing (see Emulated Auto Claims workflow above). The Create Control Data step sets up the control data for a specific instance, for example as shown in the code it checks what the scenario type is and sets things like the timing delays based on that (and it in turn uses helper functions to get random numbers within ranges to calculate the timings and ensure a spread of differentiated data). Create Control Data calls a further service flow named Create Initial Claim Data which sets the values of static data such as policy and loss data pertaining to the claim. The example below shows some of the potential options values for the data in Create Policy Related Data . The script also includes some helper functions referenced in the main logic to get random data and it also chooses from the options lists based on a roll-of-the-dice additional random choice. Each emulation run includes a single outlier case which is designed to skew the normal data trends and be identifiable in the BAI Dashboards . When the instance within the batch within the emulation has been designated to contain that outlier is encountered, the logic in Set Outlier Overrides is used to set special values as shown here (hint: use your favorite search engine if you don't know what the otherVehicleMake / model is referring to). Finally to complete the story, the process activity Start Auto Claim Instance is implemented by a call to the Linked Process named Emulate - Start Auto Claim Instance . This then uses the passed in instance of EmulatedAutoClaim BO that has been configured in previous steps to invoke and start a process instance via a send message event as shown here.","title":"Workflow for emulated BAI"},{"location":"development/bai-emulated-workflow/#workflow-for-emulated-bai","text":"","title":"Workflow for emulated BAI"},{"location":"development/bai-emulated-workflow/#introduction-and-rationale","text":"The integration with Business Automation Insights (BAI) is best illustrated in the special scenario BAI scenario walkthrough . While the main scenario BAW workflow as seen in Main scenario walkthrough is also integrated with BAI, the amount of human interaction in that workflow makes it hard in a short space of elapsed time to demonstrate a realistic BAI business situation where there would be 100s, 1000s or more claim instances processed. Therefore an additional workflow was designed and implemented in order to provide an emulated close approximation of the main workflow while also allowing for batch loading of large amounts of instances so that the resulting BAI Dashboards would have interesting and more true-to-life aggregated data to gain insights from.","title":"Introduction and rationale"},{"location":"development/bai-emulated-workflow/#emulated-auto-claims-workflow","text":"The BAI emulation workflow is accessed from the Process Apps section of BAW Workflow Center as shown below. The Emulate Auto Claim Processing process is shown below. It matches the main steps with tracking enabled in the main scenario workflow (in terms of the activity names and the flow of control). All the steps are system steps with no human interaction so that the emulation can run without any need for intervention. The color-coding shows that there are Straight Through Processing (STP) activities (in green) and emulated long-running activities (in orange). The data passed around in variables is a EmulatedAutoClaim Business Object (BO) which contains controlData of type ClaimControl and processData of type ClaimData . The ClaimControl BO shown expanded here contains settings that are used to determine what kind of an Auto Claim instance this will be and what emulated wait times to implement for each of the long-running activities in scope. The ClaimData BO contains attributes that map one-to-one to the tracked fields that are defined in the Tracking Groups used to send business event data over the Dynamic Event Framework (DEF) to BAI. Looking a little further at the process activities, here we see an example of an STP activity Claim Initial Preparation which is implemented as a Service Flow named Activity Controller (this is a generic service that implements all the activity logic). In the data mapping for the activity the autoClaim.processData and autoClaim.controlData are passed in and out of the service along with the activityName . The initial instantiation and population of these BOs is done in the emulation batch generator discussed in Emulation batch generator later. In the Activity Controller Service Flow the script step contains various helper functions and in the main logic it has a switch statement to provide specific processing according to he passed in activityName . It updates the business data in claimData using values of other business data and control settings in control (which is the ClaimControl BO). Back in Emulate Auto Claim Processing the activity Estimate Damage is an example of a long-running activity (in the main scenario it would have one or more human interactions). It is implemented by a linked process named Emulate Long Running Activity . The Emulate Long Running Activity process also calls the Activity Controller Service Flow , however it does it after a timer delay where the amount of time is controlled by activityWaitTime which has been passed in to this process from the calling process Emulate Auto Claim Processing . Here again in Activity Controller you can see the relevant part of the script where the processing for the Estimate Damage activity takes place. The estimates are derived by calling a function getVehicleDamage . The called function within the script is shown here, it uses various random assignments of values according to the relative perceived values of the various vehicle makes (for example repairs to a Mercedes are likely to be most expensive relative to the configured maximum claim amount).","title":"Emulated auto claims workflow"},{"location":"development/bai-emulated-workflow/#emulation-batch-generator","text":"The emulation batch generator is a process that allows the user to set control values that control a batch run of between 100 and 10000 instances of an emulated auto claim. The process definition Denim Compute Auto Claims - Emulation Generator is shown below. The user interaction to allow setting the emulation control parameters is an activity Set Emulation Controls which is implemented as a Client-Side Human Service with the same name. The data collected is then passed as output into the process (to the tw.local.control variable). The variable is an instance of EmulationControl BO shown below which stores the user settings to be later used in setting up the batch of instances and configuring the control data on each instance in the batch. The next activity in the process ( Configure Batch Control ) uses this data to then set up how the batches are to be controlled. The rest of the Denim Compute Auto Claims - Emulation Generator process involves looping for the entire requested set of instances and within that setting up a batch (with a time delay between them in Set Batch Delay ) and looping within that batch to configure ( Configure Emulation Instances ) and start ( Start Auto Claim Instance ) emulated claim instances. The activity Set Batch Delay is implemented by a Linked Process named Emulate - Set Batch Delay which uses a Timer to delay a few minutes between each batch submission. The Configure Emulation Instances Service Flow implements the activity Configure Emulation Instances and is shown below. This service uses the over control data for the emulation and the batch parameters to create a list of EmulatedAutoClaim BOs where each will be sent as the start data for creating and instance of Emulate Auto Claim Processing (see Emulated Auto Claims workflow above). The Create Control Data step sets up the control data for a specific instance, for example as shown in the code it checks what the scenario type is and sets things like the timing delays based on that (and it in turn uses helper functions to get random numbers within ranges to calculate the timings and ensure a spread of differentiated data). Create Control Data calls a further service flow named Create Initial Claim Data which sets the values of static data such as policy and loss data pertaining to the claim. The example below shows some of the potential options values for the data in Create Policy Related Data . The script also includes some helper functions referenced in the main logic to get random data and it also chooses from the options lists based on a roll-of-the-dice additional random choice. Each emulation run includes a single outlier case which is designed to skew the normal data trends and be identifiable in the BAI Dashboards . When the instance within the batch within the emulation has been designated to contain that outlier is encountered, the logic in Set Outlier Overrides is used to set special values as shown here (hint: use your favorite search engine if you don't know what the otherVehicleMake / model is referring to). Finally to complete the story, the process activity Start Auto Claim Instance is implemented by a call to the Linked Process named Emulate - Start Auto Claim Instance . This then uses the passed in instance of EmulatedAutoClaim BO that has been configured in previous steps to invoke and start a process instance via a send message event as shown here.","title":"Emulation batch generator"},{"location":"development/capture/","text":"Capture development Business Automation Content Analyzer (BACA) is a no code environment. So, although we include this topic under the Development section, it is really rather configuration that is done for a BACA solution. It is used in the solution when BAW needs to automatically process an uploaded vehicle repair estimate. For further details please see the design section Mediated BACA integration . BACA ontology In the BACA front-end, the Ontology Management Tool is used to define document classes such as our Auto Repair Estimate and within that, we define a set of keys which is data we want to train BACA to recognize in uploaded documents and to parse the results out (in our case we want the JSON representation). In the image below we show the set of keys and have highlighted one example repairer code . We also then define a set of key aliases for a key to represent the different terms we might expect in a document that really mean the field is a repairer code . Just to illustrate the different keys, below we see the focus on tax rate and it expanded to show the various key aliases defined for it. Note that the Showtime section documents that in BAW, there are a number of environment variables related to BACA that need to be specified when configuring the deployed solution. The API section of the BACA front-end is where you find the API Key and the host information for the endpoint (the Request URL section). BACA-related micro-services A BACA Mediator micro-service has been developed that is used by BAW (as described in BACA integration ) to interact with BACA. The micro-service is implemented in Node.js and deployed on the Red Hat OpenShift Container Platform (OCP). Analyze a document For this single micro-service, and because we are not showing a full CI/CD DevOps cycle, we are using a simple code editor to display the code. If you are interested in Cloud Native development using the latest tooling, then have a look at Kabanero . The application is shown here with the core Node.js code ( server.js ) to declare dependencies and use Express to start a HTTP Server listening for inbound REST requests. Node.js is a modular runtime, you declare dependencies on other packages and then specify them (along with the metadata about the application itself) in the package.json file shown here. Returning to server.js , we expose a HTTP POST method to allow consumers (in our case this is BAW) to send a document to be analyzed by BACA. The code is commented with the major steps involved in processing the file and then delegating to the BACAAccessor class to perform the analysis. We expose the various services using OpenAPI 2.0 and here we see the provided YAML document describing the interface. This is where the operations that BAW invokes are referenced and how they map to the REST paths. The BACAAccessor class is shown below. It exposes methods to implement the core logic for each of the REST methods exposed in server.js . Here you can see the analyze method of the class. The remainder of the method implementation uses the request Node.js module to make the outbound HTTP REST API call to BACA and process the results (using in turn some helper mapper functions). The mapping helpers are provided in utils/mapping-utils.js and they in turn make use of a common Node.js utility module called lodash . Retrieve analysis status The other exposed REST methods follow a similar pattern. After uploading a document to BACA for analysis you check the status to verify it was parsed successfully. The /status/:analyzerId REST method is shown below. The delegated implementation is the status method in BACAAccessor as shown below. Retrieve JSON result Once an uploaded analysis has completed and the status reports it has been parsed successfully, you then need to call BACA to retrieve the results. This is where the micro-service has the most post-processing to do because the JSON payload from BACA is quite complex and we need to expose a more business interface to BAW plus we have to manipulate the BACA returned JSON in order to construct the nested array structure of Vehicle Parts inside the estimate. As with the other methods, the point of entry is in server.js which then delegates to BACAAccessor to do the heavy lifting. It also passes in a callback function ( processBACARetrieve ) that will process the response. In BACAAccessor the retrieve method invokes BACA and then processes the response. It uses helper methods and functions to process the response which we will see next. BACA returns identified data as a set of key-value pairs (KVP). The mapKVPData method shown below is used to extract those KVP entries that match the ones that we are interested in (as represented in the passed in parameter kvpKeys ). The next helper function that is invoked parseTableList is provided in the utils\\tablelist-utils.js file. BACA when it encounters a table construct in a document returns the data in a complex nested structure within the JSON referred to as the TableList . This has to be processed in order to identify the table and data within it that maps to our desire list of vehicle part objects. This is done by calling the function getTableForClass (shown here) to get the specific table and then createItemsList to process the items from it. The createItemsList function has to walk a tree of complex nested structures returned from BACA in order to identify (based on BACA ontology keys ) object structures that are intended by the presence of repeated data in a table under column headings. Back in server.js the callback function that was passed to the BACAAccessor method is invoked with the response. It first sets up some mappings in order to translate the BACA objects to ones that are expected by BAW and are defined in the OpenAPI interface of our micro-service. The function then invokes a helper function mapEstimateData to create the final response data and then sets it on the HTTP Response. This helper function is defined in the utils\\estimate-mappers.js file. It in turn invokes a number of other functions to parse different parts of the target structure (we will look at these next). The getSourceValue function shown below is used to map and manipulate the top level KVP data that is in the estimate (the vehicle identity and total amounts fields). The extractEstimateParts function first calls getCandidatePart in order to get the part data from the array and then it filters on those that have a valid first property representing the key of the object (the sku in our case). This is necessary because not all candidate parts are in fact parts because the repairers also include labor costs in the table which has a similar structure but it not in fact a part object at all. The getCandidatePart function is used to map the item to a potential part (which may also be the special labor cost item) and convert the more generic data to the specific types expected. Finally the extractLaborCost function reuses getCandidatePart and then parses the data to identify the special case where the candidate has no sku property but contains the other expected fields. It then calculates the total cost to return to the caller. Delete resources Once you have finished with an analysis in BACA, there is an API to call that will remove any resources used (such as the uploaded document the analysis was performed against). As with the other methods, the point of entry is in server.js which then delegates to the cleanup method in BACAAccessor . It then sends the HTTP Response on return from BACAAccessor . The cleanup method is shown which then invokes BACA using the r2 Node.js module and processes the response. Packaging In order to run the micro-service in a Kubernetes environment (such as our target OCP environment), we have to provide a declarative Dockerfile that is used by Docker to build the container for deployment. This is shown below which contains various directives for copying source code and configuring permissions and specifying the initial entry script to run. And here is that entry script which simple invokes an npm command that was defined earlier in the Node.js packaging file package.json .","title":"Capture"},{"location":"development/capture/#capture-development","text":"Business Automation Content Analyzer (BACA) is a no code environment. So, although we include this topic under the Development section, it is really rather configuration that is done for a BACA solution. It is used in the solution when BAW needs to automatically process an uploaded vehicle repair estimate. For further details please see the design section Mediated BACA integration .","title":"Capture development"},{"location":"development/capture/#baca-ontology","text":"In the BACA front-end, the Ontology Management Tool is used to define document classes such as our Auto Repair Estimate and within that, we define a set of keys which is data we want to train BACA to recognize in uploaded documents and to parse the results out (in our case we want the JSON representation). In the image below we show the set of keys and have highlighted one example repairer code . We also then define a set of key aliases for a key to represent the different terms we might expect in a document that really mean the field is a repairer code . Just to illustrate the different keys, below we see the focus on tax rate and it expanded to show the various key aliases defined for it. Note that the Showtime section documents that in BAW, there are a number of environment variables related to BACA that need to be specified when configuring the deployed solution. The API section of the BACA front-end is where you find the API Key and the host information for the endpoint (the Request URL section).","title":"BACA ontology"},{"location":"development/capture/#baca-related-micro-services","text":"A BACA Mediator micro-service has been developed that is used by BAW (as described in BACA integration ) to interact with BACA. The micro-service is implemented in Node.js and deployed on the Red Hat OpenShift Container Platform (OCP).","title":"BACA-related micro-services "},{"location":"development/capture/#analyze-a-document","text":"For this single micro-service, and because we are not showing a full CI/CD DevOps cycle, we are using a simple code editor to display the code. If you are interested in Cloud Native development using the latest tooling, then have a look at Kabanero . The application is shown here with the core Node.js code ( server.js ) to declare dependencies and use Express to start a HTTP Server listening for inbound REST requests. Node.js is a modular runtime, you declare dependencies on other packages and then specify them (along with the metadata about the application itself) in the package.json file shown here. Returning to server.js , we expose a HTTP POST method to allow consumers (in our case this is BAW) to send a document to be analyzed by BACA. The code is commented with the major steps involved in processing the file and then delegating to the BACAAccessor class to perform the analysis. We expose the various services using OpenAPI 2.0 and here we see the provided YAML document describing the interface. This is where the operations that BAW invokes are referenced and how they map to the REST paths. The BACAAccessor class is shown below. It exposes methods to implement the core logic for each of the REST methods exposed in server.js . Here you can see the analyze method of the class. The remainder of the method implementation uses the request Node.js module to make the outbound HTTP REST API call to BACA and process the results (using in turn some helper mapper functions). The mapping helpers are provided in utils/mapping-utils.js and they in turn make use of a common Node.js utility module called lodash .","title":"Analyze a document"},{"location":"development/capture/#retrieve-analysis-status","text":"The other exposed REST methods follow a similar pattern. After uploading a document to BACA for analysis you check the status to verify it was parsed successfully. The /status/:analyzerId REST method is shown below. The delegated implementation is the status method in BACAAccessor as shown below.","title":"Retrieve analysis status"},{"location":"development/capture/#retrieve-json-result","text":"Once an uploaded analysis has completed and the status reports it has been parsed successfully, you then need to call BACA to retrieve the results. This is where the micro-service has the most post-processing to do because the JSON payload from BACA is quite complex and we need to expose a more business interface to BAW plus we have to manipulate the BACA returned JSON in order to construct the nested array structure of Vehicle Parts inside the estimate. As with the other methods, the point of entry is in server.js which then delegates to BACAAccessor to do the heavy lifting. It also passes in a callback function ( processBACARetrieve ) that will process the response. In BACAAccessor the retrieve method invokes BACA and then processes the response. It uses helper methods and functions to process the response which we will see next. BACA returns identified data as a set of key-value pairs (KVP). The mapKVPData method shown below is used to extract those KVP entries that match the ones that we are interested in (as represented in the passed in parameter kvpKeys ). The next helper function that is invoked parseTableList is provided in the utils\\tablelist-utils.js file. BACA when it encounters a table construct in a document returns the data in a complex nested structure within the JSON referred to as the TableList . This has to be processed in order to identify the table and data within it that maps to our desire list of vehicle part objects. This is done by calling the function getTableForClass (shown here) to get the specific table and then createItemsList to process the items from it. The createItemsList function has to walk a tree of complex nested structures returned from BACA in order to identify (based on BACA ontology keys ) object structures that are intended by the presence of repeated data in a table under column headings. Back in server.js the callback function that was passed to the BACAAccessor method is invoked with the response. It first sets up some mappings in order to translate the BACA objects to ones that are expected by BAW and are defined in the OpenAPI interface of our micro-service. The function then invokes a helper function mapEstimateData to create the final response data and then sets it on the HTTP Response. This helper function is defined in the utils\\estimate-mappers.js file. It in turn invokes a number of other functions to parse different parts of the target structure (we will look at these next). The getSourceValue function shown below is used to map and manipulate the top level KVP data that is in the estimate (the vehicle identity and total amounts fields). The extractEstimateParts function first calls getCandidatePart in order to get the part data from the array and then it filters on those that have a valid first property representing the key of the object (the sku in our case). This is necessary because not all candidate parts are in fact parts because the repairers also include labor costs in the table which has a similar structure but it not in fact a part object at all. The getCandidatePart function is used to map the item to a potential part (which may also be the special labor cost item) and convert the more generic data to the specific types expected. Finally the extractLaborCost function reuses getCandidatePart and then parses the data to identify the special case where the candidate has no sku property but contains the other expected fields. It then calculates the total cost to return to the caller.","title":"Retrieve JSON result"},{"location":"development/capture/#delete-resources","text":"Once you have finished with an analysis in BACA, there is an API to call that will remove any resources used (such as the uploaded document the analysis was performed against). As with the other methods, the point of entry is in server.js which then delegates to the cleanup method in BACAAccessor . It then sends the HTTP Response on return from BACAAccessor . The cleanup method is shown which then invokes BACA using the r2 Node.js module and processes the response.","title":"Delete resources"},{"location":"development/capture/#packaging","text":"In order to run the micro-service in a Kubernetes environment (such as our target OCP environment), we have to provide a declarative Dockerfile that is used by Docker to build the container for deployment. This is shown below which contains various directives for copying source code and configuring permissions and specifying the initial entry script to run. And here is that entry script which simple invokes an npm command that was defined earlier in the Node.js packaging file package.json .","title":"Packaging"},{"location":"development/case-activities/","text":"Case activities Leveraging case persistence The claims solution implements a Policy case which is persisted as a record to represent the insurance policy. The Policy case type stores the policy information and claims are generated from the policy by a create claim activity, which creates the new claim case and also transfer existing policy information into the claim. Creating a Claim from Policy When the first notice of loss is received by the claim representative the first task is to search for the insured Policy, once the Policy is found the claim representative starts a discretionary activity that will create the new Claim case and also automatically transfer the insured policy data into the claim. The activity that creates the new claim case is implemented using FileNet workflow. FileNet Workflow and Process are both supported with BAW. FileNet workflow is used to implement this activity to leverage the standard CE_Operations and ICM_Operations. The equivalent of these two builtin operations are not yet available in BAW 19.0.0.2 but will be available for the process in a subsequent release. The details of the activity can be accessed from BAW Case Builder. The Builder does not provide the ability to open the details of each step of the workflow or to edit these steps. In order to edit the step is necessary to open this FileNet Workflow using \"FileNet Process Designer\" which is a tool available as java application. \"Filenet Process Design\" is a workflow design tool used to implement FiLeNet Workflows and even if the names are similar it should not be confused with \"BPM Process Designer\" . The FileNet Process Designer tool can usually be found on BAW installation directory: install_root/FileNet/ContentEngine/tools/PE/pedesigner.bat connection point Where is the the FileNet workflow system connection point configured in your environment. In the following is show the general view of the workflow and the details of the step and ICM_Operation to create a new claim case. Case data model and business objects The Case data model currently does not fully support Business Objects, therefore an interim solution has been implemented to map properties. The interface of the Case Manager data model with other BAW components may require the implementation of distinct case properties to match each element of a business object. A new upcoming version of Case Manager will fully support business objects and allow better data integration with other BAW components.","title":"Case activities"},{"location":"development/case-activities/#case-activities","text":"","title":"Case activities"},{"location":"development/case-activities/#leveraging-case-persistence","text":"The claims solution implements a Policy case which is persisted as a record to represent the insurance policy. The Policy case type stores the policy information and claims are generated from the policy by a create claim activity, which creates the new claim case and also transfer existing policy information into the claim.","title":"Leveraging case persistence"},{"location":"development/case-activities/#creating-a-claim-from-policy","text":"When the first notice of loss is received by the claim representative the first task is to search for the insured Policy, once the Policy is found the claim representative starts a discretionary activity that will create the new Claim case and also automatically transfer the insured policy data into the claim. The activity that creates the new claim case is implemented using FileNet workflow. FileNet Workflow and Process are both supported with BAW. FileNet workflow is used to implement this activity to leverage the standard CE_Operations and ICM_Operations. The equivalent of these two builtin operations are not yet available in BAW 19.0.0.2 but will be available for the process in a subsequent release. The details of the activity can be accessed from BAW Case Builder. The Builder does not provide the ability to open the details of each step of the workflow or to edit these steps. In order to edit the step is necessary to open this FileNet Workflow using \"FileNet Process Designer\" which is a tool available as java application. \"Filenet Process Design\" is a workflow design tool used to implement FiLeNet Workflows and even if the names are similar it should not be confused with \"BPM Process Designer\" . The FileNet Process Designer tool can usually be found on BAW installation directory: install_root/FileNet/ContentEngine/tools/PE/pedesigner.bat connection point Where is the the FileNet workflow system connection point configured in your environment. In the following is show the general view of the workflow and the details of the step and ICM_Operation to create a new claim case.","title":"Creating a Claim from Policy"},{"location":"development/case-activities/#case-data-model-and-business-objects","text":"The Case data model currently does not fully support Business Objects, therefore an interim solution has been implemented to map properties. The interface of the Case Manager data model with other BAW components may require the implementation of distinct case properties to match each element of a business object. A new upcoming version of Case Manager will fully support business objects and allow better data integration with other BAW components.","title":"Case data model and business objects"},{"location":"development/case-user-interface/","text":"Case user interface Case property validation Case implements out of the box property validation in relation to type and format according template. However, additional, more complex validations can be implemented with custom scripts. The solution implements a script action on the Work Details page of the activity Gather Accident Information . The script action validates two date properties: Date Reported Date-Time of Loss These properties are compared against the current date and prevent a user to enter a date that is in future. The script action Validate and Complete replaces the original Complete action on the Work Details page. In order to implement the validation, it is necessary to create a custom Complete action using a Script Action added to the Work Details page associated with that particular step in the workflow. In this case, the Activity has on one step and has the same name of the Activity \"Gather Accident Information\": You can review the implementation by opening Cases pages and select under Work Details the page \"Gather Accident Information\" Once the page is open for editing, select the Toolbar, open Settings (gear symbol on the right side), and check the new custom button in the Toolbar menu. Once the new button opens, verify that the Action \"Script Action\" is selected, the label is according to \"Validate and Complete\" and the script action code is in the Execute pane. The source code of the script action can be found in the validate-and-complete-workitem.js . Transfer case property to document When a user files a document into the case, for instance a Police Report, some case properties should be transferred and automatically updated into the document class (Case Document Type). The Claims solution implements a custom script action to perform this property transfer and update. The script action is implemented and deployed into the work details page when an activity is created to file a police report. The following properties are read from the case and transferred into the document class: Policy Number Claim Number This script action requires the document class to have Policy Number and Claim Number as properties. Review the implementation of the code by opening the Work Details page \"Gather Accident Information\" like in the previous section and select this time the \"Case Information\" widget and open the Settings (gear on the right corner). Select the custom button \"Add Document and Set Property\" and review the settings of the script action and the code in the field Execute. The script action source code can be found in the file-police-report.js /source/case.","title":"Case user interface"},{"location":"development/case-user-interface/#case-user-interface","text":"","title":"Case user interface"},{"location":"development/case-user-interface/#case-property-validation","text":"Case implements out of the box property validation in relation to type and format according template. However, additional, more complex validations can be implemented with custom scripts. The solution implements a script action on the Work Details page of the activity Gather Accident Information . The script action validates two date properties: Date Reported Date-Time of Loss These properties are compared against the current date and prevent a user to enter a date that is in future. The script action Validate and Complete replaces the original Complete action on the Work Details page. In order to implement the validation, it is necessary to create a custom Complete action using a Script Action added to the Work Details page associated with that particular step in the workflow. In this case, the Activity has on one step and has the same name of the Activity \"Gather Accident Information\": You can review the implementation by opening Cases pages and select under Work Details the page \"Gather Accident Information\" Once the page is open for editing, select the Toolbar, open Settings (gear symbol on the right side), and check the new custom button in the Toolbar menu. Once the new button opens, verify that the Action \"Script Action\" is selected, the label is according to \"Validate and Complete\" and the script action code is in the Execute pane. The source code of the script action can be found in the validate-and-complete-workitem.js .","title":"Case property validation"},{"location":"development/case-user-interface/#transfer-case-property-to-document","text":"When a user files a document into the case, for instance a Police Report, some case properties should be transferred and automatically updated into the document class (Case Document Type). The Claims solution implements a custom script action to perform this property transfer and update. The script action is implemented and deployed into the work details page when an activity is created to file a police report. The following properties are read from the case and transferred into the document class: Policy Number Claim Number This script action requires the document class to have Policy Number and Claim Number as properties. Review the implementation of the code by opening the Work Details page \"Gather Accident Information\" like in the previous section and select this time the \"Case Information\" widget and open the Settings (gear on the right corner). Select the custom button \"Add Document and Set Property\" and review the settings of the script action and the code in the field Execute. The script action source code can be found in the file-police-report.js /source/case.","title":"Transfer case property to document"},{"location":"development/decisions/","text":"Decisions development Business Object Model The BOM is composed of two BOM entries, shown on the figure below The first one, claim-model , is built bottom-up from the Java object model. The second, util-model , holds purely virtual BOM classes that provides utilities to the decision service rules. The domains for the different attributes of the object model are defined using Excel-based dynamic domains. The domains.xls spreadsheet under the resources folder of the BOM defines the different domains. The domains are then associated with virtual classes defined under the types folder of claim-model . Finally, for each attribute which value is backed a domain, we create a corresponding virtual attribute which type is the domain class. We remove any verbalization from the attribute that maps to the Java model, and instead verbalize the virtual attribute. We add the suffix Virtual to the original attribute name as the naming convention for the virtual attribute. For example, in the figure below, we create the descriptionVirtual attribute corresponding to the description attributes that is derived from the Java model. Sample rule artifacts We show here a few rules from the segment-claim decision operation for illustration purpose. The rule flow for this operation is a simple sequence of 3 tasks. The first one computes a complexity score for different aspects of the accident: damages , injuries and others . The second one is aggregating the different component scores and normalizes them on a single scale. The third task derives a categorical complexity (low, medium, high) from the normalized score. Below are 2 examples of rules, respectively contributing to the damages and injuries component: single vehicle damage rule definition fatal injury rule definition Deployment configuration The claim-processing deployment configuration is defined to encapsulate the 3 decision operations segment-claim , assess-fraud and review-escalation . Running this deployment configuration generates and deploys the claim_processing RuleApp, which encapsulates 3 rulesets, exposed through the following decision end-points: http://{host}:{port}/DecisionService/rest/v1/claim_processing/1.0/segment_claim/1.0 http://{host}:{port}/DecisionService/rest/v1/claim_processing/1.0/assess_fraud/1.0 http://{host}:{port}/DecisionService/rest/v1/claim_processing/1.0/review_escalation/1.0 Connecting ruleset to BAI To enable an ODM ruleset to send events to the BAI component, the bai.emitter.enabled ruleset property must be set to true . The additional boolean ruleset properties below allow to control the scope of the events that are emitted: - bai.emitter.input - bai.emitter.output - bai.emitter.trace We have set-up the following properties in the deployment configuration. Once deployed, the ruleset property values can be adjusted in the Rule Execution Server console. Ruleset testing Once the claim processing decision service projects are deployed to Decision Center, you can exercise test scenarios on the different decision operations. Sample test scenarios, in the form of business users friendly Excel spreadsheets, are provided here . To run a test suite, open the claim-processing decision service, and on the Tests tab, click the New Test Suite (+ sig) button, the select the decision operation you want to test, for example segment-claim : In the file to use of the Scenarios section of the page, select the Scenario File - Claim Segmentation.xlsx file, then click the Save and Run button: One the test suite run is complete, you will be switched to the Reports tab. You can then click on the newly generated test execution report to inspect the individual test scenarios execution:","title":"Decisions"},{"location":"development/decisions/#decisions-development","text":"","title":"Decisions development"},{"location":"development/decisions/#business-object-model","text":"The BOM is composed of two BOM entries, shown on the figure below The first one, claim-model , is built bottom-up from the Java object model. The second, util-model , holds purely virtual BOM classes that provides utilities to the decision service rules. The domains for the different attributes of the object model are defined using Excel-based dynamic domains. The domains.xls spreadsheet under the resources folder of the BOM defines the different domains. The domains are then associated with virtual classes defined under the types folder of claim-model . Finally, for each attribute which value is backed a domain, we create a corresponding virtual attribute which type is the domain class. We remove any verbalization from the attribute that maps to the Java model, and instead verbalize the virtual attribute. We add the suffix Virtual to the original attribute name as the naming convention for the virtual attribute. For example, in the figure below, we create the descriptionVirtual attribute corresponding to the description attributes that is derived from the Java model.","title":"Business Object Model"},{"location":"development/decisions/#sample-rule-artifacts","text":"We show here a few rules from the segment-claim decision operation for illustration purpose. The rule flow for this operation is a simple sequence of 3 tasks. The first one computes a complexity score for different aspects of the accident: damages , injuries and others . The second one is aggregating the different component scores and normalizes them on a single scale. The third task derives a categorical complexity (low, medium, high) from the normalized score. Below are 2 examples of rules, respectively contributing to the damages and injuries component: single vehicle damage rule definition fatal injury rule definition","title":"Sample rule artifacts"},{"location":"development/decisions/#deployment-configuration","text":"The claim-processing deployment configuration is defined to encapsulate the 3 decision operations segment-claim , assess-fraud and review-escalation . Running this deployment configuration generates and deploys the claim_processing RuleApp, which encapsulates 3 rulesets, exposed through the following decision end-points: http://{host}:{port}/DecisionService/rest/v1/claim_processing/1.0/segment_claim/1.0 http://{host}:{port}/DecisionService/rest/v1/claim_processing/1.0/assess_fraud/1.0 http://{host}:{port}/DecisionService/rest/v1/claim_processing/1.0/review_escalation/1.0","title":"Deployment configuration"},{"location":"development/decisions/#connecting-ruleset-to-bai","text":"To enable an ODM ruleset to send events to the BAI component, the bai.emitter.enabled ruleset property must be set to true . The additional boolean ruleset properties below allow to control the scope of the events that are emitted: - bai.emitter.input - bai.emitter.output - bai.emitter.trace We have set-up the following properties in the deployment configuration. Once deployed, the ruleset property values can be adjusted in the Rule Execution Server console.","title":"Connecting ruleset to BAI"},{"location":"development/decisions/#ruleset-testing","text":"Once the claim processing decision service projects are deployed to Decision Center, you can exercise test scenarios on the different decision operations. Sample test scenarios, in the form of business users friendly Excel spreadsheets, are provided here . To run a test suite, open the claim-processing decision service, and on the Tests tab, click the New Test Suite (+ sig) button, the select the decision operation you want to test, for example segment-claim : In the file to use of the Scenarios section of the page, select the Scenario File - Claim Segmentation.xlsx file, then click the Save and Run button: One the test suite run is complete, you will be switched to the Reports tab. You can then click on the newly generated test execution report to inspect the individual test scenarios execution:","title":"Ruleset testing"},{"location":"development/insights/","text":"Insights development Introduction This section provides more in depth details to the design information in the insights design section. In order to integrate from BAW to BAI we have augmented different BAW solutions. The main scenario (see the main scenario walkthrough ) is configured but as this involved a lot of human interaction it takes some time to progress through instances. In order to generate load and demonstrate realistic levels of claim cases we also have an emulated process and this is used as the basis for the BAI Dashboards as demonstrated in the BAI scenario walkthrough . BAW tracking (general) The data to be tracked in the workflows and sent to BAI as Dynamic Event Framework (DEF) events is defined in tracking groups in the BAW Toolkit named Denim Compute Auto Claims - Common . Here you see they are found in the Performance section within Process Designer. AutoClaimTG is used to track static data that is available at the start of the workflow while AutoClaimUpdatesTG tracks more dynamic data that is updated in several places as the workflow progresses through to final settlement of the claim. Here we see the details within one of the tracking groups and highlighted is one of the tracked fields named lossDateReported of type Date/Time . BAW tracking (main scenario) For further information on the workflow itself please see Development - Workflow . In the main scenario workflow the tracking points are set in Initiate Claims Processing as highlighted in the image below. The linked process named Loss Assessment also has some additional tracking points defined as shown below. The post tracking point on the activity Claim Initial Preparation is highlighted below. Here you can see that it is tracking group named AutoClaimTG that is used and has data from the variables of the process mapped to the tracked fields. In the rest of the process the more dynamic data is mapped in various places to AutoClaimUpdatesTG . In the highlighted example an explicit tracking intermediate event is used because you cannot map to two different tracking groups at the same point, so because the data is available after Claim Initial Preparation we need this additional tracking point to capture the dynamic updates in AutoClaimUpdatesTG . BAW tracking (emulated scenario) For further information on the workflow itself please see the Workflow for emulated BAI section. In the emulated scenario workflow the tracking points are set in Emulate Auto Claim Processing as highlighted in the image below. The pre-tracking point on the activity Claim Initial Preparation is highlighted below. Here you can see that it is tracking group named AutoClaimTG that is used and has data from the variables of the process mapped to the tracked fields. The post-tracking point on the activity Claim Initial Preparation is highlighted below. Here you can see that it is tracking group named AutoClaimUpdatesTG that is used and has data from the variables of the process mapped to the tracked fields. This differs slightly from the main scenario it emulates due to the fact that there the initial data for tracking is available at the start of Claim Initial Preparation (because the workflow configures the emulated data prior to this activity). BAI (Kibana) search index patterns In the Kibana dashboards some of the visualizations reference scripted fields defined on an index pattern. These are used where there is a requirement to derive or calculate data fields from the source tracked fields passed as events. The scripted fields section is shown here on process-s* which is the chosen index pattern that has been used as the basis for searches, visualizations, and dashboards. Below you can see that three scripted fields have been defined named ClaimDelta , Settlement Duration , and FormattedLossDate . ClaimDelta is used to calculate the difference between estimate and settlement amounts in the claim which is then used in the visualization named Denim Compute - Delta Amounts . SettlementDuration is used to calculate the time difference between the date the loss was reported and the date the claim was settled. FormattedLossDate is just a way to display the loss date in a more condensed format in order to display it more succinctly on the visualization named Denim Compute - Settlement Duration by Loss Status . BAI (Kibana) searches The data events sent from BAW to BAI are organized into active and completed summaries. BAI then has a number of index patterns defined that reference the ElasticSearch indices for these summaries. For the scenario end-goal of showing claim summaries on various charts (visualizations) we want just the top level process summary information. Here in the Search definition named Denim Compute - Auto Claims All Processes we are using the process-s* index pattern and have configured various filters so that we only see process level data for our process application. Note there are a number of filters configured and set as disabled. What this allows is for the user to turn them on / off in order to search for specific results in here while leaving the default saved settings that are relied upon by the visualizations that aggregate the data for display in the dashboards. The Search is configured with viewable columns based on the Selected fields section where these fields have been added from the Available fields section below. BAI (Kibana) visualizations All of the visualizations defined start with the keywords \"Denim Compute\" and can be found by providing a search entry as shown here. To take an example of a Pie type here you see Denim Compute - Claims by Fraud Status and it references the Search we discussed in the previous section. In this example it uses a Count aggregation and then divides the Pie into Buckets based on the unique Terms found in the events for the claimFraudStatus tracked field . In the Options section here we could have elected to Show Labels however we found that they did not resize properly when subsequently laying out in the available space of the target dashboard which seems to be a Kibana issue. An example of a Metric type is shown here in Denim Compute - Delta Amounts . The Metrics section shows that the source field is the ClaimData Scripted field discussed earlier. In Denim Compute - Settlements by Driver Age we have an example of a Bar type that shows various aggregates for comparison side by side on a bar chart. The dimension (defined in the Buckets section) that they are organized by is the driver's age ( driverAge tracked field ). Additionally a Histogram is used as the bucket type which then automatically groups the age ranges into groups of 10. In this Bar we want to show aggregates that have different scales (for example the Claims numbers will be much lower that the settlement amounts), therefore to do that we assign the aggregates to different axes in the Metrics Axes section. In Denim Compute - Max Settlement by Loss State we have an example of a Region Map type that shows data according to the US state it relates to (specifically the lossState tracked field ). In the Options section the settings are configured specific to rendering US states on the map (which matches our scenario data that only uses a subset of US states). In Denim Compute - Avg Estimates by Policy Cover we have an example of a Gauge type that organizes estimated amounts according to the type of Policy Cover. Gauges allow for setting ranges that allow you to track things like a RAG status as illustrated here. This is done by configuring Ranges as shown. In Denim Compute - Avg Settlement by Vehicle Make we have an example of a Data Table type that organizes settlement amounts according to the policy holder's vehicle make and within that the vehicle make of the other vehicle involved. In the Buckets section the Split Table setting arranges columns of the insuredVehicleMake tracked field and Split Rows further sub-divides each row against the otherVehicleMake tracked field . When using this Data Table we spotted a deficiency in the options the Kibana Visualization provides. Ideally we want to display the average figures for the column as well as for the intersections of insured vehicle / other vehicle. However the only option is to Show total which in facts adds all the averages up and is not what we want. This is not displayed on the provided saved visualization named Denim Compute - Avg Settlement by Vehicle Make , we are only showing it here in order to illustrate the issue). There are various other Visualizations for you to explore, we have covered the essential ones here that illustrate some of the data and configuration patterns used. BAI (Kibana) dashboards The Kibana Dashboards provided for the scenario are shown below. Looking in more detail at one of them ( Denim Compute - Completed Auto Claims ), here we have filters defined on the dashboard itself. This allows for reusing Visualizations and their underlying Searches across different Dashboards by setting filters here that are specific to the intent of the dashboard (in this case to only focus on completed Auto Claim instances). Navigating between the provided Dashboards is achieved by breadcrumbs implemented in Markdown in a special type of Markdown Visualization . Also highlighted are two of the earlier covered Visualizations laid out as tiles on the dashboard. Below you see the different filters defined for the other Dashboards in the scenario. There is also a disabled filter in the case of Denim Compute - Suspected Fraudulent Auto Claims which means all potential frauds are displayed by default. The end user can then enable and adjust the provided filter to drill down into the sub-categories of suspected fraud (those confirmed to be real frauds versus those cleared by the fraud investigation).","title":"Insights"},{"location":"development/insights/#insights-development","text":"","title":"Insights development"},{"location":"development/insights/#introduction","text":"This section provides more in depth details to the design information in the insights design section. In order to integrate from BAW to BAI we have augmented different BAW solutions. The main scenario (see the main scenario walkthrough ) is configured but as this involved a lot of human interaction it takes some time to progress through instances. In order to generate load and demonstrate realistic levels of claim cases we also have an emulated process and this is used as the basis for the BAI Dashboards as demonstrated in the BAI scenario walkthrough .","title":"Introduction"},{"location":"development/insights/#baw-tracking-general","text":"The data to be tracked in the workflows and sent to BAI as Dynamic Event Framework (DEF) events is defined in tracking groups in the BAW Toolkit named Denim Compute Auto Claims - Common . Here you see they are found in the Performance section within Process Designer. AutoClaimTG is used to track static data that is available at the start of the workflow while AutoClaimUpdatesTG tracks more dynamic data that is updated in several places as the workflow progresses through to final settlement of the claim. Here we see the details within one of the tracking groups and highlighted is one of the tracked fields named lossDateReported of type Date/Time .","title":"BAW tracking (general)"},{"location":"development/insights/#baw-tracking-main-scenario","text":"For further information on the workflow itself please see Development - Workflow . In the main scenario workflow the tracking points are set in Initiate Claims Processing as highlighted in the image below. The linked process named Loss Assessment also has some additional tracking points defined as shown below. The post tracking point on the activity Claim Initial Preparation is highlighted below. Here you can see that it is tracking group named AutoClaimTG that is used and has data from the variables of the process mapped to the tracked fields. In the rest of the process the more dynamic data is mapped in various places to AutoClaimUpdatesTG . In the highlighted example an explicit tracking intermediate event is used because you cannot map to two different tracking groups at the same point, so because the data is available after Claim Initial Preparation we need this additional tracking point to capture the dynamic updates in AutoClaimUpdatesTG .","title":"BAW tracking (main scenario)"},{"location":"development/insights/#baw-tracking-emulated-scenario","text":"For further information on the workflow itself please see the Workflow for emulated BAI section. In the emulated scenario workflow the tracking points are set in Emulate Auto Claim Processing as highlighted in the image below. The pre-tracking point on the activity Claim Initial Preparation is highlighted below. Here you can see that it is tracking group named AutoClaimTG that is used and has data from the variables of the process mapped to the tracked fields. The post-tracking point on the activity Claim Initial Preparation is highlighted below. Here you can see that it is tracking group named AutoClaimUpdatesTG that is used and has data from the variables of the process mapped to the tracked fields. This differs slightly from the main scenario it emulates due to the fact that there the initial data for tracking is available at the start of Claim Initial Preparation (because the workflow configures the emulated data prior to this activity).","title":"BAW tracking (emulated scenario)"},{"location":"development/insights/#bai-kibana-search-index-patterns","text":"In the Kibana dashboards some of the visualizations reference scripted fields defined on an index pattern. These are used where there is a requirement to derive or calculate data fields from the source tracked fields passed as events. The scripted fields section is shown here on process-s* which is the chosen index pattern that has been used as the basis for searches, visualizations, and dashboards. Below you can see that three scripted fields have been defined named ClaimDelta , Settlement Duration , and FormattedLossDate . ClaimDelta is used to calculate the difference between estimate and settlement amounts in the claim which is then used in the visualization named Denim Compute - Delta Amounts . SettlementDuration is used to calculate the time difference between the date the loss was reported and the date the claim was settled. FormattedLossDate is just a way to display the loss date in a more condensed format in order to display it more succinctly on the visualization named Denim Compute - Settlement Duration by Loss Status .","title":"BAI (Kibana) search index patterns"},{"location":"development/insights/#bai-kibana-searches","text":"The data events sent from BAW to BAI are organized into active and completed summaries. BAI then has a number of index patterns defined that reference the ElasticSearch indices for these summaries. For the scenario end-goal of showing claim summaries on various charts (visualizations) we want just the top level process summary information. Here in the Search definition named Denim Compute - Auto Claims All Processes we are using the process-s* index pattern and have configured various filters so that we only see process level data for our process application. Note there are a number of filters configured and set as disabled. What this allows is for the user to turn them on / off in order to search for specific results in here while leaving the default saved settings that are relied upon by the visualizations that aggregate the data for display in the dashboards. The Search is configured with viewable columns based on the Selected fields section where these fields have been added from the Available fields section below.","title":"BAI (Kibana) searches"},{"location":"development/insights/#bai-kibana-visualizations","text":"All of the visualizations defined start with the keywords \"Denim Compute\" and can be found by providing a search entry as shown here. To take an example of a Pie type here you see Denim Compute - Claims by Fraud Status and it references the Search we discussed in the previous section. In this example it uses a Count aggregation and then divides the Pie into Buckets based on the unique Terms found in the events for the claimFraudStatus tracked field . In the Options section here we could have elected to Show Labels however we found that they did not resize properly when subsequently laying out in the available space of the target dashboard which seems to be a Kibana issue. An example of a Metric type is shown here in Denim Compute - Delta Amounts . The Metrics section shows that the source field is the ClaimData Scripted field discussed earlier. In Denim Compute - Settlements by Driver Age we have an example of a Bar type that shows various aggregates for comparison side by side on a bar chart. The dimension (defined in the Buckets section) that they are organized by is the driver's age ( driverAge tracked field ). Additionally a Histogram is used as the bucket type which then automatically groups the age ranges into groups of 10. In this Bar we want to show aggregates that have different scales (for example the Claims numbers will be much lower that the settlement amounts), therefore to do that we assign the aggregates to different axes in the Metrics Axes section. In Denim Compute - Max Settlement by Loss State we have an example of a Region Map type that shows data according to the US state it relates to (specifically the lossState tracked field ). In the Options section the settings are configured specific to rendering US states on the map (which matches our scenario data that only uses a subset of US states). In Denim Compute - Avg Estimates by Policy Cover we have an example of a Gauge type that organizes estimated amounts according to the type of Policy Cover. Gauges allow for setting ranges that allow you to track things like a RAG status as illustrated here. This is done by configuring Ranges as shown. In Denim Compute - Avg Settlement by Vehicle Make we have an example of a Data Table type that organizes settlement amounts according to the policy holder's vehicle make and within that the vehicle make of the other vehicle involved. In the Buckets section the Split Table setting arranges columns of the insuredVehicleMake tracked field and Split Rows further sub-divides each row against the otherVehicleMake tracked field . When using this Data Table we spotted a deficiency in the options the Kibana Visualization provides. Ideally we want to display the average figures for the column as well as for the intersections of insured vehicle / other vehicle. However the only option is to Show total which in facts adds all the averages up and is not what we want. This is not displayed on the provided saved visualization named Denim Compute - Avg Settlement by Vehicle Make , we are only showing it here in order to illustrate the issue). There are various other Visualizations for you to explore, we have covered the essential ones here that illustrate some of the data and configuration patterns used.","title":"BAI (Kibana) visualizations"},{"location":"development/insights/#bai-kibana-dashboards","text":"The Kibana Dashboards provided for the scenario are shown below. Looking in more detail at one of them ( Denim Compute - Completed Auto Claims ), here we have filters defined on the dashboard itself. This allows for reusing Visualizations and their underlying Searches across different Dashboards by setting filters here that are specific to the intent of the dashboard (in this case to only focus on completed Auto Claim instances). Navigating between the provided Dashboards is achieved by breadcrumbs implemented in Markdown in a special type of Markdown Visualization . Also highlighted are two of the earlier covered Visualizations laid out as tiles on the dashboard. Below you see the different filters defined for the other Dashboards in the scenario. There is also a disabled filter in the case of Denim Compute - Suspected Fraudulent Auto Claims which means all potential frauds are displayed by default. The end user can then enable and adjust the provided filter to drill down into the sub-categories of suspected fraud (those confirmed to be real frauds versus those cleared by the fraud investigation).","title":"BAI (Kibana) dashboards"},{"location":"development/process-flow-control/","text":"Process flow control Process hierarchy The top level process that is called from the Gather Accident Information Case activity when the FNOL (First Notice of Loss) stage is complete is Initiate Claims Processing , shown below. The process diagram is color-coded to highlight specific patterns of processing as follows: Orange for integration between Case and Process Green for integration between Process and ODM (Operational Decision Manager) Purple for integration between process and ECM (Enterprise Content Management) The Initiate Claims Processing process includes a call to the linked process Loss Assessment . In the below image, we focus on the MVP path through that process (which matches the scenario in the Scenario walkthrough section). There are additional steps to handle the situation where the ODM service Review Escalation Conditions results in a path that needs review and potential rework which you can expect by opening up the process in the Process Designer tool. Loss Assessment in turn calls the linked process Estimate Damage which involves two user tasks (that are implemented in BAW with client-side human services that contain one or more coaches for user interaction) Tender for Estimates and Review and Select Estimate . In between these two user tasks is a message exchange (for more details on the message exchange pattern see the Process message exchange section in the Process services section) which invokes a further process and receives the result. The Provide Repair Estimates process is a kind of batch processing paradigm as it needs to manage the soliciting and obtaining of multiple repair estimates which it does by invoking a further linked process, Provide Repair Estimates Per Vehicle , using a multi-instance loop construct. The Provide Repair Estimates Per Vehicle process shown below handles the processing for each vehicle in the claim. It then delegates to a further multi-instance loop to call the linked process Provide Repair Estimates Per Repairer . The Provide Repair Estimates Per Repairer process then handles the collecting of a repair estimate from an individual repairer for a single vehicle. Process synchronization with Case As discussed in the Workflow Design section, the workflow solution involves a collaboration between Case ad-hoc activities and process sequential activities. That collaboration is achieved in a number of ways, first the main process is implemented as a Case activity as highlighted below. Within Initiate Claims Processing , there is a need to synchronize the case properties by reading them from the parent Case and this is done in the service flow implementation Map Case Correlation Properties , which is part of the referenced Toolkit Denim Compute Auto Claims Toolkit . For more details on the synchronization framework, please see the Case properties synchronization section within the Process services section. When the process needs to pass control back to Case and invoke another case activity. It does that by updating one or more Case properties that are used as the pre-condition of that target activity. The logic to do this is illustrated by the activity with service flow implementation Request Claim Adjuster Report shown below, which again uses the synchronization framework. Here we see the target Case activity called Create Adjuster Report (which is implemented as a P8 process) and the highlighted start pre-condition which references a Case property that has been updated to trigger this activity. The above screen shot also shows the follow-on Case activity called Signal Adjuster Report Created which is triggered from a property update that is done within the previous Create Adjuster Report activity. That activity is implemented as a further process that then uses a Message Send Event (highlighted) to communicate to the main Initiate Claims Processing process. For further details on the mechanics of message event exchanges, see the Process message exchange section of Process services . To complete the message exchange pattern, here we see the awaiting Intermediate Message Event of type receiving inside Initiate Claims Processing which then \"awakens\" that quiesced process based on correlating information supplied to it from Signal Adjuster Report Created .","title":"Process flow control"},{"location":"development/process-flow-control/#process-flow-control","text":"","title":"Process flow control"},{"location":"development/process-flow-control/#process-hierarchy","text":"The top level process that is called from the Gather Accident Information Case activity when the FNOL (First Notice of Loss) stage is complete is Initiate Claims Processing , shown below. The process diagram is color-coded to highlight specific patterns of processing as follows: Orange for integration between Case and Process Green for integration between Process and ODM (Operational Decision Manager) Purple for integration between process and ECM (Enterprise Content Management) The Initiate Claims Processing process includes a call to the linked process Loss Assessment . In the below image, we focus on the MVP path through that process (which matches the scenario in the Scenario walkthrough section). There are additional steps to handle the situation where the ODM service Review Escalation Conditions results in a path that needs review and potential rework which you can expect by opening up the process in the Process Designer tool. Loss Assessment in turn calls the linked process Estimate Damage which involves two user tasks (that are implemented in BAW with client-side human services that contain one or more coaches for user interaction) Tender for Estimates and Review and Select Estimate . In between these two user tasks is a message exchange (for more details on the message exchange pattern see the Process message exchange section in the Process services section) which invokes a further process and receives the result. The Provide Repair Estimates process is a kind of batch processing paradigm as it needs to manage the soliciting and obtaining of multiple repair estimates which it does by invoking a further linked process, Provide Repair Estimates Per Vehicle , using a multi-instance loop construct. The Provide Repair Estimates Per Vehicle process shown below handles the processing for each vehicle in the claim. It then delegates to a further multi-instance loop to call the linked process Provide Repair Estimates Per Repairer . The Provide Repair Estimates Per Repairer process then handles the collecting of a repair estimate from an individual repairer for a single vehicle.","title":"Process hierarchy"},{"location":"development/process-flow-control/#process-synchronization-with-case","text":"As discussed in the Workflow Design section, the workflow solution involves a collaboration between Case ad-hoc activities and process sequential activities. That collaboration is achieved in a number of ways, first the main process is implemented as a Case activity as highlighted below. Within Initiate Claims Processing , there is a need to synchronize the case properties by reading them from the parent Case and this is done in the service flow implementation Map Case Correlation Properties , which is part of the referenced Toolkit Denim Compute Auto Claims Toolkit . For more details on the synchronization framework, please see the Case properties synchronization section within the Process services section. When the process needs to pass control back to Case and invoke another case activity. It does that by updating one or more Case properties that are used as the pre-condition of that target activity. The logic to do this is illustrated by the activity with service flow implementation Request Claim Adjuster Report shown below, which again uses the synchronization framework. Here we see the target Case activity called Create Adjuster Report (which is implemented as a P8 process) and the highlighted start pre-condition which references a Case property that has been updated to trigger this activity. The above screen shot also shows the follow-on Case activity called Signal Adjuster Report Created which is triggered from a property update that is done within the previous Create Adjuster Report activity. That activity is implemented as a further process that then uses a Message Send Event (highlighted) to communicate to the main Initiate Claims Processing process. For further details on the mechanics of message event exchanges, see the Process message exchange section of Process services . To complete the message exchange pattern, here we see the awaiting Intermediate Message Event of type receiving inside Initiate Claims Processing which then \"awakens\" that quiesced process based on correlating information supplied to it from Signal Adjuster Report Created .","title":"Process synchronization with Case"},{"location":"development/process-services/","text":"Process services This section provides details on service flow implementation chains of process activities. To understand the processes themselves that this section builds upon, please see the Process flow control section. Process message exchange Processes communicate asynchronously via message events whereby one process will send an outbound message and the other receives it. In the figure below we see an example of this pattern whereby the sending process ( Signal Adjuster Report Created ) on the left side and the receiving process ( Initiate Claims Processing ) on the right side reference the same message Under Cover Agent (UCA). In the sending process, the outbound message event has Data Mapping setup with a mapping to the correlating data plus the event payload. The correlating data is important because there is a need to identify the specific Initiate Claims Processing process instance that is awaiting the response from Signale Adjuster Report Created . Here on the other side of the exchange you can see that the event data's claimCorrelationId is designated as the correlation variable and is used then to compare to the claimCaseId values of instances in order to match and resume processing in the correct instance. Case properties synchronization In order for Case properties to be referenced inside processes they need to be retrieved. Inside the process, an AutoClaim Business Object is modeled which represents those data items of interest to the process, a large majority of which are also Case properties. A framework of services has been developed to allow for obtaining Case properties and setting them on the various levels within the complex Business Object (BO) structure (and updating them as we will cover later in this section). The principle is that each BO within a higher level complex structure has a mapping service such as illustrated in the below Map Vehicle Input (which is inside the Denim Compute Auto Claim Toolkit ) for a vehicle object. That service in turn calls a generic service Map BO Input to perform the mapping. The data mapping on the service accepts as input an ANY type representing the BO to populate and then returns it populated with data from the corresponding Case properties. The other key input is a String list representing the set of property to attribute mappings. In the Set Vehicle Mappings script, the CSV list is created. This list has an entry for each target attribute in the BO and passes in for each entry a formatted string, an example of which is highlighted. To take that value DENI1_VehicleMake:make:String the first part represents the unique name of the case property, the second is the BO attribute name, and the third is the data type. Here is the corresponding case property named Vehicle Make showing where the unique identifier comes from inside Case Builder . The Map BO Input service is shown here, it makes calls to a number of helper services in order to interact with Case using the JavaScript API. Returning to the mapping, the higher level AutoClaim BO is mapped similarly in a service Map AutoClaim Input and also it delegates to lower level services (such as Map Vehicle Input shown earlier) to perform the mappings of the contained BOs within it. When data is required to be updated in Case the pattern is to set the inputs on and invoke a generic service Write Case Properties as illustrated in the service Update Claim Settlement Details (found in the Denim Compute Auto Claims Toolkit) shown here. The Map Properties script sets the casePropertyUpdateList variable used as the input mapping on the target Write Case Properties service. ODM integration BAW integration to ODM is by using a REST External Service . The service includes the three ODM operations used in the scenario, it is named Denim ODM Service and found in the Denim Compute Decision Services Toolkit Toolkit shown here: An example usage of the External Service is in Perform Claim Segmentation within Denim Compute Auto Claim Toolkit Toolkit. Note in the scenario that there is a boundary error event handler in case the ODM service is unavailable and this then sets the outputs to follow a default path. The inputs to the ODM operation are set from case properties using the pattern already described in the Case properties synchronization sub-section above. ECM integration BAW uses two main patterns to integrate to ECM, first it has to read existing documents from ECM in order to display them to the user inside client-side human service coaches and second it has to write a new JSON document to ECM with the complex data structures populated within BPM. For the first pattern a service flow named Get Claim ECM Documents in Denim Compute Auto Claims Toolkit Toolkit is called from the Loss Assessment process as shown here. In the Data Mapping inputs to that service we provide the claimCaseId and claimSubfolderPath which are used to find the Case ECM folder and the sub folder within it of interest (in this situation that is Damages Evidence ). Here is the Get Claim ECM Documents service flow where it uses the Content Integration feature to makes calls to Get Folder by Path and Get Documents in Folder . It then filters the returned documents to ensure that they match the Document Class of interest and that they have metadata for the Vehicle Plate that matches the vehicle that is bound to this particular service flow. The Loss Assessment process we saw earlier then invokes the Tender for Estimates Client Side Human Service . In the Coach layout a reusable View named Vehicle Documents (found in Denim Compute Auto Claims - Common Toolkit) is used and has the documents that were retrieved bound to it. In Vehicle Documents a Viewer control from the Content Management Toolkit is used to display the document content as shown here. Moving now to the second pattern, in the Initiate Claims Processing process one of the last activities is to store the JSON representation of the complex BO structure in the Case folder by invoking the service flow Write AutoClaim JSON Output . In Write AutoClaim JSON Output (within Denim Compute Auto Claims Toolkit) the steps include a call to a Content Integration step using the Create document operation. A previous step Convert to JSON is used to parse the AutoClaim BO and convert it into native JS equivalents and from that to extract the JSON formatted output. BACA integration As seen in the mediated BACA integration section of the workflow design , the Case Activity implementation ( Process Repair Estimate ) on receipt of an estimate PDF processes the document via BACA and then sends an event notification where it sets correlation data and the VehicleRepair BO. The event message correlates to the corresponding running instance of Provide Repair Estimates Per Repairer by matching on the correlation key data (the combination of claimNumber , repairerCode , and vehicleVIN ). This then interrupts (and in effect cancels) the user task Provide Repair Estimate as it is no longer needed when the VehicleRepair data has been validated and provided by the BACA integration. Process Repair Estimate invokes the linked process Handle Received Vehicle Repair Estimate in Denim Compute Auto Claims Toolkit to perform the main logic to fetch the uploaded document, provide it to BACA and parse the results and then create the event data for return to Process Repair Estimate . The interaction with BACA is done via a mediation micro-service implemented in Node.js and deployed on Red Hat OpenShift Container Platform (OCP). The Service Flow invoked by Handle Received Vehicle Repair Estimate that handles the top level interaction with the microservice is Analyze and Parse Auto Repair Estimate shown below. The high level sequence of the logic is: Call the microservice to upload the PDF for analysis Wait before making a call to the microservice to check the status of the analysis and check that it has been parsed OK Retrieve the resulting Vehicle Repair Estimate from the microservice The integration to the microservice (which in turn integrates to BACA) is self-contained in the Toolkit named Denim Compute Content Analysis Services . The REST integration to the microservice is via an External Service that exposes several operations (but not all as some cannot be processed in BAW as we shall see later). The Service Flow named Request Auto Repair Estimate Analysis shown below needs to use a Script node to invoke one of the operations on the interface. This is because that operation (named requestAnalysis that allows for a file upload) uses a MIME type of multipart/form-data which BAW does not yet support. The Service Flow named Get BACA Analysis Status invokes the External Service operation named getStatusByAnalyzerId as shown below. The Data Mapping section shows the various input data supplied to the operation. A number of these are REST Headers data and the other is the analyzerId which is returned as a response from the previous call to the requestAnalysis operation. The Service Flow named Get BACA Analysis Results invokes the External Service operation named retrieveJSONByAnalyzerId in order to retrieve the resulting parsed estimate data from BACA. Finally the flow also invokes the External Service operation named cleanupByAnalyzerId in order to delete the resources used by the request inside BACA.","title":"Process services"},{"location":"development/process-services/#process-services","text":"This section provides details on service flow implementation chains of process activities. To understand the processes themselves that this section builds upon, please see the Process flow control section.","title":"Process services"},{"location":"development/process-services/#process-message-exchange","text":"Processes communicate asynchronously via message events whereby one process will send an outbound message and the other receives it. In the figure below we see an example of this pattern whereby the sending process ( Signal Adjuster Report Created ) on the left side and the receiving process ( Initiate Claims Processing ) on the right side reference the same message Under Cover Agent (UCA). In the sending process, the outbound message event has Data Mapping setup with a mapping to the correlating data plus the event payload. The correlating data is important because there is a need to identify the specific Initiate Claims Processing process instance that is awaiting the response from Signale Adjuster Report Created . Here on the other side of the exchange you can see that the event data's claimCorrelationId is designated as the correlation variable and is used then to compare to the claimCaseId values of instances in order to match and resume processing in the correct instance.","title":"Process message exchange"},{"location":"development/process-services/#case-properties-synchronization","text":"In order for Case properties to be referenced inside processes they need to be retrieved. Inside the process, an AutoClaim Business Object is modeled which represents those data items of interest to the process, a large majority of which are also Case properties. A framework of services has been developed to allow for obtaining Case properties and setting them on the various levels within the complex Business Object (BO) structure (and updating them as we will cover later in this section). The principle is that each BO within a higher level complex structure has a mapping service such as illustrated in the below Map Vehicle Input (which is inside the Denim Compute Auto Claim Toolkit ) for a vehicle object. That service in turn calls a generic service Map BO Input to perform the mapping. The data mapping on the service accepts as input an ANY type representing the BO to populate and then returns it populated with data from the corresponding Case properties. The other key input is a String list representing the set of property to attribute mappings. In the Set Vehicle Mappings script, the CSV list is created. This list has an entry for each target attribute in the BO and passes in for each entry a formatted string, an example of which is highlighted. To take that value DENI1_VehicleMake:make:String the first part represents the unique name of the case property, the second is the BO attribute name, and the third is the data type. Here is the corresponding case property named Vehicle Make showing where the unique identifier comes from inside Case Builder . The Map BO Input service is shown here, it makes calls to a number of helper services in order to interact with Case using the JavaScript API. Returning to the mapping, the higher level AutoClaim BO is mapped similarly in a service Map AutoClaim Input and also it delegates to lower level services (such as Map Vehicle Input shown earlier) to perform the mappings of the contained BOs within it. When data is required to be updated in Case the pattern is to set the inputs on and invoke a generic service Write Case Properties as illustrated in the service Update Claim Settlement Details (found in the Denim Compute Auto Claims Toolkit) shown here. The Map Properties script sets the casePropertyUpdateList variable used as the input mapping on the target Write Case Properties service.","title":"Case properties synchronization"},{"location":"development/process-services/#odm-integration","text":"BAW integration to ODM is by using a REST External Service . The service includes the three ODM operations used in the scenario, it is named Denim ODM Service and found in the Denim Compute Decision Services Toolkit Toolkit shown here: An example usage of the External Service is in Perform Claim Segmentation within Denim Compute Auto Claim Toolkit Toolkit. Note in the scenario that there is a boundary error event handler in case the ODM service is unavailable and this then sets the outputs to follow a default path. The inputs to the ODM operation are set from case properties using the pattern already described in the Case properties synchronization sub-section above.","title":"ODM integration"},{"location":"development/process-services/#ecm-integration","text":"BAW uses two main patterns to integrate to ECM, first it has to read existing documents from ECM in order to display them to the user inside client-side human service coaches and second it has to write a new JSON document to ECM with the complex data structures populated within BPM. For the first pattern a service flow named Get Claim ECM Documents in Denim Compute Auto Claims Toolkit Toolkit is called from the Loss Assessment process as shown here. In the Data Mapping inputs to that service we provide the claimCaseId and claimSubfolderPath which are used to find the Case ECM folder and the sub folder within it of interest (in this situation that is Damages Evidence ). Here is the Get Claim ECM Documents service flow where it uses the Content Integration feature to makes calls to Get Folder by Path and Get Documents in Folder . It then filters the returned documents to ensure that they match the Document Class of interest and that they have metadata for the Vehicle Plate that matches the vehicle that is bound to this particular service flow. The Loss Assessment process we saw earlier then invokes the Tender for Estimates Client Side Human Service . In the Coach layout a reusable View named Vehicle Documents (found in Denim Compute Auto Claims - Common Toolkit) is used and has the documents that were retrieved bound to it. In Vehicle Documents a Viewer control from the Content Management Toolkit is used to display the document content as shown here. Moving now to the second pattern, in the Initiate Claims Processing process one of the last activities is to store the JSON representation of the complex BO structure in the Case folder by invoking the service flow Write AutoClaim JSON Output . In Write AutoClaim JSON Output (within Denim Compute Auto Claims Toolkit) the steps include a call to a Content Integration step using the Create document operation. A previous step Convert to JSON is used to parse the AutoClaim BO and convert it into native JS equivalents and from that to extract the JSON formatted output.","title":"ECM integration"},{"location":"development/process-services/#baca-integration","text":"As seen in the mediated BACA integration section of the workflow design , the Case Activity implementation ( Process Repair Estimate ) on receipt of an estimate PDF processes the document via BACA and then sends an event notification where it sets correlation data and the VehicleRepair BO. The event message correlates to the corresponding running instance of Provide Repair Estimates Per Repairer by matching on the correlation key data (the combination of claimNumber , repairerCode , and vehicleVIN ). This then interrupts (and in effect cancels) the user task Provide Repair Estimate as it is no longer needed when the VehicleRepair data has been validated and provided by the BACA integration. Process Repair Estimate invokes the linked process Handle Received Vehicle Repair Estimate in Denim Compute Auto Claims Toolkit to perform the main logic to fetch the uploaded document, provide it to BACA and parse the results and then create the event data for return to Process Repair Estimate . The interaction with BACA is done via a mediation micro-service implemented in Node.js and deployed on Red Hat OpenShift Container Platform (OCP). The Service Flow invoked by Handle Received Vehicle Repair Estimate that handles the top level interaction with the microservice is Analyze and Parse Auto Repair Estimate shown below. The high level sequence of the logic is: Call the microservice to upload the PDF for analysis Wait before making a call to the microservice to check the status of the analysis and check that it has been parsed OK Retrieve the resulting Vehicle Repair Estimate from the microservice The integration to the microservice (which in turn integrates to BACA) is self-contained in the Toolkit named Denim Compute Content Analysis Services . The REST integration to the microservice is via an External Service that exposes several operations (but not all as some cannot be processed in BAW as we shall see later). The Service Flow named Request Auto Repair Estimate Analysis shown below needs to use a Script node to invoke one of the operations on the interface. This is because that operation (named requestAnalysis that allows for a file upload) uses a MIME type of multipart/form-data which BAW does not yet support. The Service Flow named Get BACA Analysis Status invokes the External Service operation named getStatusByAnalyzerId as shown below. The Data Mapping section shows the various input data supplied to the operation. A number of these are REST Headers data and the other is the analyzerId which is returned as a response from the previous call to the requestAnalysis operation. The Service Flow named Get BACA Analysis Results invokes the External Service operation named retrieveJSONByAnalyzerId in order to retrieve the resulting parsed estimate data from BACA. Finally the flow also invokes the External Service operation named cleanupByAnalyzerId in order to delete the resources used by the request inside BACA.","title":"BACA integration "},{"location":"development/process-user-interface/","text":"Process user interface Referencing views The BPM UI API provides a number of ways to reference other controls (Views) depending on the context. The most simple example is to reference a control from itself: Next when you want to reference another control inside an event of a control you use the ${\\ controlId\\ } notation: You can use a shortcut notation of getSibling() to refer to a control at same level (i.e. inside the same view): From inside a Table column you need to use ${../\\ controlId\\ } to navigate up to reference controls at the same level as the Table (this principle also applies from inside a view to reference things that are siblings of the view on the parent Coach): (Note the above also shows an example of using internal Table shortcut references \u2013 in this case to reference a control from same row with ${txtSKU=} notation). To reference controls that are on the Coach (the page) you use page.ui.get() : To reference controls inside a view\u2019s logic use this.ui.get() : To reference view logic (functions) from a contained control inside a view use view.xxx : Using formulas The BPM UI API provides formulas where you reference other fields and when those fields change the formula is automatically recalculated. An example of that is in the Create Settlement Offer Coach where the Claim Settlement Amount is a calculated field that uses a formula to refer to other fields. Any time one of those referenced fields changes then the formula is re-evaluated: However sometimes setting a formula does not meet the requirements and you have to adopt an alternative approach. Here is an example of trying to set a formula that references the selected rows in some tables: The first issue is that it gives errors because the getSelectedRecord() at load returns null: The second issue (and the critical one in this case) is that an automatic update does not seem to get triggered when you select the row in the table. To work around this we take a different approach were we will use a function to perform the calculation and set the target field and place this function in a reusable view that we then add to the Coach: The view function is shown here: Now then the table row is selected we can call the view\u2019s function from the event: View events examples Events on a view (control) are the key to detecting a change and reacting to it (e.g. a data field is set or updated) with BPM UI API calls. A typical event is an on click of a button \u2013 here is an example of invoking a view\u2019s function and passing in as an argument a reference to another control\u2019s bound data property: Here is an example of reacting to a data change event and it also shows you can put multiple statements in the implementation (you can even add arbitrary JS logic like if() {} else {} ): Her is an example of detecting a table row selection: Table view API manipulation The BPM UI Table view API has a number of methods, snippets of the ones that were used most regularly in the processing are shown here: Editing table rows with Flyout The BPM UI Samples recommend using a view inside a table column that edits a row (and uses Deferred Sections for lazy loading). The key to this is that the Modal section is placed within a table column and bound to the list\u2019s currentItem : While this allows for editing the table row, it makes direct edits to the underlying row and does not preserve the existing value. In our scenario we want to provide an enhanced user experience so they can Edit a row and then decide whether to commit those changes or cancel them. To support this we can not use the approach of binding to the list\u2019s currentItem . Instead we create a separate local variable of type VehiclePart which represents the currently editable Part and bind that to the Modal that will include the Part editing fields: When Edit is clicked the event then calls an editPart() method defined in a reusable view and passes in the SKU for the selected row: Here is the editPart() method within Provide Repair Estimate Functions view that then sets the bound data on the mobile from the selected item in the Parts table plus it also sets a hidden field for the part index as we will need to reference this in the case of the user choosing the Save Changes option on the Modal: The Save Changes boundary event is then used to transition to a client-side script where the partsList is updated at the saved index (and if the user selects to Cancel we just close the Modal and do not make any updates to the partsList ): Putting reusable functions in a view Formulas etc on controls are great for one time actions but you often find that you want to do multiple things on an event and while you can add many statements separated by \u201c;\u201d in an event it becomes unwieldly and difficult to maintain. Also if you find yourself having to do a lot of chained behavior on different controls (e.g. a table updates so you have to check button visibility, plus popup a model, plus re-initialize a variable, etc) then it is hard to keep track. (Another example is for a Table with a requirement to switch Button visibility depending on row count \u2013 the Table\u2019s on rows loaded event worked for most cases but it did not fire when you programmatically called table.add() to add a new row). So we considered it better to have a function that does the multiple things and call that function. However if you use Custom HTML on a Coach then you can\u2019t reference that function in client-side scripts so you would end up with duplicating the logic (where you had to transition out of Coach and call the same behavioural logic). To get round this we created a view that has no content (well we put a hidden 100px Spacer on it so we could see it visually on the parent Coach canvas): And an example of referencing the functions: The downside is that the view with functions has to refer to control Id names that are volatile (if they change on Coach then the code inside the view functions will break): However you would be in a similar situation if you used Custom HTML as again you would have to edit the code for every change to a Control ID . Transitioning to client-side scripts While the ability to encapsulate all UI logic in a view is desirable to allow for maximum reuse, sometimes it is not possible to self-contain everything on the Coach or in a view \u2013 you have to transition out to a client-side script to complete the necessary logic. The most common situation is where you need to refer to a variable that is not bound to anything on the coach so it does not have any DOM representation that you can make BPM UI API calls (e.g. page.ui.get(\\ controlId\\ ) ) against. Here is an example from the Tender For Estimates Coach where the vehicleList.repairerList does not have any binding to a visual control and so it cannot be referenced in BPM UI API calls: Here is another example where there is a reference to duplicatePartWarning which has no binding on the Coach: (Note: duplicatePartWarning is a String bound to the Help section of a Modal Alert view from UI Toolkit that is added on the Coach and there is no exposed API method on it that lets you set the Help value thus the need to manipulate the duplicatePartWarning variable): Note in both of the above cases it may be possible to avoid these transitions if really necessary by putting hidden controls on the Coach that are bound to the variables that need to be manipulated. That way the BPM UI API can be used to access and update the bindings on those hidden controls. An other example is where the event is not exposed \u2013 such as in a Modal with buttons encapsulated within it so you cannot get access to the button events: You have to then react to the boundary event instead and transition to a client-side script : You may also be able to avoid this situation in a number of (increasing complexity in terms of coding) ways. The easiest solution might be to not encapsulate the buttons inside the Modal but have them directly on the Coach / view that needs to react to their events. An alternative, and more complex, option would be to expose the inner button events to the interface of the containing view. This requires using BPM UI Coach Framework APIs to detect the button events programmatically and to fire an event option on the parent view. Coach views loading order The loading order of the tree of views contained in a coach is inside out \u2013 a section with controls loads after all the contained controls inside it have loaded and so on. We utilise a top level Vertical Layout container in each Coach and do initialization things that reference multiple controls in the onLoad() event of it \u2013 that way we can ensure all dependent controls are already loaded and can be referenced with page.ui.get(\\ controlId\\ ) / ${\\ controlId\\ } etc. Here is an example with the outermost Vertical Layout shown and in there it calls an embedded view with the JS functions to perform checking whether to enable the Complete button or not: Deferred section views and asynchronous loading A common pattern when using a Modal popup is to defer the loading of it until needed (especially if it is complex and put within the column of a table) using the Deferred Section (DS) view. You have to be very careful about the loading order \u2013 you will get errors showing up in the browser debugger such as \u201c page.ui.get(\\ controlId>) not defined \u201d if you are trying to reference DOM items that are not loaded yet. A way to avoid such issues is to encapsulate the actions in a function and ensure it is called only when the DS has loaded. Here is a view with a DS that inside has a Modal and a Table and other controls: An initializing function ( showModal() below) is called from parent Coach (say from a button click event) that checks whether the DS is loaded and if not it calls lazyLoad() on it otherwise it calls the function that prepares the view by setting visibility etc: This same prepareView() function should be called only when the DS has lazy-loaded: Forcing boundary event triggers Sometimes you want to react to an event on a control by executing some client-side script logic but the control does not have its own boundary event that fits the need. So you add a Navigation Event control: And in the source for the event (in this example the return from a Service Call ) you call fire() on it: Which then lets you transition out to the client-side script : Map control reacting to binding changes We use a BPM UI Map view in the Tender for Estimates Coach to show the location of a Vehicle . The BPM UI Map view has configuration options for the latitude / longitude settings: While these work on first load of the Map inside the popup Modal on the Coach, subsequent changing of these values (for example when the selected Vehicle is changed) did not trigger an update of the location shown on the Map. To get around this we need to use a specific Map API method. First let\u2019s look at the sequence of events, When a Vehicle is selected the ZIP is passed to a Service Call view ( Get Coordinates ) which invokes a service to translate the ZIP into Geo coordinates. On return from that Service Call we then need to force a transition and that is done by in turn calling the fire() method of a Navigation Event view: The Navigation Event then provides us a boundary event which we can use to transition to the Show Map client-side script : And in the script (among other things) we have to explicitly call the setCenter() method on the Map to ensure it refreshes: Note that while this results in the Map correctly showing the new coordinates, however the Marker is not shown. Access view configuration option complex BO There is a defined way to navigate through the structure of a complex BO ( Business Object ) bound as a configuration option of a view. In this case you get the top level BO ( searchableVehicleParts ) and then a property that is a list ( partsList ): Then a further call uses the index to get a specific entry from the list and then reference attributes of that entry (an instance of VehiclePart BO):","title":"Process user interface"},{"location":"development/process-user-interface/#process-user-interface","text":"","title":"Process user interface"},{"location":"development/process-user-interface/#referencing-views","text":"The BPM UI API provides a number of ways to reference other controls (Views) depending on the context. The most simple example is to reference a control from itself: Next when you want to reference another control inside an event of a control you use the ${\\ controlId\\ } notation: You can use a shortcut notation of getSibling() to refer to a control at same level (i.e. inside the same view): From inside a Table column you need to use ${../\\ controlId\\ } to navigate up to reference controls at the same level as the Table (this principle also applies from inside a view to reference things that are siblings of the view on the parent Coach): (Note the above also shows an example of using internal Table shortcut references \u2013 in this case to reference a control from same row with ${txtSKU=} notation). To reference controls that are on the Coach (the page) you use page.ui.get() : To reference controls inside a view\u2019s logic use this.ui.get() : To reference view logic (functions) from a contained control inside a view use view.xxx :","title":"Referencing views"},{"location":"development/process-user-interface/#using-formulas","text":"The BPM UI API provides formulas where you reference other fields and when those fields change the formula is automatically recalculated. An example of that is in the Create Settlement Offer Coach where the Claim Settlement Amount is a calculated field that uses a formula to refer to other fields. Any time one of those referenced fields changes then the formula is re-evaluated: However sometimes setting a formula does not meet the requirements and you have to adopt an alternative approach. Here is an example of trying to set a formula that references the selected rows in some tables: The first issue is that it gives errors because the getSelectedRecord() at load returns null: The second issue (and the critical one in this case) is that an automatic update does not seem to get triggered when you select the row in the table. To work around this we take a different approach were we will use a function to perform the calculation and set the target field and place this function in a reusable view that we then add to the Coach: The view function is shown here: Now then the table row is selected we can call the view\u2019s function from the event:","title":"Using formulas"},{"location":"development/process-user-interface/#view-events-examples","text":"Events on a view (control) are the key to detecting a change and reacting to it (e.g. a data field is set or updated) with BPM UI API calls. A typical event is an on click of a button \u2013 here is an example of invoking a view\u2019s function and passing in as an argument a reference to another control\u2019s bound data property: Here is an example of reacting to a data change event and it also shows you can put multiple statements in the implementation (you can even add arbitrary JS logic like if() {} else {} ): Her is an example of detecting a table row selection:","title":"View events examples"},{"location":"development/process-user-interface/#table-view-api-manipulation","text":"The BPM UI Table view API has a number of methods, snippets of the ones that were used most regularly in the processing are shown here:","title":"Table view API manipulation"},{"location":"development/process-user-interface/#editing-table-rows-with-flyout","text":"The BPM UI Samples recommend using a view inside a table column that edits a row (and uses Deferred Sections for lazy loading). The key to this is that the Modal section is placed within a table column and bound to the list\u2019s currentItem : While this allows for editing the table row, it makes direct edits to the underlying row and does not preserve the existing value. In our scenario we want to provide an enhanced user experience so they can Edit a row and then decide whether to commit those changes or cancel them. To support this we can not use the approach of binding to the list\u2019s currentItem . Instead we create a separate local variable of type VehiclePart which represents the currently editable Part and bind that to the Modal that will include the Part editing fields: When Edit is clicked the event then calls an editPart() method defined in a reusable view and passes in the SKU for the selected row: Here is the editPart() method within Provide Repair Estimate Functions view that then sets the bound data on the mobile from the selected item in the Parts table plus it also sets a hidden field for the part index as we will need to reference this in the case of the user choosing the Save Changes option on the Modal: The Save Changes boundary event is then used to transition to a client-side script where the partsList is updated at the saved index (and if the user selects to Cancel we just close the Modal and do not make any updates to the partsList ):","title":"Editing table rows with Flyout"},{"location":"development/process-user-interface/#putting-reusable-functions-in-a-view","text":"Formulas etc on controls are great for one time actions but you often find that you want to do multiple things on an event and while you can add many statements separated by \u201c;\u201d in an event it becomes unwieldly and difficult to maintain. Also if you find yourself having to do a lot of chained behavior on different controls (e.g. a table updates so you have to check button visibility, plus popup a model, plus re-initialize a variable, etc) then it is hard to keep track. (Another example is for a Table with a requirement to switch Button visibility depending on row count \u2013 the Table\u2019s on rows loaded event worked for most cases but it did not fire when you programmatically called table.add() to add a new row). So we considered it better to have a function that does the multiple things and call that function. However if you use Custom HTML on a Coach then you can\u2019t reference that function in client-side scripts so you would end up with duplicating the logic (where you had to transition out of Coach and call the same behavioural logic). To get round this we created a view that has no content (well we put a hidden 100px Spacer on it so we could see it visually on the parent Coach canvas): And an example of referencing the functions: The downside is that the view with functions has to refer to control Id names that are volatile (if they change on Coach then the code inside the view functions will break): However you would be in a similar situation if you used Custom HTML as again you would have to edit the code for every change to a Control ID .","title":"Putting reusable functions in a view"},{"location":"development/process-user-interface/#transitioning-to-client-side-scripts","text":"While the ability to encapsulate all UI logic in a view is desirable to allow for maximum reuse, sometimes it is not possible to self-contain everything on the Coach or in a view \u2013 you have to transition out to a client-side script to complete the necessary logic. The most common situation is where you need to refer to a variable that is not bound to anything on the coach so it does not have any DOM representation that you can make BPM UI API calls (e.g. page.ui.get(\\ controlId\\ ) ) against. Here is an example from the Tender For Estimates Coach where the vehicleList.repairerList does not have any binding to a visual control and so it cannot be referenced in BPM UI API calls: Here is another example where there is a reference to duplicatePartWarning which has no binding on the Coach: (Note: duplicatePartWarning is a String bound to the Help section of a Modal Alert view from UI Toolkit that is added on the Coach and there is no exposed API method on it that lets you set the Help value thus the need to manipulate the duplicatePartWarning variable): Note in both of the above cases it may be possible to avoid these transitions if really necessary by putting hidden controls on the Coach that are bound to the variables that need to be manipulated. That way the BPM UI API can be used to access and update the bindings on those hidden controls. An other example is where the event is not exposed \u2013 such as in a Modal with buttons encapsulated within it so you cannot get access to the button events: You have to then react to the boundary event instead and transition to a client-side script : You may also be able to avoid this situation in a number of (increasing complexity in terms of coding) ways. The easiest solution might be to not encapsulate the buttons inside the Modal but have them directly on the Coach / view that needs to react to their events. An alternative, and more complex, option would be to expose the inner button events to the interface of the containing view. This requires using BPM UI Coach Framework APIs to detect the button events programmatically and to fire an event option on the parent view.","title":"Transitioning to client-side scripts"},{"location":"development/process-user-interface/#coach-views-loading-order","text":"The loading order of the tree of views contained in a coach is inside out \u2013 a section with controls loads after all the contained controls inside it have loaded and so on. We utilise a top level Vertical Layout container in each Coach and do initialization things that reference multiple controls in the onLoad() event of it \u2013 that way we can ensure all dependent controls are already loaded and can be referenced with page.ui.get(\\ controlId\\ ) / ${\\ controlId\\ } etc. Here is an example with the outermost Vertical Layout shown and in there it calls an embedded view with the JS functions to perform checking whether to enable the Complete button or not:","title":"Coach views loading order"},{"location":"development/process-user-interface/#deferred-section-views-and-asynchronous-loading","text":"A common pattern when using a Modal popup is to defer the loading of it until needed (especially if it is complex and put within the column of a table) using the Deferred Section (DS) view. You have to be very careful about the loading order \u2013 you will get errors showing up in the browser debugger such as \u201c page.ui.get(\\ controlId>) not defined \u201d if you are trying to reference DOM items that are not loaded yet. A way to avoid such issues is to encapsulate the actions in a function and ensure it is called only when the DS has loaded. Here is a view with a DS that inside has a Modal and a Table and other controls: An initializing function ( showModal() below) is called from parent Coach (say from a button click event) that checks whether the DS is loaded and if not it calls lazyLoad() on it otherwise it calls the function that prepares the view by setting visibility etc: This same prepareView() function should be called only when the DS has lazy-loaded:","title":"Deferred section views and asynchronous loading"},{"location":"development/process-user-interface/#forcing-boundary-event-triggers","text":"Sometimes you want to react to an event on a control by executing some client-side script logic but the control does not have its own boundary event that fits the need. So you add a Navigation Event control: And in the source for the event (in this example the return from a Service Call ) you call fire() on it: Which then lets you transition out to the client-side script :","title":"Forcing boundary event triggers"},{"location":"development/process-user-interface/#map-control-reacting-to-binding-changes","text":"We use a BPM UI Map view in the Tender for Estimates Coach to show the location of a Vehicle . The BPM UI Map view has configuration options for the latitude / longitude settings: While these work on first load of the Map inside the popup Modal on the Coach, subsequent changing of these values (for example when the selected Vehicle is changed) did not trigger an update of the location shown on the Map. To get around this we need to use a specific Map API method. First let\u2019s look at the sequence of events, When a Vehicle is selected the ZIP is passed to a Service Call view ( Get Coordinates ) which invokes a service to translate the ZIP into Geo coordinates. On return from that Service Call we then need to force a transition and that is done by in turn calling the fire() method of a Navigation Event view: The Navigation Event then provides us a boundary event which we can use to transition to the Show Map client-side script : And in the script (among other things) we have to explicitly call the setCenter() method on the Map to ensure it refreshes: Note that while this results in the Map correctly showing the new coordinates, however the Marker is not shown.","title":"Map control reacting to binding changes"},{"location":"development/process-user-interface/#access-view-configuration-option-complex-bo","text":"There is a defined way to navigate through the structure of a complex BO ( Business Object ) bound as a configuration option of a view. In this case you get the top level BO ( searchableVehicleParts ) and then a property that is a list ( partsList ): Then a further call uses the index to get a specific entry from the list and then reference attributes of that entry (an instance of VehiclePart BO):","title":"Access view configuration option complex BO"},{"location":"development/sundries/","text":"DBA solution development findings About BAW Process and Case data integration BAW 19.0.0.1 offers JavaScript facilities (object and associated set of methods) to allow the data integration between Process and Case. These objects and methods allow to transfer case properties into the process and subsequently to save back into the case property data updated. In processes that implement case activities, you can interact with the JavaScript case operations through the new operations that have been added to the TWProcessInstance JavaScript API in Business Automation Workflow. Useful methods are: addCommentToParentActivity addCommentToParentCase completeParentCaseStage createCaseUsingSameCaseType createCaseUsingSpecifiedCaseType createDiscretionaryActivityInParentCase createDiscretionaryActivityInParentCaseWithWorkflowParams createParentCaseDiscretionaryActivityWithProps createSubfolderUnderParentCase getParentActivityPropertyNames getParentActivityPropertyValues getParentCaseCasePropertyNames getParentCaseCasePropertyValues getParentCaseStructure relateParentCase searchParentCaseActivities setParentActivityPropertyValues setParentCaseCasePropertyValues unrelateParentCase Case properties validation Case Manager delivers basic property validation OOTB. For instance, properties are validated according to type and whether they are required or not. Other more complex validation rules, with more complex logic require the implementation of scripts. The claims processing solution implements some of these script-based property validation to showcase the concept. About ODM Integrating multiple decision services based on a common BOM While ODM rulesets are grouped in a RuleApp deployment artifact, the corresponding service definitions are generated individually for each ruleset, as shown below: In our use case, we use one claim_processing RuleApp, and we have to generate three separate service definition files, one for each segment_claim , assess_fraud and review_escalation . Because all three decision services are sharing the same input object model , importing these separate service definition files in BAW will create three separate (but identical) versions of the business objects hierarchy. One solution, using e.g. the YAML definition files, is to rename each Request and Response definitions in the generated service definition files with a unique name, and then merge the multiple YAML files into a single one that reflects the services exposed by the RuleApp.","title":"Sundries"},{"location":"development/sundries/#dba-solution-development-findings","text":"","title":"DBA solution development findings"},{"location":"development/sundries/#about-baw","text":"","title":"About BAW"},{"location":"development/sundries/#process-and-case-data-integration","text":"BAW 19.0.0.1 offers JavaScript facilities (object and associated set of methods) to allow the data integration between Process and Case. These objects and methods allow to transfer case properties into the process and subsequently to save back into the case property data updated. In processes that implement case activities, you can interact with the JavaScript case operations through the new operations that have been added to the TWProcessInstance JavaScript API in Business Automation Workflow. Useful methods are: addCommentToParentActivity addCommentToParentCase completeParentCaseStage createCaseUsingSameCaseType createCaseUsingSpecifiedCaseType createDiscretionaryActivityInParentCase createDiscretionaryActivityInParentCaseWithWorkflowParams createParentCaseDiscretionaryActivityWithProps createSubfolderUnderParentCase getParentActivityPropertyNames getParentActivityPropertyValues getParentCaseCasePropertyNames getParentCaseCasePropertyValues getParentCaseStructure relateParentCase searchParentCaseActivities setParentActivityPropertyValues setParentCaseCasePropertyValues unrelateParentCase","title":"Process and Case data integration"},{"location":"development/sundries/#case-properties-validation","text":"Case Manager delivers basic property validation OOTB. For instance, properties are validated according to type and whether they are required or not. Other more complex validation rules, with more complex logic require the implementation of scripts. The claims processing solution implements some of these script-based property validation to showcase the concept.","title":"Case properties validation"},{"location":"development/sundries/#about-odm","text":"","title":"About ODM"},{"location":"development/sundries/#integrating-multiple-decision-services-based-on-a-common-bom","text":"While ODM rulesets are grouped in a RuleApp deployment artifact, the corresponding service definitions are generated individually for each ruleset, as shown below: In our use case, we use one claim_processing RuleApp, and we have to generate three separate service definition files, one for each segment_claim , assess_fraud and review_escalation . Because all three decision services are sharing the same input object model , importing these separate service definition files in BAW will create three separate (but identical) versions of the business objects hierarchy. One solution, using e.g. the YAML definition files, is to rename each Request and Response definitions in the generated service definition files with a unique name, and then merge the multiple YAML files into a single one that reflects the services exposed by the RuleApp.","title":"Integrating multiple decision services based on a common BOM"},{"location":"development/workflow-intro/","text":"Workflow development This section covers the BAW development items of interest. Before looking at this section, it is recommended to first be familiar with the workflow design . To follow along in this section, it helps to have the solution installed as described in the Deploy section. This section is sub-divided into the following chapters: Process flow control which covers the BPMN processes and how they co-ordinate the process logic. Process services which covers the process activities implemented as service flows . Process user interface which covers the user task implementations within a process that use client-side human services and coaches . Case activities which covers the case activity implementations. Case user interface which covers the user task implementations within case activities. Workflow for emulated BAI which covers the specific workflow implementation used to allow for emulation of data loads into Business Automation Insights (BAI).","title":"Introduction"},{"location":"development/workflow-intro/#workflow-development","text":"This section covers the BAW development items of interest. Before looking at this section, it is recommended to first be familiar with the workflow design . To follow along in this section, it helps to have the solution installed as described in the Deploy section. This section is sub-divided into the following chapters: Process flow control which covers the BPMN processes and how they co-ordinate the process logic. Process services which covers the process activities implemented as service flows . Process user interface which covers the user task implementations within a process that use client-side human services and coaches . Case activities which covers the case activity implementations. Case user interface which covers the user task implementations within case activities. Workflow for emulated BAI which covers the specific workflow implementation used to allow for emulation of data loads into Business Automation Insights (BAI).","title":"Workflow development"},{"location":"environment/rhos-create-cluster/","text":"IBM Cloud allows the creation of managed OpenShift clusters. The architecture of this service is shown on the figure below, with more information available here . To deploy a managed OpenShift cluster on IBM Cloud, ensure that you have the following IBM Cloud IAM access policies: The Administrator platform role for IBM Cloud Kubernetes Service The Writer or Manager service role for IBM Cloud Kubernetes Service The Administrator platform role for IBM Cloud Container Registry Make sure that the API key for the IBM Cloud region and resource group is set up with the correct infrastructure permissions, Super User, or the minimum roles to create a cluster. Once your account has the above IAM policies: Log in to your IBM Account. Select Kubernetes from the hamburger menu and click Create Cluster . For Select a plan, choose Standard. For the Cluster type and version, choose OpenShift. Red Hat OpenShift on IBM Cloud supports OpenShift version 3.11 only, which includes Kubernetes version 1.11. The operating system is Red Hat Enterprise Linux 7. Fill out your cluster name, resource group, and tags. For the Location, set the geography to North America or Europe, select either a Single one availability zone, and then select Washington, DC or London worker zones. For Default worker pool, choose an available flavor for your worker nodes, ideally with at least 16 cores and 32 GB RAM. Set a number of worker nodes to create per zone; We will use 4 nodes for our cluster. Finish by clicking on Create cluster . From the cluster details page, click OpenShift web console. From the dropdown menu in the OpenShift container platform menu bar, click Application Console . The Application Console lists all project namespaces in your cluster. You can navigate to a namespace to view your applications, builds, and other Kubernetes resources. Prepare a boot node As the master nodes are managed by IBM Cloud and cannot be accessed via ssh , we have to choose a boot node to proceed further with the installation. The boot node can be a linux VM or your Mac laptop. The boot node needs to have sufficient disk space and latest version of the docker installed. Additionally on the boot node, various CLIs need to be installed. Install ibmcloud CLI ibmcloud can be installed using curl -sL https://ibm.biz/idt-installer | bash . Once installed, verify access: Run ibmcloud login -sso Get the onetime code to login and then select the appropriate account you have deployed the OpenShift cluster to. Run the command ibmcloud ks clusters which displays the OpenShift clusters. Install oc CLI For information about how to install the OpenShift CLI oc on your operating system's path, see the docs . Once installed, from the OpenShift web console menu bar, click your profile IAM#user.name@email.com Copy Login Command . Paste the copied oc login command into your terminal to authenticate via the CLI. Load the Cloud Pak images Download the Cloud Pak from Software Downloads or Passport Advantage. To load the package into managed OpenShift internal registry, you need to sign into the docker registry. To login to the docker registry, use the command: docker login -u $(oc whoami) -p $(oc whoami -t) docker-registry.default.svc:5000 The above command fails as docker-registry.default.svc is not accessible from outside. To expose that, open another command window. Login to OpenShift (oc, kubectl commands should be able to run).Run the command: kubectl -n default port-forward svc/docker-registry 5000:5000 This exposes port 5000 on the boot node (wherever this is run). You need to leave the window open or else the port-forwarding will stop. Update /etc/hosts with 127.0.0.1 docker-registry.default.svc and you can now login to Docker. Install Helm Create namespace tiller: oc new-project tiller oc project tiller Add the line into file ~/.bash_profile and apply the environment variable change export TILLER_NAMESPACE=tiller source ~/.bash_profile Initialize Helm wget https://get.helm.sh/helm-v2.14.2-linux-amd64.tar.gz tar xzvf helm-v2.14.2-linux-amd64.tar.gz cd linux-amd64 chmod 755 helm rm -rf /usr/local/bin/helm mv helm /usr/local/bin/helm helm init --client-only Create Tiller by OpenShift template: oc process -f https://github.com/openshift/origin/raw/master/examples/helm/tiller-template.yaml -p TILLER_NAMESPACE= tiller -p HELM_VERSION=v2.14.2 | oc create -f - oc rollout status deployment tiller helm version Setup shared services Setup NFS Install NFS: yum install nfs-utils Create share folders: mkdir -p /data/persistentvolumes mkdir -p /data/casemanagement Add the share folders paths into file /etc/exports : /data/persistentvolumes *(rw,sync,no_root_squash) /data/casemanagement *(rw,sync,no_root_squash) Start NFS: systemctl enable nfs-server.service systemctl start nfs-server.service Setup DB2 Using DB2 version v11.1.1.1 (required for BACA). Unpack installation files: yum install unzip tar xzvf DB2_AWSE_REST_Svr_11.1_Lnx_86-64.tar.gz unzip DB2_AWSE_Restricted_Activation_11.1.zip Install DB2 by response file: /data/downloads/db2/server_awse_o/db2setup -r /root/db2/db2server-dba.rsp Add license: /opt/ibm/db2/V11.1/adm/db2licm -a /data/downloads/db2/awse_o/db2/license/db2awse_o.lic You can ignore the following warning while installing DB2: Summary of prerequisites that are not met on the current system: DBT3514W The db2prereqcheck utility failed to find the following 32-bit library file: /lib/libpam.so* . DBT3514W The db2prereqcheck utility failed to find the following 32-bit library file: libstdc++.so.6 . Upgrade DB2 from v11.1.0 to v11.1.1.1: tar xzvf special_36118_linuxx64_universal_fixpack.tar.gz ## Stop the instance db2inst1 su - db2inst1 db2 list application db2 force applications all db2 terminate db2stop force db2licd -end exit ps -ef |grep db2fm /opt/ibm/db2/V11.1/bin/db2fmcu -d /opt/ibm/db2/V11.1/bin/db2fm -i db2inst1 -D ## Disable auto start /opt/ibm/db2/V11.1/instance/db2iauto -off db2inst1 su - db2inst1 ipclean exit su - dasusr1 /opt/ibm/db2/V11.1/das/bin/db2admin stop exit ## Upgrade DB2 cd /data/downloads/db2/fixpack11.1.1/universal ./installFixPack -b /opt/ibm/db2/V11.1 ## Upgrade DB2 instance /opt/ibm/db2/V11.1/instance/db2iupdt db2inst1 /opt/ibm/db2/V11.1/instance/db2iauto -on db2inst1 su - db2inst1 db2start db2level exit Create the ODM database: su - db2inst1 db2start db2 create database odmdb automatic storage yes using codeset UTF-8 territory US pagesize 32768; db2 connect to odmdb db2 list applications Setup IBM SDS Using IBM Security Directory Server version 6.4. Mount SDS ISO: mkdir /mnt/iso mount -t iso9660 -o loop /data/downloads/sds/sds64-linux-x86-64.iso /mnt/iso/ Install SDS: yum install ksh # setup ldap user and group groupadd idsldap useradd -g idsldap -d /home/idsldap -m -s /bin/ksh idsldap passwd idsldap ## enter password usermod -a -G idsldap root groups root # skip db2 installation mkdir -p /opt/ibm/ldap/V6.4/install touch /opt/ibm/ldap/V6.4/install/IBMLDAP_INSTALL_SKIPDB2REQ # install gskit cd /mnt/iso/ibm_gskit rpm -Uhv gsk*linux.x86_64.rpm # install sds rpms cd /mnt/iso/license ./idsLicense ## Enter 1 to accept the license agreement cd /mnt/iso/images rpm --force -ihv idsldap*rpm cd cd /data/downloads/sds unzip sds64-premium-feature-act-pkg.zip cd /data/downloads/sds/sdsV6.4/entitlement rpm --force -ihv idsldap-ent64-6.4.0-0.x86_64.rpm # install ibm jdk cd /mnt/iso/ibm_jdk tar -xf 6.0.16.2-ISS-JAVA-LinuxX64-FP0002.tar -C /opt/ibm/ldap/V6.4/ # setup db2 path vi /opt/ibm/ldap/V6.4/etc/ldapdb.properties currentDB2InstallPath=/opt/ibm/db2/V11.1 currentDB2Version=11.1.0.0 # create and configure instance cd /opt/ibm/ldap/V6.4/sbin ./idsadduser -u dsinst1 -g grinst1 -w password # Enter 1 to continue # create instance ./idsicrt -I dsinst1 -p 389 -s 636 -e mysecretkey! -l /home/dsinst1 -G grinst1 -w password # Enter 1 to continue # configure a database for a directory server instance. ./idscfgdb -I dsinst1 -a dsinst1 -w password -t dsinst1 -l /home/dsinst1 # set the administration DN and administrative password for an instance ./idsdnpw -I dsinst1 -u cn=root -p password # Add suffix ./idscfgsuf -I dsinst1 -s o=IBM,c=US Start SDS Server: ./ibmslapd -I dsinst1 ./ibmdiradm -I dsinst1","title":"Create OCP cluster"},{"location":"environment/rhos-create-cluster/#prepare-a-boot-node","text":"As the master nodes are managed by IBM Cloud and cannot be accessed via ssh , we have to choose a boot node to proceed further with the installation. The boot node can be a linux VM or your Mac laptop. The boot node needs to have sufficient disk space and latest version of the docker installed. Additionally on the boot node, various CLIs need to be installed.","title":"Prepare a boot node"},{"location":"environment/rhos-create-cluster/#install-ibmcloud-cli","text":"ibmcloud can be installed using curl -sL https://ibm.biz/idt-installer | bash . Once installed, verify access: Run ibmcloud login -sso Get the onetime code to login and then select the appropriate account you have deployed the OpenShift cluster to. Run the command ibmcloud ks clusters which displays the OpenShift clusters.","title":"Install ibmcloud CLI"},{"location":"environment/rhos-create-cluster/#install-oc-cli","text":"For information about how to install the OpenShift CLI oc on your operating system's path, see the docs . Once installed, from the OpenShift web console menu bar, click your profile IAM#user.name@email.com Copy Login Command . Paste the copied oc login command into your terminal to authenticate via the CLI.","title":"Install oc CLI"},{"location":"environment/rhos-create-cluster/#load-the-cloud-pak-images","text":"Download the Cloud Pak from Software Downloads or Passport Advantage. To load the package into managed OpenShift internal registry, you need to sign into the docker registry. To login to the docker registry, use the command: docker login -u $(oc whoami) -p $(oc whoami -t) docker-registry.default.svc:5000 The above command fails as docker-registry.default.svc is not accessible from outside. To expose that, open another command window. Login to OpenShift (oc, kubectl commands should be able to run).Run the command: kubectl -n default port-forward svc/docker-registry 5000:5000 This exposes port 5000 on the boot node (wherever this is run). You need to leave the window open or else the port-forwarding will stop. Update /etc/hosts with 127.0.0.1 docker-registry.default.svc and you can now login to Docker.","title":"Load the Cloud Pak images"},{"location":"environment/rhos-create-cluster/#install-helm","text":"Create namespace tiller: oc new-project tiller oc project tiller Add the line into file ~/.bash_profile and apply the environment variable change export TILLER_NAMESPACE=tiller source ~/.bash_profile Initialize Helm wget https://get.helm.sh/helm-v2.14.2-linux-amd64.tar.gz tar xzvf helm-v2.14.2-linux-amd64.tar.gz cd linux-amd64 chmod 755 helm rm -rf /usr/local/bin/helm mv helm /usr/local/bin/helm helm init --client-only Create Tiller by OpenShift template: oc process -f https://github.com/openshift/origin/raw/master/examples/helm/tiller-template.yaml -p TILLER_NAMESPACE= tiller -p HELM_VERSION=v2.14.2 | oc create -f - oc rollout status deployment tiller helm version","title":"Install Helm"},{"location":"environment/rhos-create-cluster/#setup-shared-services","text":"","title":"Setup shared services"},{"location":"environment/rhos-create-cluster/#setup-nfs","text":"Install NFS: yum install nfs-utils Create share folders: mkdir -p /data/persistentvolumes mkdir -p /data/casemanagement Add the share folders paths into file /etc/exports : /data/persistentvolumes *(rw,sync,no_root_squash) /data/casemanagement *(rw,sync,no_root_squash) Start NFS: systemctl enable nfs-server.service systemctl start nfs-server.service","title":"Setup NFS"},{"location":"environment/rhos-create-cluster/#setup-db2","text":"Using DB2 version v11.1.1.1 (required for BACA). Unpack installation files: yum install unzip tar xzvf DB2_AWSE_REST_Svr_11.1_Lnx_86-64.tar.gz unzip DB2_AWSE_Restricted_Activation_11.1.zip Install DB2 by response file: /data/downloads/db2/server_awse_o/db2setup -r /root/db2/db2server-dba.rsp Add license: /opt/ibm/db2/V11.1/adm/db2licm -a /data/downloads/db2/awse_o/db2/license/db2awse_o.lic You can ignore the following warning while installing DB2: Summary of prerequisites that are not met on the current system: DBT3514W The db2prereqcheck utility failed to find the following 32-bit library file: /lib/libpam.so* . DBT3514W The db2prereqcheck utility failed to find the following 32-bit library file: libstdc++.so.6 . Upgrade DB2 from v11.1.0 to v11.1.1.1: tar xzvf special_36118_linuxx64_universal_fixpack.tar.gz ## Stop the instance db2inst1 su - db2inst1 db2 list application db2 force applications all db2 terminate db2stop force db2licd -end exit ps -ef |grep db2fm /opt/ibm/db2/V11.1/bin/db2fmcu -d /opt/ibm/db2/V11.1/bin/db2fm -i db2inst1 -D ## Disable auto start /opt/ibm/db2/V11.1/instance/db2iauto -off db2inst1 su - db2inst1 ipclean exit su - dasusr1 /opt/ibm/db2/V11.1/das/bin/db2admin stop exit ## Upgrade DB2 cd /data/downloads/db2/fixpack11.1.1/universal ./installFixPack -b /opt/ibm/db2/V11.1 ## Upgrade DB2 instance /opt/ibm/db2/V11.1/instance/db2iupdt db2inst1 /opt/ibm/db2/V11.1/instance/db2iauto -on db2inst1 su - db2inst1 db2start db2level exit Create the ODM database: su - db2inst1 db2start db2 create database odmdb automatic storage yes using codeset UTF-8 territory US pagesize 32768; db2 connect to odmdb db2 list applications","title":"Setup DB2"},{"location":"environment/rhos-create-cluster/#setup-ibm-sds","text":"Using IBM Security Directory Server version 6.4. Mount SDS ISO: mkdir /mnt/iso mount -t iso9660 -o loop /data/downloads/sds/sds64-linux-x86-64.iso /mnt/iso/ Install SDS: yum install ksh # setup ldap user and group groupadd idsldap useradd -g idsldap -d /home/idsldap -m -s /bin/ksh idsldap passwd idsldap ## enter password usermod -a -G idsldap root groups root # skip db2 installation mkdir -p /opt/ibm/ldap/V6.4/install touch /opt/ibm/ldap/V6.4/install/IBMLDAP_INSTALL_SKIPDB2REQ # install gskit cd /mnt/iso/ibm_gskit rpm -Uhv gsk*linux.x86_64.rpm # install sds rpms cd /mnt/iso/license ./idsLicense ## Enter 1 to accept the license agreement cd /mnt/iso/images rpm --force -ihv idsldap*rpm cd cd /data/downloads/sds unzip sds64-premium-feature-act-pkg.zip cd /data/downloads/sds/sdsV6.4/entitlement rpm --force -ihv idsldap-ent64-6.4.0-0.x86_64.rpm # install ibm jdk cd /mnt/iso/ibm_jdk tar -xf 6.0.16.2-ISS-JAVA-LinuxX64-FP0002.tar -C /opt/ibm/ldap/V6.4/ # setup db2 path vi /opt/ibm/ldap/V6.4/etc/ldapdb.properties currentDB2InstallPath=/opt/ibm/db2/V11.1 currentDB2Version=11.1.0.0 # create and configure instance cd /opt/ibm/ldap/V6.4/sbin ./idsadduser -u dsinst1 -g grinst1 -w password # Enter 1 to continue # create instance ./idsicrt -I dsinst1 -p 389 -s 636 -e mysecretkey! -l /home/dsinst1 -G grinst1 -w password # Enter 1 to continue # configure a database for a directory server instance. ./idscfgdb -I dsinst1 -a dsinst1 -w password -t dsinst1 -l /home/dsinst1 # set the administration DN and administrative password for an instance ./idsdnpw -I dsinst1 -u cn=root -p password # Add suffix ./idscfgsuf -I dsinst1 -s o=IBM,c=US Start SDS Server: ./ibmslapd -I dsinst1 ./ibmdiradm -I dsinst1","title":"Setup IBM SDS"},{"location":"environment/rhos-install-baca/","text":"Prepare your environment by following the instructions on IBM Cloud Pak for Automation 19.0.1 on Certified Kubernetes . The instructions ask you to validate that you have the right version of Kubernetes, Helm, Kubernetes CLI and OpenShift container platform CLI installed on the machine that you are using for installation. Furthermore, the containers require access to database(s) and LDAP (if using LDAP). Ensure that the version of the database and LDAP are supported for IBM Cloud Pak for Automation version that you are installing. Ensure that you have downloaded PPA package for the version of IBM Business Automation Content Analyzer you are planning to install. We used ICP4A19.0.1-baca.tgz to install IBM Business Automation Content Analyzer version 19.0.1. Download the loadimages.sh script from GitHub. Create an OpenShift project (namespace) where you want to install IBM Business Automation Content Analyzer version 19.0.1 and make the new project (namespace) the current project. To create new project use command oc new-project projectname where projectname is the name of the project for example baca . Login to the OpenShift Docker registry following instructions in the Load the Cloud Pak images . Run the loadimages.sh script to tag and push the product container images into your Docker registry. Specify the two mandatory parameters in the command line that loadimages.sh script requires. The namespace is the projectname that you specified earlier. In the screen shot below, target docker registry is docker-registry.default.svc:5000 and namespace is denimcompute . IBM BACA 19.0.1 consists of 17 images and the loadimages.sh script lists all the images that it uploads to the Docker registry on successful completion. Prepare environment for IBM Business Automation Content Analyzer. See Preparing to install automation containers on Kubernetes . These procedures include setting up databases, LDAP, storage, and configuration files that are required for use and operation. If you are using LDAP for authentication, BACA users need to be configured on the LDAP server. An initial user is created in IBM Business Automation Content Analyzer when first creating the Db2 database. The user name must match the LDAP user name when specified. If LDAP is not used for authentication, pre-setup of users is not required. Refer to Preparing users for IBM Business Automation Content Analyzer for more information. If you do not have a DB2 database, set up a new DB2 environment. Download database scripts from this GitHub repo . Refer to Creating and Configuring a DB2 database for more information. Log into DB2 instance using DB2 instance administrator id such as db2inst1 (refer the section Setup DB2 . Set up the base database by running the following command: ./CreateBaseDB.sh. Enter the following details: Name of the IBM Business Automation Content Analyzer Base database - (enter a unique name of 8 characters or less and no special characters). Name of database user - (enter a database user name) - can be a new or existing Db2 user. Password for the user - (enter a password) - each time when prompted. If it is an existing user, the prompt is skipped Enter Y when you are asked Would you like to continue (Y/N): Create the tenant database by following these instructions . Once the tenant database is created and populated with initial content, the tenant id and ontology is displayed (see screen shots below). External storage is needed in the content services environment. You set up and configure storage to prepare for the container configuration and deployment. Follow instructions at Configuring storage for the Business Automation Content Analyzer environment to create persistent volumes (PV) and persistent volume claims (PVC). The screen below shows pv and pvc created in project projectname (for example baca ) created earlier. You need to create SSL certificates and secrets before you install via Helm chart. You can run the init_deployment.sh script provided here to create SSL certificates and secrets. The init_deployment.sh scripts requires you to populate parameters in common.sh file. Use common_OCP_template.sh as basis for common.sh to be used for installation on OpenShift. Information on populating the parameters in common.sh can be found at Prerequisite install parameters . The screen shot below captures common.sh that we used in our installation process. Please note that we selected PVCCHOICE as 2 as we created, persistent volumes and persistent volume claims manually in the earlier step. Furthermore, please set HELM_INIT_BEFORE as Y as Helm has been installed earlier and we do not want to install Helm as part of running init_deployment.sh script. Run ./init_deployment.sh so that it creates SSL certificates and secrets required for Content Analyzer. Please note the init_deployment.sh uses loginToCluster function in bashfunctions.sh to log into OpenShift cluster. The loginToCluster function assumes that Kubernetes API server is exposed on port 8443 and also requires user id and password to log into the cluster. If you are using Managed OpenShift cluster on IBM Cloud then this assumption is not valid. Thus, you will have to modify the function to use login command copied from OpenShift web console. Copy login command is available as drop down in upper right corner of OpenShift web console and update loginToCLuster function with the copied command. Please make sure that before you run the script, the OpenShift project that you created earlier is the current project. If it is not, make it a current project using the command oc project projectname . Validate the objects created by running the following commands. To check secrets, run the following command: kubectl -n projectname get secrets or oc get secrets Verify that 9 secrets were created (7 if not using LDAP or ingress). Refer to Create PVs, PVCs, certificates and secrets using init_deployment.sh for more information. Run ./generateMemoryValues.sh limited or ./generateMemoryValues.sh distributed . For smaller system (5 worker-nodes or less) where the Mongo database pods will be on the same worker node as other pods, use limited option. Copy these values for replacement in the values.yaml file if you want to deploy CA using Helm chart or replacing these values in the ca-deploy.yml file if you want to deploy CA using kubernetes YAML files. Refer to Limiting the amount of available memory for more information. Download the Helm Chart . Extract the helm chart from ibm-dba-prod-1.0.0.tgz and copy to the ibm-dbamc-baca-prod directory. Edit the values.yaml file to populate configuration parameters. Go through the Helm Chart configuration parameters section and populating values.yaml with correct values for options with the parameters and values. Note that anything not documented here typically does not need to be changed. After the values.yaml is filled out properly, you can proceed to deploy Content Analyzer with the Helm chart. However, before you run helm, the Tiller server will need at least \"edit\" access to each project where it will manage applications. In the case that Tiller will be handling Charts containing Role objects, admin access will be needed. Refer to Getting started with Helm on OpenShift for more information. Run the command oc policy add-role-to-user edit \"system:serviceaccount:${TILLER_NAMESPACE}:tiller\" to grant the Tiller edit access to the project. To deploy Content Analyzer, from the ibm-dba-baca-prod directory run: $ helm install . --name celery projectname -f values.yaml --namespace projectname --tiller-namespace tiller where projectname is the name of the project you created earlier. Due to the configuration of the readiness probes, after the pods start, it may take up to 10 or more minutes before the pods enter a ready state. Refer to Deploying with Helm charts for more information. Once all the pods are running, complete the post deployments steps listed here .","title":"Installing BACA"},{"location":"environment/rhos-install-bai/","text":"Install Kafka Add the Helm repository: helm repo add incubator http://storage.googleapis.com/kubernetes-charts-incubator Create the namespace: oc new-project kafka oc project kafka ## Grant the tiller server edit access to current project oc adm policy add-role-to-user edit system:serviceaccount:tiller:tiller Install the Helm chart: helm install incubator/kafka --name my-kafka --namespace kafka -f values.yaml Set-up the Kafka bootstrap server: If access is from inside the OpenShift cluster environment, then the bootstrap server is my-kafka-headless.kafka.svc.cluster.local:9092 . If access is from external system, we need to: 1- Retrieve the OpenShift ingress address with the following command: 2- Use this ingress address to set the bootstrap server to {rhos-ingress-ip}:31090,{rhos-ingress-ip}:31091,{rhos-ingress-ip}:31092 . 3- Add the line {rhos-ingress-ip} kafka.cluster.local to the /etc/hosts file. Install BAI Create PVs on the NFS server: mkdir -p /data/persistentvolumes/bai/ibm-bai-pv sudo chown 9999:9999 /data/persistentvolumes/bai/ibm-bai-pv sudo chmod 770 /data/persistentvolumes/bai/ibm-bai-pv mkdir -p /data/persistentvolumes/bai/ibm-bai-ek-pv-0 mkdir -p /data/persistentvolumes/bai/ibm-bai-ek-pv-1 mkdir -p /data/persistentvolumes/bai/ibm-bai-ek-pv-2 mkdir -p /data/persistentvolumes/bai/ibm-bai-ek-pv-3 sudo chown 1000:1000 /data/persistentvolumes/bai/ibm-bai-ek-pv-0 sudo chown 1000:1000 /data/persistentvolumes/bai/ibm-bai-ek-pv-1 sudo chown 1000:1000 /data/persistentvolumes/bai/ibm-bai-ek-pv-2 sudo chown 1000:1000 /data/persistentvolumes/bai/ibm-bai-ek-pv-3 sudo chmod 770 /data/persistentvolumes/bai/ibm-bai-ek-pv-0 sudo chmod 770 /data/persistentvolumes/bai/ibm-bai-ek-pv-1 sudo chmod 770 /data/persistentvolumes/bai/ibm-bai-ek-pv-2 sudo chmod 770 /data/persistentvolumes/bai/ibm-bai-ek-pv-3 mkdir /data/persistentvolumes/bai/es-snapshots-pv sudo chown 1000:1000 /data/persistentvolumes/bai/es-snapshots-pv sudo chmod 770 /data/persistentvolumes/bai/es-snapshots-pv Create namespace baiproject : oc new-project baiproject oc project baiproject ## Grant the tiller server edit access to current project oc adm policy add-role-to-user edit system:serviceaccount:tiller:tiller Update SCC: oc create serviceaccount bai-prod-release-bai-psp-sa oc adm policy add-scc-to-user ibm-privileged-scc -z bai-prod-release-bai-psp-sa oc adm policy add-scc-to-user ibm-anyuid-scc -z bai-prod-release-bai-psp-sa Login to Docker and push BAI images: docker login -u $(oc whoami) -p $(oc whoami -t) docker-registry.default.svc:5000 wget https://raw.githubusercontent.com/icp4a/cert-kubernetes/19.0.1/scripts/loadimages.sh chmod +x loadimages.sh ./loadimages.sh -p /data/downloads/icp4a/ICP4A19.0.1-bai.tgz -r docker-registry.default.svc:5000/baiproject Create PV: oc apply -f pv.yaml Download the Helm chart and install it: wget https://github.com/icp4a/cert-kubernetes/raw/master/BAI/helm-charts/ibm-business-automation-insights-3.1.0.tgz helm install ibm-business-automation-insights-3.1.0.tgz --name bai-prod-release --namespace baiproject -f values.yaml Expose the Kibana service oc apply -f route.yaml","title":"Installing BAI"},{"location":"environment/rhos-install-bai/#install-kafka","text":"Add the Helm repository: helm repo add incubator http://storage.googleapis.com/kubernetes-charts-incubator Create the namespace: oc new-project kafka oc project kafka ## Grant the tiller server edit access to current project oc adm policy add-role-to-user edit system:serviceaccount:tiller:tiller Install the Helm chart: helm install incubator/kafka --name my-kafka --namespace kafka -f values.yaml Set-up the Kafka bootstrap server: If access is from inside the OpenShift cluster environment, then the bootstrap server is my-kafka-headless.kafka.svc.cluster.local:9092 . If access is from external system, we need to: 1- Retrieve the OpenShift ingress address with the following command: 2- Use this ingress address to set the bootstrap server to {rhos-ingress-ip}:31090,{rhos-ingress-ip}:31091,{rhos-ingress-ip}:31092 . 3- Add the line {rhos-ingress-ip} kafka.cluster.local to the /etc/hosts file.","title":"Install Kafka"},{"location":"environment/rhos-install-bai/#install-bai","text":"Create PVs on the NFS server: mkdir -p /data/persistentvolumes/bai/ibm-bai-pv sudo chown 9999:9999 /data/persistentvolumes/bai/ibm-bai-pv sudo chmod 770 /data/persistentvolumes/bai/ibm-bai-pv mkdir -p /data/persistentvolumes/bai/ibm-bai-ek-pv-0 mkdir -p /data/persistentvolumes/bai/ibm-bai-ek-pv-1 mkdir -p /data/persistentvolumes/bai/ibm-bai-ek-pv-2 mkdir -p /data/persistentvolumes/bai/ibm-bai-ek-pv-3 sudo chown 1000:1000 /data/persistentvolumes/bai/ibm-bai-ek-pv-0 sudo chown 1000:1000 /data/persistentvolumes/bai/ibm-bai-ek-pv-1 sudo chown 1000:1000 /data/persistentvolumes/bai/ibm-bai-ek-pv-2 sudo chown 1000:1000 /data/persistentvolumes/bai/ibm-bai-ek-pv-3 sudo chmod 770 /data/persistentvolumes/bai/ibm-bai-ek-pv-0 sudo chmod 770 /data/persistentvolumes/bai/ibm-bai-ek-pv-1 sudo chmod 770 /data/persistentvolumes/bai/ibm-bai-ek-pv-2 sudo chmod 770 /data/persistentvolumes/bai/ibm-bai-ek-pv-3 mkdir /data/persistentvolumes/bai/es-snapshots-pv sudo chown 1000:1000 /data/persistentvolumes/bai/es-snapshots-pv sudo chmod 770 /data/persistentvolumes/bai/es-snapshots-pv Create namespace baiproject : oc new-project baiproject oc project baiproject ## Grant the tiller server edit access to current project oc adm policy add-role-to-user edit system:serviceaccount:tiller:tiller Update SCC: oc create serviceaccount bai-prod-release-bai-psp-sa oc adm policy add-scc-to-user ibm-privileged-scc -z bai-prod-release-bai-psp-sa oc adm policy add-scc-to-user ibm-anyuid-scc -z bai-prod-release-bai-psp-sa Login to Docker and push BAI images: docker login -u $(oc whoami) -p $(oc whoami -t) docker-registry.default.svc:5000 wget https://raw.githubusercontent.com/icp4a/cert-kubernetes/19.0.1/scripts/loadimages.sh chmod +x loadimages.sh ./loadimages.sh -p /data/downloads/icp4a/ICP4A19.0.1-bai.tgz -r docker-registry.default.svc:5000/baiproject Create PV: oc apply -f pv.yaml Download the Helm chart and install it: wget https://github.com/icp4a/cert-kubernetes/raw/master/BAI/helm-charts/ibm-business-automation-insights-3.1.0.tgz helm install ibm-business-automation-insights-3.1.0.tgz --name bai-prod-release --namespace baiproject -f values.yaml Expose the Kibana service oc apply -f route.yaml","title":"Install BAI"},{"location":"environment/rhos-install-baw/","text":"Add the following content in the file /etc/security/limits.conf on all servers: # - stack - maximum stack size (KB) root soft stack 32768 root hard stack 32768 # - nofile - maximum number of open files root soft nofile 65536 root hard nofile 65536 # - nproc - maximum number of processes root soft nproc 16384 root hard nproc 16384 # - fsize - maximum file size root soft fsize 6291453 root hard fsize 6291453 Execute the scripts in all servers: echo 3000 /proc/sys/net/core/netdev_max_backlog echo 3000 /proc/sys/net/core/somaxconn echo 15 /proc/sys/net/ipv4/tcp_keepalive_intvl echo 5 /proc/sys/net/ipv4/tcp_keepalive_probes Unpack the installation files: tar xzvf BAW_18_0_0_1_Linux_x86_1_of_3.tar.gz tar xzvf BAW_18_0_0_1_Linux_x86_2_of_3.tar.gz tar xzvf BAW_18_0_0_1_Linux_x86_3_of_3.tar.gz apt-get install unzip unzip 8.5.5-WS-WAS-FP015-part1.zip unzip 8.5.5-WS-WAS-FP015-part2.zip unzip 8.5.5-WS-WAS-FP015-part3.zip Install the Installation Manager: cd /downloads/BAW18001/IM64/tools ./imcl install com.ibm.cic.agent -repositories /downloads/BAW18001/IM64/repository.config -installationDirectory /opt/ibm/IM/eclipse -showVerboseProgress -log IM_Installation.log -acceptLicense Install BAW: cd /opt/ibm/IM/eclipse/tools/ # install BAW 18.0.0.1 ./imcl install com.ibm.bpm.ADV.v85,WorkflowEnterprise.NonProduction com.ibm.websphere.ND.v85,core.feature,ejbdeploy,thinclient,embeddablecontainer,samples,com.ibm.sdk.6_64bit -acceptLicense -installationDirectory /opt/IBM/BPM -repositories /downloads/BAW18001/repository/repos_64bit -properties user.wasjava=java8 -showVerboseProgress -log silentinstall.log # install BAW 19.0.0.1 and WAS 8.5.5.15 fix pack ./imcl install com.ibm.websphere.ND.v85 com.ibm.bpm.ADV.v85,WorkflowEnterprise.NonProduction -acceptLicense -installationDirectory /opt/IBM/BPM -repositories /downloads/WAS85515/repository.config,/downloads/BAW19001/workflow.19001.delta.repository.zip -properties user.wasjava=java8 -showVerboseProgress -log silent_update.txt # check the installed packages /opt/IBM/BPM/bin/versionInfo.sh -maintenancePackages Mount share folder for Case Management on all servers: apt-get install nfs-common mkdir -p /data/casemanagement mount {nfs-machine-ip}:/data/casemanagement /data/casemanagement The sample installation properties file can be found in the folder /opt/IBM/BPM/BPM/samples/config/advanced/Advanced-PC-ThreeClusters-DB2.properties . Create a copy of the properties file, then update server hostname, dbserver connection and set the property bpm.de.caseManager.networkSharedDirectory to NFS share folder /data/casemanagement . Create BAW Deployment manager node profile and node1 profile: cd /opt/IBM/BPM/bin ./BPMConfig.sh -create -de Advanced-PC-ThreeClusters-DB2.properties /opt/IBM/BPM/profiles/DmgrProfile/bin/startManager.sh /opt/IBM/BPM/profiles/Node1Profile/bin/startNode.sh Create BAW node2 profile: cd /opt/IBM/BPM/bin ./BPMConfig.sh -create -de Advanced-PC-ThreeClusters-DB2.properties /opt/IBM/BPM/profiles/Node2Profile/bin/startNode.sh Create database: The DB scripts can be found in /opt/IBM/BPM/profiles/DmgrProfile/dbscripts db2 -stf CMNDB-Cell/createDatabase.sql db2 connect to CMNDB db2 -tvf CMNDB-Cell/createSchema_Advanced.sql db2 -tvf CMNDB/createSchema_Advanced.sql db2 -tvf CMNDB/createSchema_Messaging.sql db2 connect reset db2 -stf BPMDB/createDatabase.sql db2 connect to BPMDB db2 -tvf BPMDB/createSchema_Advanced.sql db2 -tdGO -vf BPMDB/createProcedure_Advanced.sql db2 connect reset db2 -stf PDWDB/createDatabase.sql db2 connect to PDWDB db2 -tvf PDWDB/createSchema_Advanced.sql db2 connect reset mkdir -p /home/db2inst1/db2inst1/CPEDB/DOSSA/datafs1 mkdir -p /home/db2inst1/db2inst1/CPEDB/DOSSA/datafs2 mkdir -p /home/db2inst1/db2inst1/CPEDB/DOSSA/datafs3 mkdir -p /home/db2inst1/db2inst1/CPEDB/DOSSA/indexfs1 mkdir -p /home/db2inst1/db2inst1/CPEDB/DOSSA/indexfs2 mkdir -p /home/db2inst1/db2inst1/CPEDB/DOSSA/lobfs1 mkdir -p /home/db2inst1/db2inst1/CPEDB/TOSSA/datafs1 mkdir -p /home/db2inst1/db2inst1/CPEDB/TOSSA/datafs2 mkdir -p /home/db2inst1/db2inst1/CPEDB/TOSSA/datafs3 mkdir -p /home/db2inst1/db2inst1/CPEDB/TOSSA/indexfs1 mkdir -p /home/db2inst1/db2inst1/CPEDB/TOSSA/indexfs2 mkdir -p /home/db2inst1/db2inst1/CPEDB/TOSSA/lobfs1 mkdir -p /home/db2inst1/db2inst1/CPEDB/sys mkdir -p /home/db2inst1/db2inst1/CPEDB/systmp mkdir -p /home/db2inst1/db2inst1/CPEDB/usr mkdir -p /home/db2inst1/db2inst1/CPEDB/log chmod -R 777 /home/db2inst1/db2inst1/CPEDB # replace @DB_DIR@ with /home/db2inst1/db2inst1 in CPEDB/createDatabase_ECM.sql and CPEDB/createTablespace_Advanced.sql db2 -stf CPEDB/createDatabase_ECM.sql db2 connect to CPEDB db2 -tvf CPEDB/createTablespace_Advanced.sql db2 connect reset db2 connect to CPEDB db2 -tvf CPEDB/createSchema_Advanced.sql db2 connect reset Bootstrap Process Server Data: /opt/IBM/BPM/profiles/DmgrProfile/bin/bootstrapProcessServerData.sh -clusterName AppCluster Create a group caseAdmin in the WAS Admin console, and assign deadmin to the group. Create the Object Store for Case Management: /opt/IBM/BPM/profiles/DmgrProfile/bin/wsadmin.sh -user deadmin -password deadmin -host dbamc-icp-ubuntu-baw3.csplab.local -port 8880 -lang jython print AdminTask.createObjectStoreForContent(['-clusterName', 'AppCluster', '-PEWorkflowSystemAdminGroup', 'caseAdmin','-creationUser','deadmin','-password','deadmin']) Update connection timeout setting to at least 600 seconds in the WAS Admin console: * Servers Server Types WebSphere application servers Configuration tab Container Settings Container Services Transaction service Total transaction lifetime timeout * Servers Server Types WebSphere application servers Configuration tab Container Settings Container Services Transaction service Maximum transaction lifetime timeout * Servers Server Types WebSphere application servers Configuration tab Container Settings Container Services ORB service Request timeout * Servers Server Types WebSphere application servers Configuration tab Container Settings Container Services ORB service Locate request timeout * Resources JDBC Data sources [Content Engine or Case Manager data source name] Connection Pool properties Connection timeout * Resources JDBC Data sources [Content Engine or Case Manager XA data source name] Connection Pool properties Connection timeout Configure Case Management profile: ## Register the Administration Console for Content Platform Engine (ACCE) Plug-in /opt/IBM/BPM/CaseManagement/configure/configmgr_cl execute -task registeracceplugin -profile /opt/IBM/BPM/profiles/DmgrProfile/CaseManagement/De1/profiles/ICM_dev/ICM_dev.cfgp ## Configure the Case Management Object Stores /opt/IBM/BPM/CaseManagement/configure/configmgr_cl execute -task configcmos -profile /opt/IBM/BPM/profiles/DmgrProfile/CaseManagement/De1/profiles/ICM_dev/ICM_dev.cfgp ## Define the Default Project Area /opt/IBM/BPM/CaseManagement/configure/configmgr_cl execute -task definedefaultprojectarea -profile /opt/IBM/BPM/profiles/DmgrProfile/CaseManagement/De1/profiles/ICM_dev/ICM_dev.cfgp ## Configure Case Integration with IBM Business Automation Workflow /opt/IBM/BPM/CaseManagement/configure/configmgr_cl execute -task configibmbpm -profile /opt/IBM/BPM/profiles/DmgrProfile/CaseManagement/De1/profiles/ICM_dev/ICM_dev.cfgp ## Deploy the Content Platform Engine Workflow Service /opt/IBM/BPM/CaseManagement/configure/configmgr_cl execute -task deployibmbpmis -profile /opt/IBM/BPM/profiles/DmgrProfile/CaseManagement/De1/profiles/ICM_dev/ICM_dev.cfgp ## Register the IBM Business Automation Workflow Plug-in /opt/IBM/BPM/CaseManagement/configure/configmgr_cl execute -task registerbawplugin -profile /opt/IBM/BPM/profiles/DmgrProfile/CaseManagement/De1/profiles/ICM_dev/ICM_dev.cfgp ## Register the Case Management Services Plug-in /opt/IBM/BPM/CaseManagement/configure/configmgr_cl execute -task registerservices -profile /opt/IBM/BPM/profiles/DmgrProfile/CaseManagement/De1/profiles/ICM_dev/ICM_dev.cfgp ## Register the Case Widgets Package /opt/IBM/BPM/CaseManagement/configure/configmgr_cl execute -task registercaseclient -profile /opt/IBM/BPM/profiles/DmgrProfile/CaseManagement/De1/profiles/ICM_dev/ICM_dev.cfgp ## Register the IBM Business Automation Workflow Case Administration Client Plug-in /opt/IBM/BPM/CaseManagement/configure/configmgr_cl execute -task registeradmin -profile /opt/IBM/BPM/profiles/DmgrProfile/CaseManagement/De1/profiles/ICM_dev/ICM_dev.cfgp ## Register Project Area /opt/IBM/BPM/CaseManagement/configure/configmgr_cl execute -task registerprojectarea -profile /opt/IBM/BPM/profiles/DmgrProfile/CaseManagement/De1/profiles/ICM_dev/ICM_dev.cfgp ## Configure Business Rules /opt/IBM/BPM/CaseManagement/configure/configmgr_cl execute -task configrules -profile /opt/IBM/BPM/profiles/DmgrProfile/CaseManagement/De1/profiles/ICM_dev/ICM_dev.cfgp ## Register the Case Monitor Widgets Package /opt/IBM/BPM/CaseManagement/configure/configmgr_cl execute -task registericmmonitor -profile /opt/IBM/BPM/profiles/DmgrProfile/CaseManagement/De1/profiles/ICM_dev/ICM_dev.cfgp Enable BAI event /opt/IBM/BPM/profiles/DmgrProfile/bin/wsadmin.sh -user deadmin -password deadmin -f /opt/IBM/BPM/BPM/Lombardi/tools/def/EnableBAI.py --enable /opt/IBM/BPM/profiles/DmgrProfile/bin/wsadmin.sh -conntype none -profileName DmgrProfile -f /opt/IBM/BPM/ProcessChoreographer/admin/setStateObserver.py -cluster AppCluster -enable DEF Restart the BAW server The sample BAI configuration file is in the path /opt/IBM/BPM/BPM/Lombardi/tools/def/BAIConfigure.properties , update it and then execute: /opt/IBM/BPM/profiles/DmgrProfile/bin/wsadmin.sh -user deadmin -password deadmin -f /opt/IBM/BPM/BPM/Lombardi/tools/def/EnableBAI.py --update=connection --property= /root/baw/BAIConfigure.properties Restart the BPMEventEmitter application from WAS admin console.","title":"Installing BAW"},{"location":"environment/rhos-install-cm/","text":"Content Engine Create GCD and Object Store databases: mkdir -p /data/db2fs mkdir -p /data/database/config ## copy the GCDDB.sh and OS1DB.sh into folder /data/database/config export CUR_COMMIT=ON su - db2inst1 -c db2set DB2_WORKLOAD=FILENET_CM echo set CUR_COMMIT=$CUR_COMMIT chown db2inst1:db2iadm1 /data/database/config/*.sh chmod 755 /data/database/config/*.sh chown -R db2inst1:db2iadm1 /data/db2fs su - db2inst1 -c /data/database/config/GCDDB.sh GCDDB su - db2inst1 -c /data/database/config/OS1DB.sh OS1DB Create PVs in NFS server: mkdir -p /data/persistentvolumes/cpe/configDropins/overrides mkdir -p /data/persistentvolumes/cpe/logs mkdir -p /data/persistentvolumes/cpe/cpefnlogstore mkdir -p /data/persistentvolumes/cpe/bootstrap mkdir -p /data/persistentvolumes/cpe/textext mkdir -p /data/persistentvolumes/cpe/icmrules mkdir -p /data/persistentvolumes/cpe/asa chown 50001:50000 /data/persistentvolumes/cpe/configDropins chown 50001:50000 /data/persistentvolumes/cpe/configDropins/overrides chown 50001:50000 /data/persistentvolumes/cpe/logs chown 50001:50000 /data/persistentvolumes/cpe/cpefnlogstore chown 50001:50000 /data/persistentvolumes/cpe/bootstrap chown 50001:50000 /data/persistentvolumes/cpe/textext chown 50001:50000 /data/persistentvolumes/cpe/icmrules chown 50001:50000 /data/persistentvolumes/cpe/asa mkdir -p /data/persistentvolumes/css/CSS_Server/data mkdir -p /data/persistentvolumes/css/CSS_Server/temp mkdir -p /data/persistentvolumes/css/CSS_Server/log mkdir -p /data/persistentvolumes/css/CSS_Server/config mkdir -p /data/persistentvolumes/css/indexareas chown 50001:50000 /data/persistentvolumes/css/CSS_Server/data chown 50001:50000 /data/persistentvolumes/css/CSS_Server/temp chown 50001:50000 /data/persistentvolumes/css/CSS_Server/log chown 50001:50000 /data/persistentvolumes/css/CSS_Server/config chown 50001:50000 /data/persistentvolumes/css/indexareas mkdir -p /data/persistentvolumes/cmis/configDropins/overrides mkdir -p /data/persistentvolumes/cmis/logs chown 50001:50000 /data/persistentvolumes/cmis/configDropins/overrides chown 50001:50000 /data/persistentvolumes/cmis/logs Copy DB2 driver and ECM configuration files into the PVs: cp /opt/ibm/db2/V11.1/java/db2jcc4.jar /data/persistentvolumes/cpe/configDropins/overrides/ cp /opt/ibm/db2/V11.1/java/db2jcc_license_cu.jar /data/persistentvolumes/cpe/configDropins/overrides/ cp cpe/configDropins/overrides/DB2JCCDriver.xml /data/persistentvolumes/cpe/configDropins/overrides/ cp cpe/configDropins/overrides/GCD.xml /data/persistentvolumes/cpe/configDropins/overrides/ cp cpe/configDropins/overrides/ldap_TDS.xml /data/persistentvolumes/cpe/configDropins/overrides/ cp cpe/configDropins/overrides/OBJSTORE.xml /data/persistentvolumes/cpe/configDropins/overrides/ cp cmis/configDropins/overrides/ldap_TDS.xml /data/persistentvolumes/cmis/configDropins/overrides/ cp cmis/configDropins/overrides/ldap_TDS.xml /data/persistentvolumes/cmis/configDropins/overrides/ cp css/cssSelfsignedServerStore /data/persistentvolumes/css/CSS_Server/data Create the namespace ecmproject : oc new-project ecmproject oc project ecmproject ## Grant Tiller server edit access to current project oc adm policy add-role-to-user edit system:serviceaccount:tiller:tiller Configure SCC: oc adm policy add-scc-to-user privileged -z default Login to Docker and push the CM images: docker login -u $(oc whoami) -p $(oc whoami -t) docker-registry.default.svc:5000 wget https://raw.githubusercontent.com/icp4a/cert-kubernetes/19.0.1/scripts/loadimages.sh chmod +x loadimages.sh ./loadimages.sh -p /data/downloads/icp4a/ICP4A19.0.1-ecm.tgz -r docker-registry.default.svc:5000/ecmproject Configure Docker secret: kubectl create secret docker-registry admin.registrykey --docker-server=docker-registry.default.svc:5000 --docker-username=$(oc whoami) --docker-password=$(oc whoami -t) -n ecmproject Create PV: oc apply -f cpe-pv.yaml oc apply -f css-pv.yaml oc apply -f cmis-pv.yaml Install Helm charts: helm install ibm-dba-contentservices-3.0.0.tgz --name cpe-prod-release --namespace ecmproject -f cpe-values.yaml helm install ibm-dba-contentsearch-3.0.0.tgz --name css-prod-release --namespace ecmproject -f css-values.yaml helm install ibm-dba-cscmis-1.7.0.tgz --name cmis-prod-release --namespace ecmproject -f cmis-values.yaml Expose Content Platform Engine service: oc apply -f cpe-route.yaml Complete the post-deployment task Content Navigator Create ICNDB: Download the DB scripts from https://github.com/ibm-ecm/container-navigator and put the scripts and sql into the folder /data/database/config : chown db2inst1:db2iadm1 /data/database/config chown db2inst1:db2iadm1 /data/database/config/*.sh chown db2inst1:db2iadm1 /data/database/config/*.sql chmod 755 /data/database/config/*.sh su - db2inst1 cd /data/database/config ./createICNDB.sh -n ICNDB -s ICNSCHEMA -t ICNTS -u db2inst1 -a ceadmin Create PVs in NFS server: mkdir -p /data/persistentvolumes/icn/configDropins/overrides mkdir -p /data/persistentvolumes/icn/logs mkdir -p /data/persistentvolumes/icn/plugins mkdir -p /data/persistentvolumes/icn/viewerlog mkdir -p /data/persistentvolumes/icn/viewercache mkdir -p /data/persistentvolumes/icn/aspera chown 50001:50000 /data/persistentvolumes/icn/configDropins/overrides chown 50001:50000 /data/persistentvolumes/icn/logs chown 50001:50000 /data/persistentvolumes/icn/plugins chown 50001:50000 /data/persistentvolumes/icn/viewerlog chown 50001:50000 /data/persistentvolumes/icn/viewercache chown 50001:50000 /data/persistentvolumes/icn/aspera Copy DB2 drivers and configuration files into PVs: cp /opt/ibm/db2/V11.1/java/db2jcc4.jar /data/persistentvolumes/icn/configDropins/overrides/ cp /opt/ibm/db2/V11.1/java/db2jcc_license_cu.jar /data/persistentvolumes/icn/configDropins/overrides/ cp configDropins/overrides/DB2JCCDriver.xml /data/persistentvolumes/icn/configDropins/overrides/ cp configDropins/overrides/ICNDS.xml /data/persistentvolumes/icn/configDropins/overrides/ cp configDropins/overrides/ldap_TDS.xml /data/persistentvolumes/icn/configDropins/overrides/ Create the namespace fncnproject oc new-project fncnproject oc project fncnproject ## Grant Tiller server edit access to current project oc adm policy add-role-to-user edit system:serviceaccount:tiller:tiller Configure SCC: oc adm policy add-scc-to-user privileged -z default Login to Docker and push FNCN Images: docker login -u $(oc whoami) -p $(oc whoami -t) docker-registry.default.svc:5000 wget https://raw.githubusercontent.com/icp4a/cert-kubernetes/19.0.1/scripts/loadimages.sh chmod +x loadimages.sh ./loadimages.sh -p /data/downloads/icp4a/ICP4A19.0.1-fncn.tgz -r docker-registry.default.svc:5000/fncnproject Configure Docker secret: kubectl create secret docker-registry admin.registrykey --docker-server=docker-registry.default.svc:5000 --docker-username=$(oc whoami) --docker-password=$(oc whoami -t) -n fncnproject Create PV: oc apply -f pv.yaml Download and install the Helm chart: wget https://github.com/icp4a/cert-kubernetes/raw/master/NAVIGATOR/helm-charts/ibm-dba-navigator-3.0.0.tgz helm install ibm-dba-navigator-3.0.0.tgz --name navigator-prod-release --namespace fncnproject -f values.yaml Expose ICN console service: oc apply -f route.yaml Complete the post-deployment tasks","title":"Installing CM"},{"location":"environment/rhos-install-cm/#content-engine","text":"Create GCD and Object Store databases: mkdir -p /data/db2fs mkdir -p /data/database/config ## copy the GCDDB.sh and OS1DB.sh into folder /data/database/config export CUR_COMMIT=ON su - db2inst1 -c db2set DB2_WORKLOAD=FILENET_CM echo set CUR_COMMIT=$CUR_COMMIT chown db2inst1:db2iadm1 /data/database/config/*.sh chmod 755 /data/database/config/*.sh chown -R db2inst1:db2iadm1 /data/db2fs su - db2inst1 -c /data/database/config/GCDDB.sh GCDDB su - db2inst1 -c /data/database/config/OS1DB.sh OS1DB Create PVs in NFS server: mkdir -p /data/persistentvolumes/cpe/configDropins/overrides mkdir -p /data/persistentvolumes/cpe/logs mkdir -p /data/persistentvolumes/cpe/cpefnlogstore mkdir -p /data/persistentvolumes/cpe/bootstrap mkdir -p /data/persistentvolumes/cpe/textext mkdir -p /data/persistentvolumes/cpe/icmrules mkdir -p /data/persistentvolumes/cpe/asa chown 50001:50000 /data/persistentvolumes/cpe/configDropins chown 50001:50000 /data/persistentvolumes/cpe/configDropins/overrides chown 50001:50000 /data/persistentvolumes/cpe/logs chown 50001:50000 /data/persistentvolumes/cpe/cpefnlogstore chown 50001:50000 /data/persistentvolumes/cpe/bootstrap chown 50001:50000 /data/persistentvolumes/cpe/textext chown 50001:50000 /data/persistentvolumes/cpe/icmrules chown 50001:50000 /data/persistentvolumes/cpe/asa mkdir -p /data/persistentvolumes/css/CSS_Server/data mkdir -p /data/persistentvolumes/css/CSS_Server/temp mkdir -p /data/persistentvolumes/css/CSS_Server/log mkdir -p /data/persistentvolumes/css/CSS_Server/config mkdir -p /data/persistentvolumes/css/indexareas chown 50001:50000 /data/persistentvolumes/css/CSS_Server/data chown 50001:50000 /data/persistentvolumes/css/CSS_Server/temp chown 50001:50000 /data/persistentvolumes/css/CSS_Server/log chown 50001:50000 /data/persistentvolumes/css/CSS_Server/config chown 50001:50000 /data/persistentvolumes/css/indexareas mkdir -p /data/persistentvolumes/cmis/configDropins/overrides mkdir -p /data/persistentvolumes/cmis/logs chown 50001:50000 /data/persistentvolumes/cmis/configDropins/overrides chown 50001:50000 /data/persistentvolumes/cmis/logs Copy DB2 driver and ECM configuration files into the PVs: cp /opt/ibm/db2/V11.1/java/db2jcc4.jar /data/persistentvolumes/cpe/configDropins/overrides/ cp /opt/ibm/db2/V11.1/java/db2jcc_license_cu.jar /data/persistentvolumes/cpe/configDropins/overrides/ cp cpe/configDropins/overrides/DB2JCCDriver.xml /data/persistentvolumes/cpe/configDropins/overrides/ cp cpe/configDropins/overrides/GCD.xml /data/persistentvolumes/cpe/configDropins/overrides/ cp cpe/configDropins/overrides/ldap_TDS.xml /data/persistentvolumes/cpe/configDropins/overrides/ cp cpe/configDropins/overrides/OBJSTORE.xml /data/persistentvolumes/cpe/configDropins/overrides/ cp cmis/configDropins/overrides/ldap_TDS.xml /data/persistentvolumes/cmis/configDropins/overrides/ cp cmis/configDropins/overrides/ldap_TDS.xml /data/persistentvolumes/cmis/configDropins/overrides/ cp css/cssSelfsignedServerStore /data/persistentvolumes/css/CSS_Server/data Create the namespace ecmproject : oc new-project ecmproject oc project ecmproject ## Grant Tiller server edit access to current project oc adm policy add-role-to-user edit system:serviceaccount:tiller:tiller Configure SCC: oc adm policy add-scc-to-user privileged -z default Login to Docker and push the CM images: docker login -u $(oc whoami) -p $(oc whoami -t) docker-registry.default.svc:5000 wget https://raw.githubusercontent.com/icp4a/cert-kubernetes/19.0.1/scripts/loadimages.sh chmod +x loadimages.sh ./loadimages.sh -p /data/downloads/icp4a/ICP4A19.0.1-ecm.tgz -r docker-registry.default.svc:5000/ecmproject Configure Docker secret: kubectl create secret docker-registry admin.registrykey --docker-server=docker-registry.default.svc:5000 --docker-username=$(oc whoami) --docker-password=$(oc whoami -t) -n ecmproject Create PV: oc apply -f cpe-pv.yaml oc apply -f css-pv.yaml oc apply -f cmis-pv.yaml Install Helm charts: helm install ibm-dba-contentservices-3.0.0.tgz --name cpe-prod-release --namespace ecmproject -f cpe-values.yaml helm install ibm-dba-contentsearch-3.0.0.tgz --name css-prod-release --namespace ecmproject -f css-values.yaml helm install ibm-dba-cscmis-1.7.0.tgz --name cmis-prod-release --namespace ecmproject -f cmis-values.yaml Expose Content Platform Engine service: oc apply -f cpe-route.yaml Complete the post-deployment task","title":"Content Engine"},{"location":"environment/rhos-install-cm/#content-navigator","text":"Create ICNDB: Download the DB scripts from https://github.com/ibm-ecm/container-navigator and put the scripts and sql into the folder /data/database/config : chown db2inst1:db2iadm1 /data/database/config chown db2inst1:db2iadm1 /data/database/config/*.sh chown db2inst1:db2iadm1 /data/database/config/*.sql chmod 755 /data/database/config/*.sh su - db2inst1 cd /data/database/config ./createICNDB.sh -n ICNDB -s ICNSCHEMA -t ICNTS -u db2inst1 -a ceadmin Create PVs in NFS server: mkdir -p /data/persistentvolumes/icn/configDropins/overrides mkdir -p /data/persistentvolumes/icn/logs mkdir -p /data/persistentvolumes/icn/plugins mkdir -p /data/persistentvolumes/icn/viewerlog mkdir -p /data/persistentvolumes/icn/viewercache mkdir -p /data/persistentvolumes/icn/aspera chown 50001:50000 /data/persistentvolumes/icn/configDropins/overrides chown 50001:50000 /data/persistentvolumes/icn/logs chown 50001:50000 /data/persistentvolumes/icn/plugins chown 50001:50000 /data/persistentvolumes/icn/viewerlog chown 50001:50000 /data/persistentvolumes/icn/viewercache chown 50001:50000 /data/persistentvolumes/icn/aspera Copy DB2 drivers and configuration files into PVs: cp /opt/ibm/db2/V11.1/java/db2jcc4.jar /data/persistentvolumes/icn/configDropins/overrides/ cp /opt/ibm/db2/V11.1/java/db2jcc_license_cu.jar /data/persistentvolumes/icn/configDropins/overrides/ cp configDropins/overrides/DB2JCCDriver.xml /data/persistentvolumes/icn/configDropins/overrides/ cp configDropins/overrides/ICNDS.xml /data/persistentvolumes/icn/configDropins/overrides/ cp configDropins/overrides/ldap_TDS.xml /data/persistentvolumes/icn/configDropins/overrides/ Create the namespace fncnproject oc new-project fncnproject oc project fncnproject ## Grant Tiller server edit access to current project oc adm policy add-role-to-user edit system:serviceaccount:tiller:tiller Configure SCC: oc adm policy add-scc-to-user privileged -z default Login to Docker and push FNCN Images: docker login -u $(oc whoami) -p $(oc whoami -t) docker-registry.default.svc:5000 wget https://raw.githubusercontent.com/icp4a/cert-kubernetes/19.0.1/scripts/loadimages.sh chmod +x loadimages.sh ./loadimages.sh -p /data/downloads/icp4a/ICP4A19.0.1-fncn.tgz -r docker-registry.default.svc:5000/fncnproject Configure Docker secret: kubectl create secret docker-registry admin.registrykey --docker-server=docker-registry.default.svc:5000 --docker-username=$(oc whoami) --docker-password=$(oc whoami -t) -n fncnproject Create PV: oc apply -f pv.yaml Download and install the Helm chart: wget https://github.com/icp4a/cert-kubernetes/raw/master/NAVIGATOR/helm-charts/ibm-dba-navigator-3.0.0.tgz helm install ibm-dba-navigator-3.0.0.tgz --name navigator-prod-release --namespace fncnproject -f values.yaml Expose ICN console service: oc apply -f route.yaml Complete the post-deployment tasks","title":"Content Navigator"},{"location":"environment/rhos-install-odm/","text":"Create namespace odmproject : oc new-project odmproject oc project odmproject ## Grant the tiller server edit access to current project oc adm policy add-role-to-user edit system:serviceaccount:tiller:tiller Update SCC user: oc adm policy add-scc-to-user privileged -z default Login to Docker and push the ODM images: docker login -u $(oc whoami) -p $(oc whoami -t) docker-registry.default.svc:5000 wget https://raw.githubusercontent.com/icp4a/cert-kubernetes/19.0.1/scripts/loadimages.sh chmod +x loadimages.sh ./loadimages.sh -p /data/downloads/icp4a/ICP4A19.0.1-odm.tgz -r docker-registry.default.svc:5000/odmproject Create LDAP secret: oc create secret generic odm-prod-release-odm-ldap --from-file=ldap-configurations.xml=ldap-configurations.xml --from-file=webSecurity.xml=webSecurity.xml --type=Opaque Create BAI Event secret: kubectl create secret generic odm-prod-release-odm-bai-event --from-file=plugin-configuration.properties Download and install the Helm chart: wget https://github.com/icp4a/cert-kubernetes/raw/19.0.1/ODM/helm-charts/ibm-odm-prod-2.2.0.tgz helm install ibm-odm-prod-2.2.0.tgz --name odm-prod-release --namespace odmproject -f values.yaml Expose ODM services: oc create -f route.yaml","title":"Installing ODM"},{"location":"environment/rhos-intro/","text":"This chapter describes the steps required to deploy the Cloud Pak for Automation on a managed OpenShift cluster on IBM Cloud. In the next sections, we detail how to: Provision a managed OpenShift (v3.11 beta) cluster on IBM Cloud. Install the individual IBM Cloud Pak for Automation 19.0.1 components (BAW, ODM, BAI, ...) on the cluster.","title":"Introduction"},{"location":"showtime/deploy-solution/","text":"Deploying the solution This chapter shows you how to deploy the different components artifacts of the Denim Compute solution to the ICP cluster where your DBA components are deployed. Scripted deployment Under the {denim_compute_repo}/solution/scripts directory, you can find Ansible scripts that automate the deployment of the different solution artifacts (BAW solution, ODM decision service, BAI dashboards. To deploy the decision service, use: ansible-playbook main.yaml -i inventory --tags \"deploy_ds\" To load the BPM/Case application, use: ansible-playbook main.yaml -i inventory --tags \"load_case\" To import the custom BAI dashboards, use: ansible-playbook main.yaml -i inventory --tags \"import_dashboard\"","title":"Deploy"},{"location":"showtime/deploy-solution/#deploying-the-solution","text":"This chapter shows you how to deploy the different components artifacts of the Denim Compute solution to the ICP cluster where your DBA components are deployed.","title":"Deploying the solution"},{"location":"showtime/deploy-solution/#scripted-deployment","text":"Under the {denim_compute_repo}/solution/scripts directory, you can find Ansible scripts that automate the deployment of the different solution artifacts (BAW solution, ODM decision service, BAI dashboards. To deploy the decision service, use: ansible-playbook main.yaml -i inventory --tags \"deploy_ds\" To load the BPM/Case application, use: ansible-playbook main.yaml -i inventory --tags \"load_case\" To import the custom BAI dashboards, use: ansible-playbook main.yaml -i inventory --tags \"import_dashboard\"","title":"Scripted deployment"},{"location":"showtime/run-solution/","text":"Running the solution Deploy the case solution in Workflow Center Login to Workflow Center and open the Case Solution named Denim Compute Auto Claims : Click the deploy icon in the upper-right corner of the Case Builder: Create Case security configuration In the Workflow Center, select the solution and click the contextual menu and then Advanced . With the Denim Compute solution selected click Actions , then Manage and then Security Configuration . Select the option to Create a security configuration and click Next . Provide a Security manifest name (e.g. denim_security_config ) and click Next . Set permissions against the security roles (in this example all permissions are assigned to each role) and click Next . In this step, you set the administrators. You can add administrators besides the pre-defined one if you want, by clicking the Add button, or you can just click Next . In the Add Users and Groups modal dialogue you can add users or groups. You start typing partial names and then click the magnifying glass icon to find matching users that appear in the Available section. You can then use the arrows to move between Available and Selected and when done you click Add to complete. When done with this section click Next to continue. Next you map groups and users to the Case roles. Select each role and click Add then follow the earlier instructions for how to find and assign users and groups. In this example, we have just added a default user to the Claim Intake Services Role . You repeat these steps for the other Roles . Here is the final situation, where example users are added to each role (if you want a more realistic scenario you should setup different users and groups and assign them to the roles). Click Next when done with this section. Now you can check the box next to Apply the security configuration and click Apply . You should get confirmation that the security configuration was successfully applied (you can then Close this dialog). Note if you ever need to review or change the security configuration settings you can launch it again and choose the Edit a security configuration option as shown here. Create BPM user groups The solution has a number of BPM teams defined that need to be mapped to users and groups. To do that, launch the Process Admin Console and then select Server Admin section. Open User Management , select Group Management and type denim in Select Group to Modify and you should see the groups that have been created as a result of the team definitions. You then need to assign users and groups for your environment against those pre-defined groups. Here is an example where we have assigned a number of users to the denim-adjusters group. Configure servers The solution integrates with the ODM and ECM components by using defined servers. By default, these are mapped to the environment that we used for testing purpose. Obviously, you have to re-map these servers to your own cluster environment. To do this, use the following instructions: First, ensure that the deployed BAW solution is activated. Select it in the Workflow Center and click the View Details icon in the lower left corner of the tile. Next choose the Snapshots section, select your snapshot (NB: the latest available snapshot is now v0.10.0) and from the contextual menu you Activate it. Note the next steps reference environment variables which require information from the deployment of the micro-service so if you have not completed that yet then go to that section and return here to complete the BAW servers configuration. After this you can now go to the Process Admin Console and you should see the snapshot in the list of Installed Apps . You click on the snapshot and then select the Servers section and you should see the two server definitions used ( DenimODMServer for the referenced ODM Sever and IBMBAW for the referenced ECM server). You should change the settings for the respective configured servers to match how you installed your environment (the Hostname , Port (if non standard) and user credentials (where relevant) need to be configured). You should provide entries for the highlighted environment variables (note for BACA you get these values from the API section of the administration console, bacaAuth needs to be Base64 encoded of username:password). Configure BACA ontology Currently Business Automation Content Analyzer (BACA) only allows for the import and export of an entire ontology, there is no merge capability of selective import. Therefore we recommend that you backup any existing ontology before proceeding. First use the Export Ontology option as shown and save your existing ontology. Then use Import Ontology and select the dc-baca-ontology.json file. You should now have an ontology similar to that shown below: When you are finished with trying out the BACA scenario you can re-import the saved JSON export of your original ontology. Deploy the BACA Mediator micro-service NB: The following instructions are for an OCP 3.11 cluster. Configure GitHub repository We recommend using a secure private repository rather than a GitHub Public one and these instructions assume that is the case. The start point is that you should fork this repository so that you can then configure your own security settings. Generate an SSH Key pair and ensure to specify the flag for no passphrase as shown below. In your target GitHub repository go to Settings then Deploy keys and click Add deploy key . Provide a meaningful title that shows what the key is for (in our case to allow source code deploy in OCP) and then upload the public key that was generated earlier and then click Add key . You should now have something like that shown below for our repository. Create Project in OCP From the OCP web console select the Create Project option. Provide a name (we suggest something like baca-mediate-app ) and Create . Create Secret in OCP We need to create a secret to reference the SSH Key generated earlier for interacting with the source repository in GitHub . In the newly created Project, select Resources , then Secrets and click Create Secret . Select Source Secret , enter the name dc-gh-baca-mediator-secret , select SSH Key and click **Browse..\" and then find the SSH Private Key generated earlier. Check the option to link the secret and choose builder as the target Service Account , then click Create to finish. Deploy using Source to Image (S2I) There are a number of options for deploying to OCP, we are going to use Source to Image which directly builds from a source code repository such as GitHub . Click Add to Project and select the Browse Catalog option. Find and select the Node.js option which launches a dialog as shown here. Supply an application name and provide the SSH url for the target GitHub repository. Then it is important to click the advanced options link as we need to configure some non-standard items as this is a secure repository. Find the Source Secret section and select the earlier Secret that you created. There are many other options available but we want to go with the defaults so scroll down and click the Create button to complete. A number of Kubernetes Resources will now be created including a Build Config and Build that will pull the source and build a deployment. When done on the overview section you should see summary information for the Deployment Config including that it has 1 Pod (note in a realistic environment we would set this to have 1 Pod for failover but it makes it easier for us to see logs with this 1 Pod running without having to configure an ELK stack) and it is running. Take note also of the Route highlighted which is the public ingress point to the micro-service and this is needed for configuring the environment variable in the BAW config section above. Expand the twisty and you see more information on the Deployment Config including links to other resources which you can click on and explore. For now we want to verify the Pod is as expected so click on the icon of the Pod (top right). You are taken to the Pod summary and within that there is a Logs tab. In the Logs you can now verify that the micro-service is up and ready to accept requests.","title":"Run"},{"location":"showtime/run-solution/#running-the-solution","text":"","title":"Running the solution"},{"location":"showtime/run-solution/#deploy-the-case-solution-in-workflow-center","text":"Login to Workflow Center and open the Case Solution named Denim Compute Auto Claims : Click the deploy icon in the upper-right corner of the Case Builder:","title":"Deploy the case solution in Workflow Center"},{"location":"showtime/run-solution/#create-case-security-configuration","text":"In the Workflow Center, select the solution and click the contextual menu and then Advanced . With the Denim Compute solution selected click Actions , then Manage and then Security Configuration . Select the option to Create a security configuration and click Next . Provide a Security manifest name (e.g. denim_security_config ) and click Next . Set permissions against the security roles (in this example all permissions are assigned to each role) and click Next . In this step, you set the administrators. You can add administrators besides the pre-defined one if you want, by clicking the Add button, or you can just click Next . In the Add Users and Groups modal dialogue you can add users or groups. You start typing partial names and then click the magnifying glass icon to find matching users that appear in the Available section. You can then use the arrows to move between Available and Selected and when done you click Add to complete. When done with this section click Next to continue. Next you map groups and users to the Case roles. Select each role and click Add then follow the earlier instructions for how to find and assign users and groups. In this example, we have just added a default user to the Claim Intake Services Role . You repeat these steps for the other Roles . Here is the final situation, where example users are added to each role (if you want a more realistic scenario you should setup different users and groups and assign them to the roles). Click Next when done with this section. Now you can check the box next to Apply the security configuration and click Apply . You should get confirmation that the security configuration was successfully applied (you can then Close this dialog). Note if you ever need to review or change the security configuration settings you can launch it again and choose the Edit a security configuration option as shown here.","title":"Create Case security configuration"},{"location":"showtime/run-solution/#create-bpm-user-groups","text":"The solution has a number of BPM teams defined that need to be mapped to users and groups. To do that, launch the Process Admin Console and then select Server Admin section. Open User Management , select Group Management and type denim in Select Group to Modify and you should see the groups that have been created as a result of the team definitions. You then need to assign users and groups for your environment against those pre-defined groups. Here is an example where we have assigned a number of users to the denim-adjusters group.","title":"Create BPM user groups"},{"location":"showtime/run-solution/#configure-servers","text":"The solution integrates with the ODM and ECM components by using defined servers. By default, these are mapped to the environment that we used for testing purpose. Obviously, you have to re-map these servers to your own cluster environment. To do this, use the following instructions: First, ensure that the deployed BAW solution is activated. Select it in the Workflow Center and click the View Details icon in the lower left corner of the tile. Next choose the Snapshots section, select your snapshot (NB: the latest available snapshot is now v0.10.0) and from the contextual menu you Activate it. Note the next steps reference environment variables which require information from the deployment of the micro-service so if you have not completed that yet then go to that section and return here to complete the BAW servers configuration. After this you can now go to the Process Admin Console and you should see the snapshot in the list of Installed Apps . You click on the snapshot and then select the Servers section and you should see the two server definitions used ( DenimODMServer for the referenced ODM Sever and IBMBAW for the referenced ECM server). You should change the settings for the respective configured servers to match how you installed your environment (the Hostname , Port (if non standard) and user credentials (where relevant) need to be configured). You should provide entries for the highlighted environment variables (note for BACA you get these values from the API section of the administration console, bacaAuth needs to be Base64 encoded of username:password).","title":"Configure servers "},{"location":"showtime/run-solution/#configure-baca-ontology","text":"Currently Business Automation Content Analyzer (BACA) only allows for the import and export of an entire ontology, there is no merge capability of selective import. Therefore we recommend that you backup any existing ontology before proceeding. First use the Export Ontology option as shown and save your existing ontology. Then use Import Ontology and select the dc-baca-ontology.json file. You should now have an ontology similar to that shown below: When you are finished with trying out the BACA scenario you can re-import the saved JSON export of your original ontology.","title":"Configure BACA ontology"},{"location":"showtime/run-solution/#deploy-the-baca-mediator-micro-service","text":"NB: The following instructions are for an OCP 3.11 cluster.","title":"Deploy the BACA Mediator micro-service"},{"location":"showtime/run-solution/#configure-github-repository","text":"We recommend using a secure private repository rather than a GitHub Public one and these instructions assume that is the case. The start point is that you should fork this repository so that you can then configure your own security settings. Generate an SSH Key pair and ensure to specify the flag for no passphrase as shown below. In your target GitHub repository go to Settings then Deploy keys and click Add deploy key . Provide a meaningful title that shows what the key is for (in our case to allow source code deploy in OCP) and then upload the public key that was generated earlier and then click Add key . You should now have something like that shown below for our repository.","title":"Configure GitHub repository"},{"location":"showtime/run-solution/#create-project-in-ocp","text":"From the OCP web console select the Create Project option. Provide a name (we suggest something like baca-mediate-app ) and Create .","title":"Create Project in OCP"},{"location":"showtime/run-solution/#create-secret-in-ocp","text":"We need to create a secret to reference the SSH Key generated earlier for interacting with the source repository in GitHub . In the newly created Project, select Resources , then Secrets and click Create Secret . Select Source Secret , enter the name dc-gh-baca-mediator-secret , select SSH Key and click **Browse..\" and then find the SSH Private Key generated earlier. Check the option to link the secret and choose builder as the target Service Account , then click Create to finish.","title":"Create Secret in OCP"},{"location":"showtime/run-solution/#deploy-using-source-to-image-s2i","text":"There are a number of options for deploying to OCP, we are going to use Source to Image which directly builds from a source code repository such as GitHub . Click Add to Project and select the Browse Catalog option. Find and select the Node.js option which launches a dialog as shown here. Supply an application name and provide the SSH url for the target GitHub repository. Then it is important to click the advanced options link as we need to configure some non-standard items as this is a secure repository. Find the Source Secret section and select the earlier Secret that you created. There are many other options available but we want to go with the defaults so scroll down and click the Create button to complete. A number of Kubernetes Resources will now be created including a Build Config and Build that will pull the source and build a deployment. When done on the overview section you should see summary information for the Deployment Config including that it has 1 Pod (note in a realistic environment we would set this to have 1 Pod for failover but it makes it easier for us to see logs with this 1 Pod running without having to configure an ELK stack) and it is running. Take note also of the Route highlighted which is the public ingress point to the micro-service and this is needed for configuring the environment variable in the BAW config section above. Expand the twisty and you see more information on the Deployment Config including links to other resources which you can click on and explore. For now we want to verify the Pod is as expected so click on the icon of the Pod (top right). You are taken to the Pod summary and within that there is a Logs tab. In the Logs you can now verify that the micro-service is up and ready to accept requests.","title":"Deploy using Source to Image (S2I)"},{"location":"usecase/baca-scenario-walkthrough/","text":"BACA scenario walkthrough This scenario builds on the main scenario . It follows an alternate path whereby, instead of a Repairer interacting with the Process Portal to submit a repair estimate, they send it in a PDF document which is then detected and sent to Business Automation Content Analyzer (BACA) for processing. BACA identifies relevant content based on a modeled ontology. Business Automation Workflow (BAW) interacts with BACA in this scenario via an intermediary micro-service (implemented in Node.js) deployed on the OCP cluster. Scenario installation You should have: Installed the Cloud Pak for Automation as instructed in the various installation guidance documents in the OpenShift environment section. Followed the instructions in Showtime Deploy and Showtime Run to deploy and configure the BAW, BACA and Node.js micro-service solutions. Downloaded the two PDF documents from this directory . Scenario starting point As this scenario builds on the main scenario , it is recommended that a reader familiarises themselves with that first. In that document repair estimates are solicited from 4 repair shops (page 36) and we see work items in the BAW Process Portal. We are going to show that one of those repair shops (highlighted below) will in parallel send a PDF that will be automatically processed by BACA, and if successful, then the work item in Process Portal will be canceled. As the scenario will interact with a Node.js micro-service deployed on the OCP cluster, we first look at the baca-mediator-app project that uses a deployment config named rhocp-baca-mediator . The deployment config references a Kubernetes pod shown below: Selecting the pod and then the Logs section shows the output of the container, and in this case shows that the micro-service is initialized and awaiting requests. Upload invalid estimate PDF We are going to demonstrate first with a deliberately malformed PDF of an estimate, which does not follow agreed conventions and will not be able to result in it being parsed into a valid estimate. In the BAW Case Client, a Case worker searches on the Insured LName and finds the relevant in-progress Case instance (in this example there are several for a policy holder named Simmons ). They then click on it to open the Case Summary page. They next go to the Documents tab and select the target folder Repair Estimates and Invoices as shown below. This takes them to the target folder where they will upload a received PDF (we can imagine they got this via email, note it is out of scope for our scenario but there are automatic capabilities in BAW ECM to configure event handlers to automatically detect documents and move them to a defined folder location). The Case worker will now upload using the Add Document from Local System option within the target folder as shown here. In the document upload dialog, the worker chooses the Auto Repair Estimate Document Class which then displays a list of metadata properties (please ignore the Claim Number option - this is due to an issue in the test environment were it retained an older reference from an earlier version of the Document Class definition). They browse and select the file named error-estimate-example1.pdf and ensure they choose the matching Vehicle VIN and Repairer Code . When happy with the selections, clicking Add will complete the upload. At this point, we want to examine how BAW detects the uploaded document for the defined Auto Repair Estimate Document Class and starts processing it. In the Process Admin Console, we go to the Process Inspector where we can see that an instance of Process Repair Estimate has been started. That process in turn invokes the micro-service and we can see the log outputs of that interaction back in OCP for the pod, as shown below. If you examine that output closely you will see that several of the debugging log outputs show that some data has invalid values. If we now access the BAW Process Portal, we can see that a new work item has been created named Display Vehicle Estimate Errors . In the process it detects when it cannot process the uploaded PDF correctly to arrive at a valid estimate and so creates this work item to report on it. The work item shows a simple error message to highlight what went wrong and displays the PDF that was attempted to be processed. You can see from that PDF that it has some invalid data. Scrolling further down the document you can see that the table representing vehicle parts has been badly formatted and has inconsistent data that cannot be interpreted into a meaningful set of data that would constitute a valid estimate. The user can now click Complete to dismiss this report and we will next show the scenario when a valid estimate is uploaded. Upload valid estimate PDF Having seen the principles behind the scenario, we can now look at the \"happy path\". This time the case worker selects the valid document apcars-estimate-example1.pdf to add to the target folder ( Repair Estimates and Invoices ). In Process Inspector, we can observe that a second instance of Process Repair Estimate has executed and this time it has completed. We can also see highlighted the event data that has been extracted from the document by the combination of the mediator micro-service and BACA. This is then used to send an interrupting event to the instance of the process that is awaiting the completion of the earlier work item for this specific repairer. (Note you can also look again at the OCP logs for the pod and see the fresh debugging output for this newly uploaded PDF). And finally, to verify that this has happened, in BAW Process Portal, we can see that the list of work items refreshes and the one for Athelstan Prestige Cars is no longer there. At this point, the main scenario would continue from page 44. So now we have seen the interaction between BAW and BACA via a mediator micro-service deployed on OCP.","title":"BACA scenario"},{"location":"usecase/baca-scenario-walkthrough/#baca-scenario-walkthrough","text":"This scenario builds on the main scenario . It follows an alternate path whereby, instead of a Repairer interacting with the Process Portal to submit a repair estimate, they send it in a PDF document which is then detected and sent to Business Automation Content Analyzer (BACA) for processing. BACA identifies relevant content based on a modeled ontology. Business Automation Workflow (BAW) interacts with BACA in this scenario via an intermediary micro-service (implemented in Node.js) deployed on the OCP cluster.","title":"BACA scenario walkthrough"},{"location":"usecase/baca-scenario-walkthrough/#scenario-installation","text":"You should have: Installed the Cloud Pak for Automation as instructed in the various installation guidance documents in the OpenShift environment section. Followed the instructions in Showtime Deploy and Showtime Run to deploy and configure the BAW, BACA and Node.js micro-service solutions. Downloaded the two PDF documents from this directory .","title":"Scenario installation"},{"location":"usecase/baca-scenario-walkthrough/#scenario-starting-point","text":"As this scenario builds on the main scenario , it is recommended that a reader familiarises themselves with that first. In that document repair estimates are solicited from 4 repair shops (page 36) and we see work items in the BAW Process Portal. We are going to show that one of those repair shops (highlighted below) will in parallel send a PDF that will be automatically processed by BACA, and if successful, then the work item in Process Portal will be canceled. As the scenario will interact with a Node.js micro-service deployed on the OCP cluster, we first look at the baca-mediator-app project that uses a deployment config named rhocp-baca-mediator . The deployment config references a Kubernetes pod shown below: Selecting the pod and then the Logs section shows the output of the container, and in this case shows that the micro-service is initialized and awaiting requests.","title":"Scenario starting point"},{"location":"usecase/baca-scenario-walkthrough/#upload-invalid-estimate-pdf","text":"We are going to demonstrate first with a deliberately malformed PDF of an estimate, which does not follow agreed conventions and will not be able to result in it being parsed into a valid estimate. In the BAW Case Client, a Case worker searches on the Insured LName and finds the relevant in-progress Case instance (in this example there are several for a policy holder named Simmons ). They then click on it to open the Case Summary page. They next go to the Documents tab and select the target folder Repair Estimates and Invoices as shown below. This takes them to the target folder where they will upload a received PDF (we can imagine they got this via email, note it is out of scope for our scenario but there are automatic capabilities in BAW ECM to configure event handlers to automatically detect documents and move them to a defined folder location). The Case worker will now upload using the Add Document from Local System option within the target folder as shown here. In the document upload dialog, the worker chooses the Auto Repair Estimate Document Class which then displays a list of metadata properties (please ignore the Claim Number option - this is due to an issue in the test environment were it retained an older reference from an earlier version of the Document Class definition). They browse and select the file named error-estimate-example1.pdf and ensure they choose the matching Vehicle VIN and Repairer Code . When happy with the selections, clicking Add will complete the upload. At this point, we want to examine how BAW detects the uploaded document for the defined Auto Repair Estimate Document Class and starts processing it. In the Process Admin Console, we go to the Process Inspector where we can see that an instance of Process Repair Estimate has been started. That process in turn invokes the micro-service and we can see the log outputs of that interaction back in OCP for the pod, as shown below. If you examine that output closely you will see that several of the debugging log outputs show that some data has invalid values. If we now access the BAW Process Portal, we can see that a new work item has been created named Display Vehicle Estimate Errors . In the process it detects when it cannot process the uploaded PDF correctly to arrive at a valid estimate and so creates this work item to report on it. The work item shows a simple error message to highlight what went wrong and displays the PDF that was attempted to be processed. You can see from that PDF that it has some invalid data. Scrolling further down the document you can see that the table representing vehicle parts has been badly formatted and has inconsistent data that cannot be interpreted into a meaningful set of data that would constitute a valid estimate. The user can now click Complete to dismiss this report and we will next show the scenario when a valid estimate is uploaded.","title":"Upload invalid estimate PDF"},{"location":"usecase/baca-scenario-walkthrough/#upload-valid-estimate-pdf","text":"Having seen the principles behind the scenario, we can now look at the \"happy path\". This time the case worker selects the valid document apcars-estimate-example1.pdf to add to the target folder ( Repair Estimates and Invoices ). In Process Inspector, we can observe that a second instance of Process Repair Estimate has executed and this time it has completed. We can also see highlighted the event data that has been extracted from the document by the combination of the mediator micro-service and BACA. This is then used to send an interrupting event to the instance of the process that is awaiting the completion of the earlier work item for this specific repairer. (Note you can also look again at the OCP logs for the pod and see the fresh debugging output for this newly uploaded PDF). And finally, to verify that this has happened, in BAW Process Portal, we can see that the list of work items refreshes and the one for Athelstan Prestige Cars is no longer there. At this point, the main scenario would continue from page 44. So now we have seen the interaction between BAW and BACA via a mediator micro-service deployed on OCP.","title":"Upload valid estimate PDF"},{"location":"usecase/bai-scenario-walkthrough/","text":"BAI scenario walkthrough Before proceeding with this scenario it is recommended you familiarize yourself with the main scenario . That scenario shows the interactions for a \"day in the life\" of a Case instance of Auto Claims and while that sends data through for visualizing in Business Automation Insights (BAI), to fully appreciate the power of BAI you need to generate some realistic loads. To that end, we have created a separate scenario that emulates the main scenario but without any real human interactions, so that it can execute many concurrent instances in batch and you can appreciate the value of BAI in a short amount of time. This emulation generates random data based on some configuration settings that are exposed for the end user to control as we shall see later in this document. If you want to follow along with this scenario on your own environment, see the instructions for installing in the Scenario installation section later in this document. Business Automation Workflow The starting point is the Process Portal where any user can launch the Denim Compute Auto Claims - Emulation Generator workflow after which a work item Set Emulation Controls will appear to be actioned. The user interface coach is displayed allowing for configuring options to control the spread of emulated instances and how they then affect the business insights data that will be later viewed in BAI. We will explain these settings a little later. If this is being run for the first time in a IBM Cloud Pak for Automation environment it is a good idea to first verify the process behaves as expected without creating a lot of workflow instances. To do this set the Number of Claims to something like 300 instances (this will exercise the emulation's batch control which submits workflow instances in minimum batches of 100) and then choose the Test Emulation button option. This will exercise a path through the workflow that does everything except for instantiating the instances (the logic has a feature flag switch that treats it as in test mode and skips the message event which will spawn workflow instances). To verify the test you access the workflow instance in Process Inspector within the Process Admin Console as shown below. As the test emulation progresses you will see data variables set including the batch of emulated Auto Claim business objects (one instance is expanded in the list below for illustration). The emulation workflow instances may take a few minutes to complete depending on the total number of claims that were specified as the logic organizes the data into batches with a timed delay between them. When the emulation workflow instance has completed (which can be verified in Process Inspector) and we are satisfied it behaves as expected we can then relaunch the workflow and this time submit a real emulation run. Here in this first section we have set Number of Claims to 500 and the Maximum Claim Amount to 50000. (Please be aware that the more instances that are created here the more work for BAW operations team to clean up afterwards and purge the completed instances. Also the more instances requested the higher the relative load on the system so we recommend you exercise due care here and not unduly load a system that might have other business-critical workloads on it. If in doubt check with your operations team). The next set of settings control the spread of paths through the Auto Claims workflow. Simple scenarios do not have any adjustment, fraud checking etc and so we set a relatively low figure for this. Next the Percentage in Progress setting controls the spread of items that have emulated delays in processing (so that later, in BAI Dashboards, you can see interactively how active instances behave and the changes that occur as they move through their lifecycles). Percentage Potential Fraud emulates the path in the workflow whereby the call out to ODM rules determines whether there is a potential fraud situation that needs investigating. The Percentage Confirmed Fraud setting then controls what percentage of those potential frauds have been determined to be actual frauds (the rest will end up as being cleared of fraud). The final section controls the time delays for each part of the workflow that is not straight-through processing (STP). Here in the settings 30 minutes delay is set for each so it will configure variable times up to that maximum, this should mean that all 500 instances will have completed in approximately 2 hours elapsed time. We can visualize this in the BAW emulation workflow itself where the items color-coded green are STP and those in orange are the human interaction steps which matches how the main scenario behaves. Once satisfied with the settings the request is submitted by clicking the Run Emulation button option. After giving it a few minutes you should start to see the instances getting created and registering in Process Inspector, as shown here. Business Automation Insights When new tracking data fields are first encountered in BAI (in Elasticsearch) the index pattern that we want to use needs to be refreshed to pick up those field definitions. This is done from the Kibana console by clicking Management then Index Patterns and selecting the index pattern you are using, which in this scenario is the process-s* entry. Then click on the Refresh icon as highlighted below and confirm. Next we want to confirm that the events have been received in BAI from BAW and we can see some of the process summaries. We do this by moving to the Discover section in Kibana and then select the Search named Denim Compute - Auto Claims All Processes (you can do this by choosing the Open option and then filtering and finding the target Search definition). The screenshot below shows a point in time view of this Search where it has retrieved 193 hits. Highlighted are the active Filters used to identify the process summaries of interest for our scenario. Note if you are following along in your own environment and you do not see any hits, make sure you have followed the instructions in the Scenario installation section of this document and in the OpenShift BAI installation section. At this point, we are ready to start examining the configured Dashboards in Kibana. Here from the dashboard section we Open the one named Denim Compute - Active Auto Claims and there we see some visualizations have been configured and arranged on the dashboard. Additionally highlighted is a feature in Kibana where you can set the dashboard to Auto Refresh and set a frequency. You can then also pause and play if you want to halt the refreshes in order to examine something in detail. Also note that there is a breadcrumbs trail that allows for switching between the various dashboards that will be used for the scenario. Scrolling down the page we can see some variations in types of visualization and the aggregated data that is displayed on them to highlight certain trends such as the influence of the Weather Condition on the estimate value of the claim. We also have a chart for tracking at what lifecycle stage various in-flight claims are ( Active Claims by Age and Status ). As the Active Claims by Age and Status X-axis legend is hard to see with the restricted space you can maximize that visualization (from the contextual menu on it) to see it in more detail. You can also use the contextual menu to inspect the underlying data for a visualization and even export it out in CSV format. After returning back to the main dashboard view, at the bottom of the page are some data table types of visualization that shows the breakdown of claims by the Vehicle Make for both the insured party and the third party involved. Next we want to look at a different dashboard that focuses on completed instances, so do that by clicking the Completed Auto Claims breadcrumb link. This dashboard is named Denim Compute - Completed Auto Claims (note how it has a filter configured that checks that the completedTime exists). In this dashboard we can now show Claim Settlement figures and compare them to the Damage Estimates . Similar to the previous dashboard, this one uses various Visualizations but this time it focuses on the aggregation of Claim Settlement figures against various dimensions. Also notice that there is a Pie Chart that highlights the breakdown of claims into whether Fraud was suspected or not and then whether it was confirmed or cleared following investigation. We will see later that there is a further dashboard dedicated to the area of suspected fraudulent claims. We again use the set of Gauges that focus on the Policy Cover dimension. In the bottom section of the dashboard we again see the Data Tables organized by Vehicle Make and again focusing on Claim Settlement data. With the visualization named Settlements by Driver Age opened in full screen mode we highlight one of the bars and can see the underlying data, in this case it is reflecting the age profiles (in the emulation we deliberately allocate more claims to certain higher risk age groups). Next we change to the dashboard named Denim Compute - Suspected Fraudulent Auto Claims (accessed by clicking on the Suspected Fraudulent Auto Claims breadcrumb). It has a small subset of the Visualizations just to illustrate that we can look at similar data but filtered to the subset of data that involves potential and investigated fraud cases. The dashboard is pre-configured with a filter that is disabled by default. This filter will allow us to find only those potential frauds that were confirmed by investigation. So to enable this we hover over the filter where the options become visible, in this case we want the one to Enable filter . The result is then dynamically updated in the dashboard and we can see in this example that of the 79 potential frauds in fact 20 were confirmed (and of course in that case the Claim Settlement figures are 0 as no payment is made for fraud cases). We can also switch the filter to Exclude matches which in this case is looking for data on all potential frauds that were not proven Again the dashboard refreshes to take account of the changed filter and we see that 59 claims were not proven and in this case they do have payments made that are shown in the Claim Settlement figures. Once the emulation batch of 500 instances have completed in Denim Compute - Completed Auto Claims we now notice that there is an anomaly with the maximum amounts. This is because the emulation logic deliberately throws an \"outlier\" case into every execution run so that we can highlight here in the dashboard. We also highlight below that the skewed figure is in the state of Florida. If we select that state in the Region Map a dynamic filter is added and applied so that the dashboard now shows data just for that state. We can also use Exclude matches as we saw before on this new filter and this allows us to examine the various dimensions without the false trends that the outlier case was contributing to. Back in the Discover section and with the Search set to Denim Compute - Auto Claims All Processes we can filter for and examine the outlier case. By default a filter on the state of FL has been provided and disabled so you enable that and it should then show just that one outlier instance. You can then expand the twisty against the row and it will allow inspecting the data in a table or as raw JSON. You now can see all the tracking data that was summarized into this process summary from the events sent to BAI from BAW. Take a look at some of the key data items and see if you can figure out what unusual set of circumstances led to this large claim and especially the high differential in damage amounts between the two vehicles involved. Also note that no fraud was suspected (see the highlighted tracking field below) even in such odd claim data, this is because the emulation uses random spreads of data and does not use any in depth intelligence in identifying and investigating fraud cases. In a later iteration, we will start infusing machine learning into this scenario in order to greatly improve this situation. Scenario installation Prerequisites You should have installed the Cloud Pak for Automation as instructed in the various installation guidance documents in the OpenShift environment section. BAW application installation Install the application auto-claims-emulation-bai.twx file into your environment (confirm that it is the version shown below or higher): Now you should have the exposed process that can be launched from Process Portal as shown below. BAI Kibana saved objects installation In your BAI Kibana console navigate to Management , Saved Objects and click Import . Select the saved index pattern definition downloaded from dc-bai-custom-index-pattern.json and click Import . This will import a custom index pattern especially for Denim Compute which is configured with formatting of data fields and includes scripted fields. When imported navigate to Index Patterns and select process-s* and verify that there are 3 Scripted fields (ignore the number against Fields which may be different in your environment). Repeat the previous steps to import the definitions downloaded from dc-bai-dashboards-and-related.json . Now when you filter against Denim , you should be able to see the imported objects (33 in total). You now should have all you need to run the scenario as described earlier in this document.","title":"BAI scenario"},{"location":"usecase/bai-scenario-walkthrough/#bai-scenario-walkthrough","text":"Before proceeding with this scenario it is recommended you familiarize yourself with the main scenario . That scenario shows the interactions for a \"day in the life\" of a Case instance of Auto Claims and while that sends data through for visualizing in Business Automation Insights (BAI), to fully appreciate the power of BAI you need to generate some realistic loads. To that end, we have created a separate scenario that emulates the main scenario but without any real human interactions, so that it can execute many concurrent instances in batch and you can appreciate the value of BAI in a short amount of time. This emulation generates random data based on some configuration settings that are exposed for the end user to control as we shall see later in this document. If you want to follow along with this scenario on your own environment, see the instructions for installing in the Scenario installation section later in this document.","title":"BAI scenario walkthrough"},{"location":"usecase/bai-scenario-walkthrough/#business-automation-workflow","text":"The starting point is the Process Portal where any user can launch the Denim Compute Auto Claims - Emulation Generator workflow after which a work item Set Emulation Controls will appear to be actioned. The user interface coach is displayed allowing for configuring options to control the spread of emulated instances and how they then affect the business insights data that will be later viewed in BAI. We will explain these settings a little later. If this is being run for the first time in a IBM Cloud Pak for Automation environment it is a good idea to first verify the process behaves as expected without creating a lot of workflow instances. To do this set the Number of Claims to something like 300 instances (this will exercise the emulation's batch control which submits workflow instances in minimum batches of 100) and then choose the Test Emulation button option. This will exercise a path through the workflow that does everything except for instantiating the instances (the logic has a feature flag switch that treats it as in test mode and skips the message event which will spawn workflow instances). To verify the test you access the workflow instance in Process Inspector within the Process Admin Console as shown below. As the test emulation progresses you will see data variables set including the batch of emulated Auto Claim business objects (one instance is expanded in the list below for illustration). The emulation workflow instances may take a few minutes to complete depending on the total number of claims that were specified as the logic organizes the data into batches with a timed delay between them. When the emulation workflow instance has completed (which can be verified in Process Inspector) and we are satisfied it behaves as expected we can then relaunch the workflow and this time submit a real emulation run. Here in this first section we have set Number of Claims to 500 and the Maximum Claim Amount to 50000. (Please be aware that the more instances that are created here the more work for BAW operations team to clean up afterwards and purge the completed instances. Also the more instances requested the higher the relative load on the system so we recommend you exercise due care here and not unduly load a system that might have other business-critical workloads on it. If in doubt check with your operations team). The next set of settings control the spread of paths through the Auto Claims workflow. Simple scenarios do not have any adjustment, fraud checking etc and so we set a relatively low figure for this. Next the Percentage in Progress setting controls the spread of items that have emulated delays in processing (so that later, in BAI Dashboards, you can see interactively how active instances behave and the changes that occur as they move through their lifecycles). Percentage Potential Fraud emulates the path in the workflow whereby the call out to ODM rules determines whether there is a potential fraud situation that needs investigating. The Percentage Confirmed Fraud setting then controls what percentage of those potential frauds have been determined to be actual frauds (the rest will end up as being cleared of fraud). The final section controls the time delays for each part of the workflow that is not straight-through processing (STP). Here in the settings 30 minutes delay is set for each so it will configure variable times up to that maximum, this should mean that all 500 instances will have completed in approximately 2 hours elapsed time. We can visualize this in the BAW emulation workflow itself where the items color-coded green are STP and those in orange are the human interaction steps which matches how the main scenario behaves. Once satisfied with the settings the request is submitted by clicking the Run Emulation button option. After giving it a few minutes you should start to see the instances getting created and registering in Process Inspector, as shown here.","title":"Business Automation Workflow"},{"location":"usecase/bai-scenario-walkthrough/#business-automation-insights","text":"When new tracking data fields are first encountered in BAI (in Elasticsearch) the index pattern that we want to use needs to be refreshed to pick up those field definitions. This is done from the Kibana console by clicking Management then Index Patterns and selecting the index pattern you are using, which in this scenario is the process-s* entry. Then click on the Refresh icon as highlighted below and confirm. Next we want to confirm that the events have been received in BAI from BAW and we can see some of the process summaries. We do this by moving to the Discover section in Kibana and then select the Search named Denim Compute - Auto Claims All Processes (you can do this by choosing the Open option and then filtering and finding the target Search definition). The screenshot below shows a point in time view of this Search where it has retrieved 193 hits. Highlighted are the active Filters used to identify the process summaries of interest for our scenario. Note if you are following along in your own environment and you do not see any hits, make sure you have followed the instructions in the Scenario installation section of this document and in the OpenShift BAI installation section. At this point, we are ready to start examining the configured Dashboards in Kibana. Here from the dashboard section we Open the one named Denim Compute - Active Auto Claims and there we see some visualizations have been configured and arranged on the dashboard. Additionally highlighted is a feature in Kibana where you can set the dashboard to Auto Refresh and set a frequency. You can then also pause and play if you want to halt the refreshes in order to examine something in detail. Also note that there is a breadcrumbs trail that allows for switching between the various dashboards that will be used for the scenario. Scrolling down the page we can see some variations in types of visualization and the aggregated data that is displayed on them to highlight certain trends such as the influence of the Weather Condition on the estimate value of the claim. We also have a chart for tracking at what lifecycle stage various in-flight claims are ( Active Claims by Age and Status ). As the Active Claims by Age and Status X-axis legend is hard to see with the restricted space you can maximize that visualization (from the contextual menu on it) to see it in more detail. You can also use the contextual menu to inspect the underlying data for a visualization and even export it out in CSV format. After returning back to the main dashboard view, at the bottom of the page are some data table types of visualization that shows the breakdown of claims by the Vehicle Make for both the insured party and the third party involved. Next we want to look at a different dashboard that focuses on completed instances, so do that by clicking the Completed Auto Claims breadcrumb link. This dashboard is named Denim Compute - Completed Auto Claims (note how it has a filter configured that checks that the completedTime exists). In this dashboard we can now show Claim Settlement figures and compare them to the Damage Estimates . Similar to the previous dashboard, this one uses various Visualizations but this time it focuses on the aggregation of Claim Settlement figures against various dimensions. Also notice that there is a Pie Chart that highlights the breakdown of claims into whether Fraud was suspected or not and then whether it was confirmed or cleared following investigation. We will see later that there is a further dashboard dedicated to the area of suspected fraudulent claims. We again use the set of Gauges that focus on the Policy Cover dimension. In the bottom section of the dashboard we again see the Data Tables organized by Vehicle Make and again focusing on Claim Settlement data. With the visualization named Settlements by Driver Age opened in full screen mode we highlight one of the bars and can see the underlying data, in this case it is reflecting the age profiles (in the emulation we deliberately allocate more claims to certain higher risk age groups). Next we change to the dashboard named Denim Compute - Suspected Fraudulent Auto Claims (accessed by clicking on the Suspected Fraudulent Auto Claims breadcrumb). It has a small subset of the Visualizations just to illustrate that we can look at similar data but filtered to the subset of data that involves potential and investigated fraud cases. The dashboard is pre-configured with a filter that is disabled by default. This filter will allow us to find only those potential frauds that were confirmed by investigation. So to enable this we hover over the filter where the options become visible, in this case we want the one to Enable filter . The result is then dynamically updated in the dashboard and we can see in this example that of the 79 potential frauds in fact 20 were confirmed (and of course in that case the Claim Settlement figures are 0 as no payment is made for fraud cases). We can also switch the filter to Exclude matches which in this case is looking for data on all potential frauds that were not proven Again the dashboard refreshes to take account of the changed filter and we see that 59 claims were not proven and in this case they do have payments made that are shown in the Claim Settlement figures. Once the emulation batch of 500 instances have completed in Denim Compute - Completed Auto Claims we now notice that there is an anomaly with the maximum amounts. This is because the emulation logic deliberately throws an \"outlier\" case into every execution run so that we can highlight here in the dashboard. We also highlight below that the skewed figure is in the state of Florida. If we select that state in the Region Map a dynamic filter is added and applied so that the dashboard now shows data just for that state. We can also use Exclude matches as we saw before on this new filter and this allows us to examine the various dimensions without the false trends that the outlier case was contributing to. Back in the Discover section and with the Search set to Denim Compute - Auto Claims All Processes we can filter for and examine the outlier case. By default a filter on the state of FL has been provided and disabled so you enable that and it should then show just that one outlier instance. You can then expand the twisty against the row and it will allow inspecting the data in a table or as raw JSON. You now can see all the tracking data that was summarized into this process summary from the events sent to BAI from BAW. Take a look at some of the key data items and see if you can figure out what unusual set of circumstances led to this large claim and especially the high differential in damage amounts between the two vehicles involved. Also note that no fraud was suspected (see the highlighted tracking field below) even in such odd claim data, this is because the emulation uses random spreads of data and does not use any in depth intelligence in identifying and investigating fraud cases. In a later iteration, we will start infusing machine learning into this scenario in order to greatly improve this situation.","title":"Business Automation Insights"},{"location":"usecase/bai-scenario-walkthrough/#scenario-installation","text":"","title":"Scenario installation"},{"location":"usecase/bai-scenario-walkthrough/#prerequisites","text":"You should have installed the Cloud Pak for Automation as instructed in the various installation guidance documents in the OpenShift environment section.","title":"Prerequisites"},{"location":"usecase/bai-scenario-walkthrough/#baw-application-installation","text":"Install the application auto-claims-emulation-bai.twx file into your environment (confirm that it is the version shown below or higher): Now you should have the exposed process that can be launched from Process Portal as shown below.","title":"BAW application installation"},{"location":"usecase/bai-scenario-walkthrough/#bai-kibana-saved-objects-installation","text":"In your BAI Kibana console navigate to Management , Saved Objects and click Import . Select the saved index pattern definition downloaded from dc-bai-custom-index-pattern.json and click Import . This will import a custom index pattern especially for Denim Compute which is configured with formatting of data fields and includes scripted fields. When imported navigate to Index Patterns and select process-s* and verify that there are 3 Scripted fields (ignore the number against Fields which may be different in your environment). Repeat the previous steps to import the definitions downloaded from dc-bai-dashboards-and-related.json . Now when you filter against Denim , you should be able to see the imported objects (33 in total). You now should have all you need to run the scenario as described earlier in this document.","title":"BAI Kibana saved objects installation"},{"location":"usecase/scenarios-intro/","text":"Scenarios introduction The following sub-sections have been created to illustrate various \"day in the life\" scenarios. You should start with the Main scenario which is an end-to-end scenario showing the use of the Business Automation Workflow (BAW) and its integration with Operational Decision Management (ODM) for automobile insurance claims processing. From there, if you are interested in: Extending the scenario with Business Automation Content Analyzer (BACA), see the BACA scenario . Extending the scenario with Business Automation Insights (BAI), see the BAI scenario .","title":"Introduction"},{"location":"usecase/scenarios-intro/#scenarios-introduction","text":"The following sub-sections have been created to illustrate various \"day in the life\" scenarios. You should start with the Main scenario which is an end-to-end scenario showing the use of the Business Automation Workflow (BAW) and its integration with Operational Decision Management (ODM) for automobile insurance claims processing. From there, if you are interested in: Extending the scenario with Business Automation Content Analyzer (BACA), see the BACA scenario . Extending the scenario with Business Automation Insights (BAI), see the BAI scenario .","title":"Scenarios introduction"},{"location":"usecase/use-case-definition/","text":"Use case definition Our reference implementation use case is automobile insurance claim processing. The different components we're using in the context of the use case are shown on the figure below. The capabilities involved in the first MVP are in gray, while the dashed boxes are the ones that will be included in future MVPs. Identifying personas Our standard DBA implementation methodology leverages IBM Design Thinking and in particular its focus on users and stakeholders' perspectives when designing business solutions. Here is an illustration of the use of one Design Thinking activity consisting in creating Empathy Maps, that is, looking at what a typical representative of a stakeholder role think, say, feel and do. Process discovery Process discovery and modeling is another key practice in our DBA methodology. We typically conduct it in a collaborative manner by leveraging our IBM Blueworks Live process mapping software available on the cloud as Software as a Service. Here is an example of the Process Map we defined to support the elaboration of the Denim Compute use case: The color coding is the following: Orange : an activity assumed on the Case Management side Purple : an activity requiring a BPM/ECM integration Green : a decision activity, invoking a ruleset Blue : a regular BPM activity. The overall process diagram for our use case is shown below. Since it involves ad-hoc activities, the diagram is not quite semantically correct. However, it gives a view of the intended sequence of events, along with the possible paths across the workflow. The first activity, Report Accident , represents the manual initial stimulus to start processing. It is not implemented either as a Case Activity or a BPMN Process. The first step under automated control is the following one, Gather Accident Information , which can either be initiated manually by a user starting a Claim case (say on receipt of a telephone call from the Insured to report accident) or by some document being added to the Claim folder. The following activities are not in scope of the current MVP: Assign Claim Adjuster Investigate Fraud Assess Escalated CLaim Pay Provider Invoice Process Simple Claim Reject Claim","title":"Use case definition"},{"location":"usecase/use-case-definition/#use-case-definition","text":"Our reference implementation use case is automobile insurance claim processing. The different components we're using in the context of the use case are shown on the figure below. The capabilities involved in the first MVP are in gray, while the dashed boxes are the ones that will be included in future MVPs.","title":"Use case definition"},{"location":"usecase/use-case-definition/#identifying-personas","text":"Our standard DBA implementation methodology leverages IBM Design Thinking and in particular its focus on users and stakeholders' perspectives when designing business solutions. Here is an illustration of the use of one Design Thinking activity consisting in creating Empathy Maps, that is, looking at what a typical representative of a stakeholder role think, say, feel and do.","title":"Identifying personas"},{"location":"usecase/use-case-definition/#process-discovery","text":"Process discovery and modeling is another key practice in our DBA methodology. We typically conduct it in a collaborative manner by leveraging our IBM Blueworks Live process mapping software available on the cloud as Software as a Service. Here is an example of the Process Map we defined to support the elaboration of the Denim Compute use case: The color coding is the following: Orange : an activity assumed on the Case Management side Purple : an activity requiring a BPM/ECM integration Green : a decision activity, invoking a ruleset Blue : a regular BPM activity. The overall process diagram for our use case is shown below. Since it involves ad-hoc activities, the diagram is not quite semantically correct. However, it gives a view of the intended sequence of events, along with the possible paths across the workflow. The first activity, Report Accident , represents the manual initial stimulus to start processing. It is not implemented either as a Case Activity or a BPMN Process. The first step under automated control is the following one, Gather Accident Information , which can either be initiated manually by a user starting a Claim case (say on receipt of a telephone call from the Insured to report accident) or by some document being added to the Claim folder. The following activities are not in scope of the current MVP: Assign Claim Adjuster Investigate Fraud Assess Escalated CLaim Pay Provider Invoice Process Simple Claim Reject Claim","title":"Process discovery"}]}